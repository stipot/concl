{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# сравнение функций потерь от сжатого и сырого контекстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* Отладка. Проблема Сжатый контекст: N/A\n",
    "* Пересмотреть функцию потерь, чтобы она отражала задачу сжатия более точно (например, использовать потери восстановления или схожести между оригиналом и сжатым контекстом).\n",
    "* Усложнить архитектуру модели ContextOptimizer, добавив больше слоев или более продвинутые блоки, такие как GRU или LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основная цель\n",
    "Обучить отдельную сеть \\( O \\) (модуль оптимизации контекста), которая динамически изменяет контекст таким образом, чтобы предсказания исходной языковой модели \\( M \\) оставались точными, несмотря на уменьшение объема информации в контексте.\n",
    "\n",
    "### Структура и Процесс Обучения\n",
    "\n",
    "1. **Основная Языковая Модель \\( M \\):**\n",
    "   - \\( M \\) — существующая языковая модель, такая как GPT-2, которая принимает на вход полный контекст и текущий фрагмент и генерирует предсказания.\n",
    "   - Цель \\( M \\) — максимальная точность предсказаний на основе полного контекста.\n",
    "\n",
    "2. **Модуль Оптимизации Контекста \\( O \\):**\n",
    "   - Модуль \\( O \\) используется для сжатия предыстории \\( C \\) с учетом текущего фрагмента \\( S \\), чтобы создать оптимизированный контекст \\( C' \\).\n",
    "   - Модуль \\( O \\) может быть реализован через нейронные сети, такие как трансформеры, рекуррентные сети или другие архитектуры, способные выделять ключевые элементы контекста.\n",
    "\n",
    "3. **Целевая Функция и Метрики:**\n",
    "   - Основной задачей является минимизация расхождения между предсказаниями модели \\( M \\) на основе \\( C' \\) и \\( C \\). Целевая функция может быть определена как:\n",
    "     \\[\n",
    "     L = \\text{Loss}(M(C', S), M(C, S))\n",
    "     \\]\n",
    "   - Для измерения качества сжатия можно использовать метрики, такие как среднеквадратичная ошибка (MSE) между эмбеддингами предсказаний, кросс-энтропия, и т.д.\n",
    "   - Важно также отслеживать сходимость потерь между \\( C' \\) и \\( C \\), чтобы убедиться, что \\( C' \\) минимально влияет на точность предсказаний.\n",
    "\n",
    "4. **Процесс Обучения:**\n",
    "   - **Шаг 1:** Из датасета создаются пары \\( (C, S, T) \\), где \\( C \\) — предыстория, \\( S \\) — текущий фрагмент, и \\( T \\) — целевой текст для предсказания.\n",
    "   - **Шаг 2:** Модуль \\( O \\) сжимает контекст \\( C \\), производя \\( C' = O(C, S) \\).\n",
    "   - **Шаг 3:** Модель \\( M \\) предсказывает на основе \\( (C', S) \\) и \\( (C, S) \\). Сравниваются предсказания для оценки потерь.\n",
    "   - **Шаг 4:** Потери используются для обновления параметров модуля \\( O \\), минимизируя влияние сжатия на точность.\n",
    "\n",
    "5. **Анализ и Диагностика:**\n",
    "   - Постепенно выводим промежуточные результаты для диагностики, включая анализ предсказаний по сжатому и полному контексту.\n",
    "   - Выполняем оценку примеров, где предсказания на основе сжатого контекста значительно отличаются от предсказаний на основе полного контекста.\n",
    "   - Используем фильтрацию данных для обучения, например, исключаем случаи, где текущий фрагмент уже достаточен для точного предсказания.\n",
    "\n",
    "### Рекомендации для Улучшения\n",
    "\n",
    "- **Итеративная Диагностика:** Важно регулярно проверять, насколько сжатый контекст \\( C' \\) сохраняет критическую информацию. Диагностика на нескольких примерах может выявить случаи, где \\( O \\) недооценивает важные части контекста.\n",
    "- **Контроль за Переобучением:** Следите за тем, чтобы модель \\( O \\) не начинала генерировать «фантазии» или добавлять избыточные детали в попытках компенсировать потерю информации.\n",
    "- **Улучшение Сходимости:** Добавьте регуляризацию или дополнительные ограничения на модель \\( O \\), чтобы предотвратить отклонения в сжатии, которые могут ухудшить точность предсказаний.\n",
    "\n",
    "### Ожидаемый Результат\n",
    "\n",
    "- Модуль \\( O \\) научится выделять только те части контекста, которые критически важны для точных предсказаний, избегая ненужного увеличения объема информации.\n",
    "- Модель \\( M \\), используя \\( C' \\), будет предсказывать с минимальной потерей точности, что укажет на успешное сжатие и оптимизацию контекста. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Подготовка Данных**\n",
    "   - **Предварительная обработка данных:** Датасет разбивается на последовательности: предыстория (context), текущий фрагмент (current_fragment), и целевой текст (target).\n",
    "   - **Фильтрация данных:** Удаляются пустые и нерелевантные записи, например, те, где текущий фрагмент не вносит значимого вклада в предсказание.\n",
    "   - **Предварительная оценка потерь:** Вычисляются потери для различных комбинаций контекста, что позволяет заранее оценить вклад контекста в точность предсказания.\n",
    "\n",
    "### 2. **Оптимизация Контекста**\n",
    "   - **Целевая функция:** Используется MSELoss для измерения расхождения между предсказаниями модели и целевыми значениями.\n",
    "   - **Сходимость потерь:** Основная цель — сходимость потерь сжатого контекста (compressed_loss) к потерям сырого контекста (raw_loss).\n",
    "   - **Контроль за «додумыванием»:** Введены штрафы за значительное отклонение между потерями сжатого и сырого контекста, чтобы предотвратить генерацию ложных зависимостей.\n",
    "\n",
    "### 3. **Адаптивная Регуляция Потерь**\n",
    "   - **Штрафы за расхождения:** Если потери сжатого контекста значительно отличаются от потерь сырого, в потери добавляется штраф, который стимулирует модель уменьшать эти различия.\n",
    "   - **Адаптивное обучение:** Обучение адаптируется на основе расхождений, улучшая способность модели сжимать контекст, сохраняя при этом точность.\n",
    "\n",
    "### 4. **Использование Промежуточных Датасетов**\n",
    "   - **Сохранение промежуточных результатов:** Включение предсказаний модели в промежуточный датасет, чтобы ускорить и упростить анализ, отладку и последующую оптимизацию.\n",
    "   - **Фильтрация на этапе подготовки:** Отбираются записи, в которых контекст оказывает значительное влияние на предсказания, отбрасывая те, где контекст несущественен.\n",
    "\n",
    "### 5. **Визуализация и Отладка**\n",
    "   - **Графики потерь:** Построение графиков с использованием Plotly для отслеживания динамики потерь сжатого и сырого контекстов.\n",
    "   - **Примеры для анализа:** Регулярный вывод примеров с различными типами предсказаний (сжатый контекст, сырой контекст, текущий фрагмент) для визуального анализа и проверки корректности работы модели.\n",
    "\n",
    "### 6. **Управление Временем и Эффективностью**\n",
    "   - **Оценка времени:** Оценка времени выполнения подготовки данных и каждой эпохи обучения на основе первых пяти циклов, что позволяет планировать ресурсозатраты.\n",
    "   - **Оптимизация кода:** Фокусировка на ключевых этапах, таких как загрузка моделей, настройка параметров и запуск оптимизации с минимальными затратами времени.\n",
    "\n",
    "### Основные цели:\n",
    "- Обеспечение качественного сжатия контекста без потери точности.\n",
    "- Сведение к минимуму расхождения потерь между сжатым и сырым контекстами.\n",
    "- Избежание нежелательных интерпретаций и «додумывания» контекста моделью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модульная структура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательные классы и методы\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Подготовка данных\n",
    "def prepare_data(dataset, max_length=300):\n",
    "    \"\"\"\n",
    "    Функция подготавливает данные из датасета, включая предысторию, текущий фрагмент и целевой фрагмент текста,\n",
    "    используя смещающееся окно по элементам датасета.\n",
    "\n",
    "    :param dataset: Датасет с текстами.\n",
    "    :param max_length: Максимальная длина текущего фрагмента.\n",
    "    :return: Список кортежей (предыстория, текущий фрагмент, целевой фрагмент).\n",
    "    \"\"\"\n",
    "    prepared_data = []\n",
    "    buffer = []\n",
    "    counter = 0\n",
    "    for example in dataset:\n",
    "        text = example[\"text\"].strip()\n",
    "\n",
    "        # Пропускаем пустые записи\n",
    "        # TODO Убрать отладочную выборку первых 1000 записей\n",
    "        if len(text) == 0 or counter > 1000:\n",
    "            continue\n",
    "        counter += 1\n",
    "        buffer.append(text)\n",
    "\n",
    "        # Если буфер содержит три фрагмента, создаем (context, current_fragment, target)\n",
    "        if len(buffer) == 3:\n",
    "            context, current_fragment, target = buffer\n",
    "\n",
    "            # Обрезаем фрагменты по max_length, если необходимо\n",
    "            context = context[:max_length].strip()\n",
    "            current_fragment = current_fragment[:max_length].strip()\n",
    "            target = target[:max_length].strip()\n",
    "\n",
    "            # Пропускаем записи, если какой-либо из фрагментов пустой\n",
    "            if all([context, current_fragment, target]):\n",
    "                prepared_data.append((context, current_fragment, target))\n",
    "\n",
    "            # Сдвигаем окно\n",
    "            buffer.pop(0)\n",
    "    return prepared_data\n",
    "\n",
    "\n",
    "# Функции для сохранения и загрузки модели\n",
    "def save_model(model, optimizer, path=\"context_optimizer.pth\"):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, path=\"context_optimizer.pth\"):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "    else:\n",
    "        print(f\"Model file not found at {path}\")\n",
    "\n",
    "\n",
    "# Визуализация с использованием Plotly FigureWidget\n",
    "def create_loss_plot():\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Raw Context Loss\"))\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Compressed Context Loss\"))\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Fragment Only Loss\"))\n",
    "    fig.update_layout(title=\"Графики потерь во время обучения\", xaxis_title=\"Эпоха\", yaxis_title=\"Среднее значение потерь\", template=\"plotly_dark\")\n",
    "    display(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def update_loss_plot(fig, epoch, raw_loss, compressed_loss, fragment_loss):\n",
    "    fig.data[0].x += (epoch,)\n",
    "    fig.data[0].y += (raw_loss,)\n",
    "    fig.data[1].x += (epoch,)\n",
    "    fig.data[1].y += (compressed_loss,)\n",
    "    fig.data[2].x += (epoch,)\n",
    "    fig.data[2].y += (fragment_loss,)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Функция для расчета compression ratio\n",
    "def calculate_compression_ratio(raw_embedding, compressed_embedding):\n",
    "    \"\"\"\n",
    "    Вычисляет отношение нормы сжатого контекста к норме сырого контекста.\n",
    "    \"\"\"\n",
    "    raw_norm = torch.norm(raw_embedding, p=2, dim=1)\n",
    "    compressed_norm = torch.norm(compressed_embedding, p=2, dim=1)\n",
    "    return compressed_norm / raw_norm\n",
    "\n",
    "\n",
    "def predict_with_compressed_context(current_fragment, tokenizer, gpt2_model, device, max_new_tokens=50, num_beams=5):\n",
    "    # Ensure the tokenizer is correctly initialized\n",
    "    if isinstance(tokenizer, GPT2Tokenizer):\n",
    "        # Set the pad_token to eos_token if not already set\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "            # Alternatively, add a new padding token if needed:\n",
    "            # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "        # Tokenize the current fragment\n",
    "        inputs = tokenizer(current_fragment, return_tensors=\"pt\", padding=True).to(device)\n",
    "        input_ids = inputs.get(\"input_ids\")\n",
    "        attention_mask = inputs.get(\"attention_mask\")  # Adding attention_mask\n",
    "\n",
    "        # Check that input_ids is not empty\n",
    "        if input_ids is not None and input_ids.shape[1] > 0:\n",
    "            with torch.no_grad():\n",
    "                # Generate predictions directly from current_fragment\n",
    "                outputs = gpt2_model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,  # Use the attention mask\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=num_beams,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=tokenizer.pad_token_id,  # Ensure pad_token_id is set correctly\n",
    "                )\n",
    "                predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                return predicted_text\n",
    "        else:\n",
    "            print(f\"Invalid inputs: input_ids length {input_ids.shape[1] if input_ids is not None else 'None'}\")\n",
    "    else:\n",
    "        print(\"Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\")\n",
    "\n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "# Использование предсказаний из предподготовленных данных\n",
    "def evaluate_with_prepared_data(prepared_data, model, tokenizer, gpt2_model, device):\n",
    "    \"\"\"\n",
    "    Выполняет оценку качества предсказаний с использованием подготовленных данных.\n",
    "    \"\"\"\n",
    "    for entry in prepared_data:\n",
    "        context = entry[\"context\"]\n",
    "        current_fragment = entry[\"current_fragment\"]\n",
    "        target = entry[\"target\"]\n",
    "        raw_prediction = entry[\"raw_prediction\"]\n",
    "        fragment_prediction = entry[\"fragment_prediction\"]\n",
    "\n",
    "        # Сжатие контекста и предсказание с ним\n",
    "        compressed_context = model.compress(context)  # Предположим, у нас есть метод compress в модели\n",
    "        compressed_prediction = predict_with_compressed_context(compressed_context, current_fragment, tokenizer, gpt2_model, device)\n",
    "\n",
    "        # Сравнение предсказаний\n",
    "        print(f\"Контекст: {context}\")\n",
    "        print(f\"Текущий фрагмент: {current_fragment}\")\n",
    "        print(f\"Целевой текст: {target}\")\n",
    "        print(f\"Предсказание по сырому контексту: {raw_prediction}\")\n",
    "        print(f\"Предсказание по текущему фрагменту: {fragment_prediction}\")\n",
    "        print(f\"Предсказание по сжатому контексту: {compressed_prediction}\\n\")\n",
    "\n",
    "\n",
    "def load_intermediate_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель сжатия контекста\n",
    "\n",
    "\n",
    "class ContextOptimizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ContextOptimizer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, context):\n",
    "        x = self.relu(self.fc1(context))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Функция для сжатия контекста\n",
    "def compress_context(context, optimizer, tokenizer, gpt2_model, device):\n",
    "    # Здесь выполняется сжатие контекста\n",
    "\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(**inputs, output_hidden_states=True)\n",
    "        raw_context_embedding = outputs.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "    # Дополнительная проверка перед сжатием\n",
    "    if raw_context_embedding is None or raw_context_embedding.size(0) == 0:\n",
    "        print(\"Warning: raw_context_embedding is empty or invalid\")\n",
    "\n",
    "    # Пример сжатия (эта часть зависит от конкретной реализации оптимизатора)\n",
    "    compressed_context = optimizer(raw_context_embedding)\n",
    "\n",
    "    # Проверка формы и значений сжатого контекста\n",
    "    if not (compressed_context is not None and isinstance(compressed_context, torch.Tensor)):\n",
    "        print(\"Compressed context is invalid\")\n",
    "\n",
    "    return compressed_context, raw_context_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "def custom_loss(compressed_output, target_embedding, raw_context_embedding):\n",
    "    \"\"\"\n",
    "    Функция потерь, которая учитывает схожесть с оригиналом и целевым вектором.\n",
    "    \"\"\"\n",
    "    # Потеря восстановления: сравниваем сжатый контекст с исходным контекстом\n",
    "    reconstruction_loss = 1 - cosine_similarity(compressed_output, raw_context_embedding, dim=1).mean()\n",
    "\n",
    "    # Потеря предсказания: сравниваем предсказанный выход с целевым\n",
    "    prediction_loss = nn.MSELoss()(compressed_output, target_embedding)\n",
    "\n",
    "    # Итоговая потеря: комбинация восстановления и предсказания\n",
    "    total_loss = prediction_loss + reconstruction_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train_context_optimizer(optimizer, intermediate_data, tokenizer, gpt2_model, epochs=4, device=\"cpu\"):\n",
    "    criterion = nn.MSELoss()\n",
    "    opt = optim.Adam(optimizer.parameters(), lr=0.001)\n",
    "    raw_loss_values = []\n",
    "    compressed_loss_values = []\n",
    "    fragment_loss_values = []\n",
    "\n",
    "    # Инициализация графика потерь\n",
    "    loss_fig = create_loss_plot()\n",
    "    # Для отслеживания потерь по каждому примеру на каждой эпохе\n",
    "    example_loss_history = {i: [] for i in range(len(intermediate_data))}\n",
    "    for epoch in range(epochs):\n",
    "        examples = []\n",
    "        total_compression_ratio = 0\n",
    "        total_compressed_loss = 0\n",
    "        total_divergence = 0  # Переменная для отслеживания расхождения между потерями\n",
    "        count = 0\n",
    "\n",
    "        epoch_losses = []  # Для хранения потерь текущей эпохи\n",
    "        examples_calculated = []\n",
    "        for idx, entry in enumerate(intermediate_data):\n",
    "            context = entry[\"context\"]\n",
    "            current_fragment = entry[\"current_fragment\"]\n",
    "            target = entry[\"target\"]\n",
    "\n",
    "            # Используем предвычисленные значения потерь для сырого контекста и текущего фрагмента\n",
    "            raw_loss = entry[\"raw_loss\"]\n",
    "            fragment_loss = entry[\"fragment_loss\"]\n",
    "\n",
    "            # Подготовка целевого предсказания\n",
    "            target_input = tokenizer(target, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "                target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "            # Сжатие контекста и вычисление потерь для сжатого контекста в комбинации с текущим фрагментом\n",
    "            compressed_context, raw_context_embedding = compress_context(context, optimizer, tokenizer, gpt2_model, device)\n",
    "            compressed_inputs = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "            compressed_context_repeated = compressed_context.unsqueeze(1).repeat(1, compressed_inputs[\"input_ids\"].size(1), 1)\n",
    "            compressed_inputs_embeds = compressed_context_repeated + gpt2_model.transformer.wte(compressed_inputs[\"input_ids\"])\n",
    "            compressed_outputs = gpt2_model(inputs_embeds=compressed_inputs_embeds, output_hidden_states=True).hidden_states[-1].mean(dim=1)\n",
    "            # compressed_loss = criterion(compressed_outputs, target_embedding)\n",
    "            # Используем custom_loss вместо обычной MSE\n",
    "            compressed_loss = custom_loss(compressed_outputs, target_embedding, raw_context_embedding)\n",
    "\n",
    "            # Вычисление расхождения потерь\n",
    "            divergence = abs(compressed_loss.item() - raw_loss)\n",
    "            total_divergence += divergence\n",
    "\n",
    "            # Корректировка потерь для учета расхождения\n",
    "            adjusted_loss = compressed_loss + divergence  # Добавляем расхождение для управления сходимостью\n",
    "\n",
    "            # Вычисление compression ratio\n",
    "            compression_ratio = calculate_compression_ratio(raw_context_embedding, compressed_context)\n",
    "            total_compression_ratio += compression_ratio.item()\n",
    "\n",
    "            # Обновление оптимизатора только по скорректированным потерям\n",
    "            opt.zero_grad()\n",
    "            adjusted_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_compressed_loss += compressed_loss.item()\n",
    "            count += 1\n",
    "\n",
    "            # Сохраняем текущую потерю для анализа изменения потерь\n",
    "            epoch_losses.append(compressed_loss.item())\n",
    "            example_loss_history[idx].append(compressed_loss.item())\n",
    "            # Сохранение примеров для диагностики\n",
    "            with torch.no_grad():\n",
    "                compressed_pred = predict_with_compressed_context(compressed_context, current_fragment, tokenizer, gpt2_model, device)\n",
    "                raw_pred = entry[\"raw_prediction\"]\n",
    "                fragment_pred = entry[\"fragment_prediction\"]\n",
    "\n",
    "                # Проверяем и обрабатываем compressed_context корректно перед декодированием\n",
    "                if compressed_context is not None and isinstance(compressed_context, torch.Tensor):\n",
    "                    # Пытаемся преобразовать в список токенов, исключая None\n",
    "                    compressed_tokens = [token for token in compressed_context.squeeze().tolist() if isinstance(token, (int, float)) and 0 <= token < tokenizer.vocab_size]\n",
    "\n",
    "                    if compressed_tokens:\n",
    "                        # Декодируем только корректные значения токенов\n",
    "                        compressed_context_text = tokenizer.decode(compressed_tokens)\n",
    "                    else:\n",
    "                        compressed_context_text = \"Empty or Invalid tokens found in compressed context\"\n",
    "                else:\n",
    "                    compressed_context_text = \"N/A\"\n",
    "\n",
    "                # Сохраняем примеры в отдельный список для всех примеров\n",
    "                examples_calculated.append(\n",
    "                    {\n",
    "                        \"context\": context,\n",
    "                        \"compressed_context\": compressed_context_text,\n",
    "                        \"current_fragment\": current_fragment,\n",
    "                        \"compressed_prediction\": compressed_pred,\n",
    "                        \"raw_prediction\": raw_pred,\n",
    "                        \"fragment_prediction\": fragment_pred,\n",
    "                        \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Анализ потерь между эпохами для вывода примеров с максимальным снижением потерь\n",
    "        if epoch > 0:\n",
    "            loss_reduction = [\n",
    "                (i, example_loss_history[i][-2] - example_loss_history[i][-1], examples_calculated[i][\"compressed_context\"], examples_calculated[i][\"compressed_prediction\"])\n",
    "                for i in range(len(epoch_losses))\n",
    "                if len(example_loss_history[i]) > 1\n",
    "            ]\n",
    "            loss_reduction.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_examples = loss_reduction[:3]  # Три примера с максимальным снижением потерь\n",
    "        else:\n",
    "            top_examples = [\n",
    "                (i, 0, examples_calculated[i][\"compressed_context\"], examples_calculated[i][\"compressed_prediction\"]) for i in range(min(3, len(examples_calculated)))\n",
    "            ]  # Первые три примера для первой эпохи\n",
    "\n",
    "        avg_compressed_loss = total_compressed_loss / count if count > 0 else 0\n",
    "        avg_compression_ratio = total_compression_ratio / count if count > 0 else 0\n",
    "        avg_divergence = total_divergence / count if count > 0 else 0\n",
    "        raw_loss_values.append(raw_loss)\n",
    "        compressed_loss_values.append(avg_compressed_loss)\n",
    "        fragment_loss_values.append(fragment_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Avg Compressed Context Loss: {avg_compressed_loss:.4f}, Avg Compression Ratio: {avg_compression_ratio:.4f}, Avg Divergence: {avg_divergence:.4f}\")\n",
    "\n",
    "        # Обновление графика после каждой эпохи\n",
    "        update_loss_plot(loss_fig, epoch + 1, raw_loss, avg_compressed_loss, fragment_loss)\n",
    "\n",
    "        # Вывод примеров с максимальным снижением потерь\n",
    "        print(f\"\\n=== Примеры с максимальным снижением потерь после эпохи {epoch+1} ===\")\n",
    "        for idx, reduction, compressed_context, compressed_prediction in top_examples:\n",
    "            example = intermediate_data[idx]\n",
    "            print(f\"\\nПример {idx+1} (Снижение потерь: {reduction:.4f}):\")\n",
    "            print(f\"Предыстория: {example['context']}\")\n",
    "            print(f\"Сжатый контекст: {compressed_context}\")  # Используем сохраненный сжатый контекст\n",
    "            print(f\"Текущий фрагмент: {example['current_fragment']}\")\n",
    "            print(f\"Предсказание по сжатому контексту: {compressed_prediction}\")  # Используем сохраненное предсказание по сжатому контексту\n",
    "            print(f\"Предсказание по сырому контексту: {example['raw_prediction']}\")\n",
    "            print(f\"Предсказание по текущему фрагменту: {example['fragment_prediction']}\")\n",
    "            print(f\"Целевой текст: {example['target']}\")\n",
    "\n",
    "    return raw_loss_values, compressed_loss_values, fragment_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\inimatic\\AppData\\Local\\Temp\\ipykernel_16092\\595258241.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from context_optimizer.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd5739e157a4cf295abbf6263d2a28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Raw Context Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '0f906216-7738-4554-9d52-e42f2ee008b4',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'mode': 'lines+markers',\n",
       "              'name': 'Compressed Context Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'e3a68387-bf89-4b08-845d-e17762bfbe62',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'mode': 'lines+markers',\n",
       "              'name': 'Fragment Only Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '6291cc5d-116b-4854-b96f-f251a1f91a63',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Графики потерь во время обучения'},\n",
       "               'xaxis': {'title': {'text': 'Эпоха'}},\n",
       "               'yaxis': {'title': {'text': 'Среднее значение потерь'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n",
      "Invalid tokenizer object. Ensure the tokenizer is an instance of GPT2Tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m load_model(context_optimizer, context_optimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_optimizer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Запуск обучения\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m raw_loss_values, compressed_loss_values, fragment_loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_context_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt2_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Сохранение модели после обучения\u001b[39;00m\n\u001b[0;32m     36\u001b[0m save_model(context_optimizer, context_optimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_optimizer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mtrain_context_optimizer\u001b[1;34m(optimizer, intermediate_data, tokenizer, gpt2_model, epochs, device)\u001b[0m\n\u001b[0;32m     55\u001b[0m     target_embedding \u001b[38;5;241m=\u001b[39m target_output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Сжатие контекста и вычисление потерь для сжатого контекста в комбинации с текущим фрагментом\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m compressed_context, raw_context_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mcompress_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt2_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m compressed_inputs \u001b[38;5;241m=\u001b[39m tokenizer(current_fragment, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     60\u001b[0m compressed_context_repeated \u001b[38;5;241m=\u001b[39m compressed_context\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, compressed_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mcompress_context\u001b[1;34m(context, optimizer, tokenizer, gpt2_model, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(context, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m gpt2_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m     raw_context_embedding \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Дополнительная проверка перед сжатием\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1315\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1315\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1129\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1118\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1119\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         output_attentions,\n\u001b[0;32m   1127\u001b[0m     )\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:651\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    649\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    650\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 651\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    653\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:575\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m    574\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[1;32m--> 575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[0;32m    577\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Настройки устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка модели GPT-2 и токенизатора\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Загрузка датасета и подготовка промежуточных данных\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "intermediate_dataset_path = \"intermediate_dataset.json\"\n",
    "\n",
    "# Генерация промежуточного датасета, если он еще не создан\n",
    "if not os.path.exists(intermediate_dataset_path):\n",
    "    generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device=device, intermediate_dataset_path=intermediate_dataset_path)\n",
    "else:\n",
    "    # Загрузка промежуточного датасета\n",
    "    intermediate_data = load_intermediate_dataset(intermediate_dataset_path)\n",
    "\n",
    "# Инициализация модели сжатия контекста\n",
    "input_size = 768  # Размер входных данных модели GPT-2\n",
    "hidden_size = 256  # Скрытый размер для оптимизатора контекста\n",
    "context_optimizer = ContextOptimizer(input_size, hidden_size).to(device)\n",
    "\n",
    "# Загрузка сохраненной модели, если она существует\n",
    "load_model(context_optimizer, context_optimizer, \"context_optimizer.pth\")\n",
    "\n",
    "# Запуск обучения\n",
    "raw_loss_values, compressed_loss_values, fragment_loss_values = train_context_optimizer(context_optimizer, intermediate_data, tokenizer, gpt2_model, epochs=10, device=device)\n",
    "\n",
    "# Сохранение модели после обучения\n",
    "save_model(context_optimizer, context_optimizer, \"context_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предподготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Начало подготовки данных...\n",
      "Оценка времени выполнения цикла: 435.63 секунд для 999 записей.\n",
      "Промежуточный датасет создан с 197 записями, включая значения потерь и предсказания.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import time\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device=\"cpu\", intermediate_dataset_path=\"intermediate_dataset.json\"):\n",
    "    intermediate_data = []\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Подготавливаем данные\n",
    "    print(\"Начало подготовки данных...\")\n",
    "    prepared_data = prepare_data(dataset, max_length=500)\n",
    "\n",
    "    # Оценка времени выполнения цикла для первых пяти итераций\n",
    "    cycle_times = []\n",
    "    for i, (context, current_fragment, target) in enumerate(prepared_data[:5]):\n",
    "        if context is None or current_fragment is None:\n",
    "            continue\n",
    "\n",
    "        cycle_start_time = time.time()\n",
    "\n",
    "        # Подготовка целевого предсказания\n",
    "        target_input = tokenizer(target, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "            target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "        # Предсказание по сырому контексту (предыстория + текущий фрагмент)\n",
    "        raw_inputs = tokenizer(context + \" \" + current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            raw_outputs = gpt2_model(**raw_inputs, output_hidden_states=True)\n",
    "            raw_combined_embedding = raw_outputs.hidden_states[-1].mean(dim=1)\n",
    "            raw_loss = criterion(raw_combined_embedding, target_embedding).item()\n",
    "            raw_prediction = tokenizer.decode(raw_outputs.logits.argmax(-1).squeeze().tolist())\n",
    "\n",
    "        # Предсказание только по текущему фрагменту\n",
    "        fragment_input = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            fragment_output = gpt2_model(**fragment_input, output_hidden_states=True)\n",
    "            fragment_embedding = fragment_output.hidden_states[-1].mean(dim=1)\n",
    "            fragment_loss = criterion(fragment_embedding, target_embedding).item()\n",
    "            fragment_prediction = tokenizer.decode(fragment_output.logits.argmax(-1).squeeze().tolist())\n",
    "\n",
    "        cycle_end_time = time.time()\n",
    "        cycle_times.append(cycle_end_time - cycle_start_time)\n",
    "\n",
    "        # Фильтрация по критерию fragment_loss > raw_loss\n",
    "        if fragment_loss > raw_loss and fragment_loss < 1:\n",
    "            # Сохранение всех значений и предсказаний\n",
    "            intermediate_data.append(\n",
    "                {\n",
    "                    \"context\": context,\n",
    "                    \"current_fragment\": current_fragment,\n",
    "                    \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                    \"raw_loss\": raw_loss,\n",
    "                    \"fragment_loss\": fragment_loss,\n",
    "                    \"raw_prediction\": raw_prediction,\n",
    "                    \"fragment_prediction\": fragment_prediction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Оценка общего времени выполнения цикла на основе первых пяти измерений\n",
    "    avg_cycle_time = sum(cycle_times) / len(cycle_times) if cycle_times else 0\n",
    "    estimated_total_time = avg_cycle_time * len(prepared_data)\n",
    "    print(f\"Оценка времени выполнения цикла: {estimated_total_time:.2f} секунд для {len(prepared_data)} записей.\")\n",
    "\n",
    "    # Продолжение обработки остальных данных после оценки времени\n",
    "    for i, (context, current_fragment, target) in enumerate(prepared_data[5:]):\n",
    "        if context is None or current_fragment is None:\n",
    "            continue\n",
    "\n",
    "        # Подготовка целевого предсказания\n",
    "        target_input = tokenizer(target, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "            target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "        # Предсказание по сырому контексту (предыстория + текущий фрагмент)\n",
    "        raw_inputs = tokenizer(context + \" \" + current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            raw_outputs = gpt2_model(**raw_inputs, output_hidden_states=True)\n",
    "            raw_combined_embedding = raw_outputs.hidden_states[-1].mean(dim=1)\n",
    "            raw_loss = criterion(raw_combined_embedding, target_embedding).item()\n",
    "            raw_prediction = tokenizer.decode(raw_outputs.logits.argmax(-1).squeeze().tolist())\n",
    "\n",
    "        # Предсказание только по текущему фрагменту\n",
    "        fragment_input = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            fragment_output = gpt2_model(**fragment_input, output_hidden_states=True)\n",
    "            fragment_embedding = fragment_output.hidden_states[-1].mean(dim=1)\n",
    "            fragment_loss = criterion(fragment_embedding, target_embedding).item()\n",
    "            fragment_prediction = tokenizer.decode(fragment_output.logits.argmax(-1).squeeze().tolist())\n",
    "\n",
    "        # Фильтрация по критерию fragment_loss > raw_loss and fragment_loss <= 1\n",
    "        if fragment_loss > raw_loss and fragment_loss < 1:\n",
    "            # Сохранение всех значений и предсказаний\n",
    "            intermediate_data.append(\n",
    "                {\n",
    "                    \"context\": context,\n",
    "                    \"current_fragment\": current_fragment,\n",
    "                    \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                    \"raw_loss\": raw_loss,\n",
    "                    \"fragment_loss\": fragment_loss,\n",
    "                    \"raw_prediction\": raw_prediction,\n",
    "                    \"fragment_prediction\": fragment_prediction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Сохранение промежуточного датасета в JSON\n",
    "    with open(intermediate_dataset_path, \"w\") as f:\n",
    "        json.dump(intermediate_data, f, indent=4)\n",
    "\n",
    "    print(f\"Промежуточный датасет создан с {len(intermediate_data)} записями, включая значения потерь и предсказания.\")\n",
    "\n",
    "\n",
    "# Настройки\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загружаем датасет wikitext-2-raw-v1\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Инициализация токенизатора и модели GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Задаем путь для сохранения промежуточного датасета\n",
    "intermediate_dataset_path = \"intermediate_dataset.json\"\n",
    "\n",
    "# Генерация промежуточного датасета\n",
    "generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device, intermediate_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало подготовки данных...\n",
      "Данные подгружены.\n",
      "Оценка времени подготовки данных: 276.26 секунд для 999 записей.\n",
      "Промежуточный датасет создан с 999 записями, включая значения потерь.\n"
     ]
    }
   ],
   "source": [
    "# Устарело\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device=\"cpu\", intermediate_dataset_path=\"intermediate_dataset.json\"):\n",
    "    intermediate_data = []\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Подготовка данных\n",
    "    print(\"Начало подготовки данных...\")\n",
    "    start_time = time.time()\n",
    "    prepared_data = prepare_data(dataset, max_length=500)\n",
    "    print(\"Данные подгружены.\")\n",
    "    # Оценка времени подготовки данных\n",
    "    preparation_times = []\n",
    "    for i, (context, current_fragment, target) in enumerate(prepared_data[:5]):\n",
    "        if context is None or current_fragment is None:\n",
    "            continue\n",
    "        preparation_times.append(time.time() - start_time)\n",
    "        start_time = time.time()\n",
    "\n",
    "    avg_preparation_time = sum(preparation_times) / len(preparation_times)\n",
    "    estimated_total_time = avg_preparation_time * len(prepared_data)\n",
    "    print(f\"Оценка времени подготовки данных: {estimated_total_time:.2f} секунд для {len(prepared_data)} записей.\")\n",
    "\n",
    "    for i, (context, current_fragment, target) in enumerate(prepared_data):\n",
    "        if context is None or current_fragment is None:\n",
    "            continue\n",
    "\n",
    "        # Подготовка целевого предсказания\n",
    "        target_input = tokenizer(target, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "            target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "        # Предсказание по сырому контексту (предыстория + текущий фрагмент)\n",
    "        raw_inputs = tokenizer(context + \" \" + current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            raw_outputs = gpt2_model(**raw_inputs, output_hidden_states=True)\n",
    "            raw_combined_embedding = raw_outputs.hidden_states[-1].mean(dim=1)\n",
    "            raw_loss = criterion(raw_combined_embedding, target_embedding).item()\n",
    "\n",
    "        # Предсказание только по текущему фрагменту\n",
    "        fragment_input = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            fragment_output = gpt2_model(**fragment_input, output_hidden_states=True)\n",
    "            fragment_embedding = fragment_output.hidden_states[-1].mean(dim=1)\n",
    "            fragment_loss = criterion(fragment_embedding, target_embedding).item()\n",
    "\n",
    "        # Сохранение всех значений и предсказаний\n",
    "        intermediate_data.append(\n",
    "            {\n",
    "                \"context\": context,\n",
    "                \"current_fragment\": current_fragment,\n",
    "                \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                \"raw_loss\": raw_loss,\n",
    "                \"fragment_loss\": fragment_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Сохранение промежуточного датасета в JSON\n",
    "    with open(intermediate_dataset_path, \"w\") as f:\n",
    "        json.dump(intermediate_data, f, indent=4)\n",
    "\n",
    "    print(f\"Промежуточный датасет создан с {len(intermediate_data)} записями, включая значения потерь.\")\n",
    "\n",
    "\n",
    "# Настройки\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загружаем датасет wikitext-2-raw-v1\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Инициализация токенизатора и модели GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)  # Используем 'cpu', можно заменить на 'cuda' при использовании GPU\n",
    "\n",
    "# Задаем путь для сохранения промежуточного датасета\n",
    "intermediate_dataset_path = \"intermediate_dataset.json\"\n",
    "\n",
    "# Генерация промежуточного датасета\n",
    "generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device, intermediate_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
