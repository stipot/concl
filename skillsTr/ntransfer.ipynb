{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "datasets = torchvision.datasets\n",
    "import copy\n",
    "\n",
    "# ---------------------------\n",
    "# Глобальные настройки\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "epochs = 5  # для демо, обычно нужно больше эпох\n",
    "\n",
    "# ---------------------------\n",
    "# Датасет MNIST\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"./data\", train=False, download=False, transform=transform)\n",
    "\n",
    "\n",
    "# Функция для фильтрации датасета по нужным классам\n",
    "def filter_dataset(dataset, classes):\n",
    "    # classes - список допустимых классов\n",
    "    mask = [y in classes for y in dataset.targets]\n",
    "    filtered_data = [(dataset.data[i], dataset.targets[i]) for i, m in enumerate(mask) if m]\n",
    "    data = torch.stack([f[0] for f in filtered_data]).unsqueeze(1).float() / 255.0\n",
    "    targets = torch.tensor([f[1].item() for f in filtered_data])\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "# Функция для создания загрузчика данных\n",
    "def make_loader(data, targets):\n",
    "    ds = torch.utils.data.TensorDataset(data, targets)\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Определим простую модель MLP или небольшую CNN\n",
    "# ---------------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Функции обучения и тестирования\n",
    "# ---------------------------\n",
    "def train_model(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Шаг 1: Обучаем модель A на классы {0,1} (двухклассовая задача)\n",
    "# Для простоты переопределим метки: 0->0, 1->1, остальные не используем\n",
    "# ---------------------------\n",
    "train_data_A, train_targets_A = filter_dataset(train_dataset, [0, 1])\n",
    "test_data_A, test_targets_A = filter_dataset(test_dataset, [0, 1])\n",
    "\n",
    "train_loader_A = make_loader(train_data_A, train_targets_A)\n",
    "test_loader_A = make_loader(test_data_A, test_targets_A)\n",
    "\n",
    "model_A = SimpleCNN(num_classes=2).to(device)\n",
    "optimizer_A = optim.Adam(model_A.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_model(model_A, train_loader_A, optimizer_A, criterion)\n",
    "acc_A = test_model(model_A, test_loader_A)\n",
    "print(\"Accuracy A (0 vs 1):\", acc_A)\n",
    "\n",
    "# ---------------------------\n",
    "# Шаг 2: Создаём модель B из копии A и дообучаем её на классы {0,1,2}\n",
    "# Теперь num_classes=3\n",
    "# ---------------------------\n",
    "model_B = SimpleCNN(num_classes=3).to(device)\n",
    "# Инициализируем B весами A в совместимых слоях:\n",
    "with torch.no_grad():\n",
    "    model_B.conv1.weight.copy_(model_A.conv1.weight)\n",
    "    model_B.conv1.bias.copy_(model_A.conv1.bias)\n",
    "    model_B.conv2.weight.copy_(model_A.conv2.weight)\n",
    "    model_B.conv2.bias.copy_(model_A.conv2.bias)\n",
    "    model_B.fc1.weight.copy_(model_A.fc1.weight)\n",
    "    model_B.fc1.bias.copy_(model_A.fc1.bias)\n",
    "    # fc2 разные размеры, инициализируем заново:\n",
    "    nn.init.xavier_normal_(model_B.fc2.weight)\n",
    "    nn.init.zeros_(model_B.fc2.bias)\n",
    "\n",
    "train_data_B, train_targets_B = filter_dataset(train_dataset, [0, 1, 2])\n",
    "test_data_B, test_targets_B = filter_dataset(test_dataset, [0, 1, 2])\n",
    "train_loader_B = make_loader(train_data_B, train_targets_B)\n",
    "test_loader_B = make_loader(test_data_B, test_targets_B)\n",
    "\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_model(model_B, train_loader_B, optimizer_B, criterion)\n",
    "acc_B = test_model(model_B, test_loader_B)\n",
    "print(\"Accuracy B (0,1,2):\", acc_B)\n",
    "\n",
    "# ---------------------------\n",
    "# Шаг 3: Создаём модель V из копии A и дообучаем на {0,1,3}\n",
    "# ---------------------------\n",
    "model_V = SimpleCNN(num_classes=3).to(device)\n",
    "with torch.no_grad():\n",
    "    model_V.conv1.weight.copy_(model_A.conv1.weight)\n",
    "    model_V.conv1.bias.copy_(model_A.conv1.bias)\n",
    "    model_V.conv2.weight.copy_(model_A.conv2.weight)\n",
    "    model_V.conv2.bias.copy_(model_A.conv2.bias)\n",
    "    model_V.fc1.weight.copy_(model_A.fc1.weight)\n",
    "    model_V.fc1.bias.copy_(model_A.fc1.bias)\n",
    "    nn.init.xavier_normal_(model_V.fc2.weight)\n",
    "    nn.init.zeros_(model_V.fc2.bias)\n",
    "\n",
    "train_data_V, train_targets_V = filter_dataset(train_dataset, [0, 1, 3])\n",
    "test_data_V, test_targets_V = filter_dataset(test_dataset, [0, 1, 3])\n",
    "train_loader_V = make_loader(train_data_V, train_targets_V)\n",
    "test_loader_V = make_loader(test_data_V, test_targets_V)\n",
    "\n",
    "optimizer_V = optim.Adam(model_V.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_model(model_V, train_loader_V, optimizer_V, criterion)\n",
    "acc_V = test_model(model_V, test_loader_V)\n",
    "print(\"Accuracy V (0,1,3):\", acc_V)\n",
    "\n",
    "# ---------------------------\n",
    "# Теперь у нас есть A, B, V.\n",
    "# A -- 0 vs 1\n",
    "# B -- 0,1 vs 2\n",
    "# V -- 0,1 vs 3\n",
    "#\n",
    "# Мы хотим объединить знания для классов {0,1,2,3}:\n",
    "# A' = A + αΔK + βΔS, где ΔK = B - A, ΔS = V - A.\n",
    "#\n",
    "# Для упрощения ограничимся тем, что просто сделаем модель A', в которой\n",
    "# будем параметризовать веса fc2 линейной комбинацией соответствующих слоёв.\n",
    "#\n",
    "# На практике всё сложнее, т.к. размеры менялись для fc2.\n",
    "# Для настоящего примера возьмём только части весов conv1,conv2,fc1.\n",
    "#\n",
    "# В реальной ситуации нужно более аккуратно согласовать архитектуры.\n",
    "# Ниже - упрощённая иллюстрация идеи.\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def get_params_vector(model):\n",
    "    # Получим параметры модели в один список тензоров\n",
    "    return [p.data.clone() for p in model.parameters()]\n",
    "\n",
    "\n",
    "params_A = get_params_vector(model_A)\n",
    "params_B = get_params_vector(model_B)\n",
    "params_V = get_params_vector(model_V)\n",
    "\n",
    "# Вычислим дельты для соответствующих слоёв (кроме последнего слоя fc2, т.к. у них разный размер выхода).\n",
    "# Предположим, мы хотим применить ΔK и ΔS только к общим слоям conv1, conv2, fc1.\n",
    "# А последний слой настроим отдельно или переобучим на малой выборке.\n",
    "\n",
    "\n",
    "def extract_base_layers(model):\n",
    "    return [model.conv1.weight, model.conv1.bias, model.conv2.weight, model.conv2.bias, model.fc1.weight, model.fc1.bias]\n",
    "\n",
    "\n",
    "def get_layer_data(layers):\n",
    "    return [l.data.clone() for l in layers]\n",
    "\n",
    "\n",
    "base_A = get_layer_data(extract_base_layers(model_A))\n",
    "base_B = get_layer_data(extract_base_layers(model_B))\n",
    "base_V = get_layer_data(extract_base_layers(model_V))\n",
    "\n",
    "ΔK = [b - a for b, a in zip(base_B, base_A)]\n",
    "ΔS = [v - a for v, a in zip(base_V, base_A)]\n",
    "\n",
    "# Теперь сделаем параметры α и β обучаемыми и попробуем найти их через минимизацию ошибки на небольшой выборке\n",
    "# из классов {0,1,2,3}.\n",
    "\n",
    "# Соберём данные для финального теста и адаптации\n",
    "final_classes = [0, 1, 2, 3]\n",
    "train_data_F, train_targets_F = filter_dataset(train_dataset, final_classes)\n",
    "test_data_F, test_targets_F = filter_dataset(test_dataset, final_classes)\n",
    "\n",
    "# Возьмём небольшую подвыборку из train_data_F для подгонки α и β:\n",
    "subset_size = 1000\n",
    "train_data_F = train_data_F[:subset_size]\n",
    "train_targets_F = train_targets_F[:subset_size]\n",
    "\n",
    "train_loader_F = make_loader(train_data_F, train_targets_F)\n",
    "test_loader_F = make_loader(test_data_F, test_targets_F)\n",
    "\n",
    "# Определим α и β как параметры PyTorch\n",
    "alpha = torch.nn.Parameter(torch.zeros(1, requires_grad=True, device=device))\n",
    "beta = torch.nn.Parameter(torch.zeros(1, requires_grad=True, device=device))\n",
    "\n",
    "# Определим модель A' - она будет копией A, но веса conv1,conv2,fc1 будут получаться как A + αΔK + βΔS\n",
    "# Последний слой fc2 мы инициализируем заново на 4 класса и будем обучать вместе с α и β.\n",
    "\n",
    "model_A_prime = SimpleCNN(num_classes=4).to(device)\n",
    "# Инициализируем A' весами A для общих слоёв\n",
    "with torch.no_grad():\n",
    "    model_A_prime.conv1.weight.copy_(model_A.conv1.weight)\n",
    "    model_A_prime.conv1.bias.copy_(model_A.conv1.bias)\n",
    "    model_A_prime.conv2.weight.copy_(model_A.conv2.weight)\n",
    "    model_A_prime.conv2.bias.copy_(model_A.conv2.bias)\n",
    "    model_A_prime.fc1.weight.copy_(model_A.fc1.weight)\n",
    "    model_A_prime.fc1.bias.copy_(model_A.fc1.bias)\n",
    "    # fc2 инициализируем заново\n",
    "    nn.init.xavier_normal_(model_A_prime.fc2.weight)\n",
    "    nn.init.zeros_(model_A_prime.fc2.bias)\n",
    "\n",
    "optimizer_F = optim.Adam(list(model_A_prime.parameters()) + [alpha, beta], lr=0.001)\n",
    "\n",
    "\n",
    "def apply_deltas_to_base(model, alpha, beta, base_A, ΔK, ΔS):\n",
    "    # Применим линейную комбинацию к базовым слоям\n",
    "    with torch.no_grad():\n",
    "        layers = extract_base_layers(model)\n",
    "        for l, a_, dk, ds in zip(layers, base_A, ΔK, ΔS):\n",
    "            l.copy_(a_ + alpha * dk + beta * ds)\n",
    "\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model_A_prime.train()\n",
    "    for data, target in train_loader_F:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # применим дельты\n",
    "        apply_deltas_to_base(model_A_prime, alpha, beta, base_A, ΔK, ΔS)\n",
    "\n",
    "        optimizer_F.zero_grad()\n",
    "        output = model_A_prime(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "\n",
    "# Тестируем результат\n",
    "apply_deltas_to_base(model_A_prime, alpha, beta, base_A, ΔK, ΔS)\n",
    "acc_A_prime = test_model(model_A_prime, test_loader_F)\n",
    "print(\"Accuracy A' (0,1,2,3):\", acc_A_prime)\n",
    "print(\"Found alpha:\", alpha.item(), \"beta:\", beta.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy A (0 vs 1): 0.9957446808510638\n",
      "Accuracy B (0 vs 2): 0.9761431411530815\n",
      "Accuracy B' (0,1,2) after skill transfer: 0.9536066094693358\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "# ---------------------------\n",
    "# Датасет MNIST\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, classes):\n",
    "    mask = torch.tensor([y in classes for y in dataset.targets])\n",
    "    data = dataset.data[mask].unsqueeze(1).float() / 255.0\n",
    "    targets = dataset.targets[mask]\n",
    "    # Перекодируем таргеты так, чтобы они шли 0,... для удобства\n",
    "    class_to_new = {c: i for i, c in enumerate(sorted(classes))}\n",
    "    new_targets = torch.tensor([class_to_new[t.item()] for t in targets])\n",
    "    return data, new_targets, class_to_new\n",
    "\n",
    "\n",
    "def make_loader(data, targets):\n",
    "    ds = torch.utils.data.TensorDataset(data, targets)\n",
    "    return torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Автокодировщик для предобучения без учителя\n",
    "# Он будет играть роль бэкбона (энкодер)\n",
    "# ---------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "            nn.Conv2d(16, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "        )\n",
    "        self.fc_enc = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "\n",
    "        self.fc_dec = nn.Linear(latent_dim, 32 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.ReLU(), nn.ConvTranspose2d(16, 1, 4, 2, 1), nn.Sigmoid())  # 14x14  # 28x28\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(h.size(0), 32, 7, 7)\n",
    "        x_recon = self.decoder(h)\n",
    "        return x_recon, z\n",
    "\n",
    "    def encode(self, x):\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(x)\n",
    "            z = z.view(z.size(0), -1)\n",
    "            z = self.fc_enc(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Обучим автокодировщик на всем train_dataset (без меток)\n",
    "full_data = train_dataset.data.unsqueeze(1).float() / 255.0\n",
    "full_targets = train_dataset.targets\n",
    "full_loader = make_loader(full_data, full_targets)\n",
    "\n",
    "autoenc = Autoencoder(latent_dim=64).to(device)\n",
    "optimizer_ae = optim.Adam(autoenc.parameters(), lr=0.001)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    autoenc.train()\n",
    "    for d, t in full_loader:\n",
    "        d = d.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        x_recon, z = autoenc(d)\n",
    "        loss = criterion_ae(x_recon, d)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "\n",
    "# Заморозим энкодер\n",
    "for p in autoenc.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Простейший модуль внимания-классификатор\n",
    "# Он будет иметь набор query-векторов для каждого класса\n",
    "# Предполагается, что мы уже имеем эмбеддинг z от энкодера.\n",
    "# Класс предсказывается по ближайшему query (по косинусной близости или евк. расстоянию)\n",
    "# ---------------------------\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        # query-вектора для каждого класса\n",
    "        self.queries = nn.Parameter(torch.randn(num_classes, embedding_dim) * 0.1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [B, embedding_dim]\n",
    "        # queries: [num_classes, embedding_dim]\n",
    "        # Посчитаем косинусную близость\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        q_norm = self.queries / (self.queries.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        # Similarity: [B, num_classes]\n",
    "        sim = torch.matmul(z_norm, q_norm.t())\n",
    "        # Предсказываем класс с максимальной близостью\n",
    "        return sim\n",
    "\n",
    "\n",
    "def train_classifier(model, loader, enc_model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for data, target in loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = enc_model.encode(data)  # извлекаем признаки\n",
    "        out = model(z)  # [B, num_classes]\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test_classifier(model, loader, enc_model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            z = enc_model.encode(data)\n",
    "            out = model(z)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---------------------------\n",
    "# Обучаем модель A для классов {0,1}\n",
    "# ---------------------------\n",
    "train_data_A, train_targets_A, mapA = filter_dataset(train_dataset, [0, 1])\n",
    "test_data_A, test_targets_A, _ = filter_dataset(test_dataset, [0, 1])\n",
    "train_loader_A = make_loader(train_data_A, train_targets_A)\n",
    "test_loader_A = make_loader(test_data_A, test_targets_A)\n",
    "\n",
    "model_A = AttentionClassifier(embedding_dim=64, num_classes=2).to(device)\n",
    "optimizer_A = optim.Adam(model_A.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_classifier(model_A, train_loader_A, autoenc, optimizer_A, criterion_cls)\n",
    "acc_A = test_classifier(model_A, test_loader_A, autoenc)\n",
    "print(\"Accuracy A (0 vs 1):\", acc_A)\n",
    "\n",
    "# ---------------------------\n",
    "# Обучаем модель B для классов {0,2}\n",
    "# ---------------------------\n",
    "# Аналогично {0,2} -> перекодируем метки: 0->0, 2->1 для обучения\n",
    "train_data_B, train_targets_B, mapB = filter_dataset(train_dataset, [0, 2])\n",
    "test_data_B, test_targets_B, _ = filter_dataset(test_dataset, [0, 2])\n",
    "train_loader_B = make_loader(train_data_B, train_targets_B)\n",
    "test_loader_B = make_loader(test_data_B, test_targets_B)\n",
    "\n",
    "model_B = AttentionClassifier(embedding_dim=64, num_classes=2).to(device)\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_classifier(model_B, train_loader_B, autoenc, optimizer_B, criterion_cls)\n",
    "acc_B = test_classifier(model_B, test_loader_B, autoenc)\n",
    "print(\"Accuracy B (0 vs 2):\", acc_B)\n",
    "\n",
    "# ---------------------------\n",
    "# Теперь у нас есть:\n",
    "# Модель A знает классы {0,1}\n",
    "# Модель B знает классы {0,2}\n",
    "\n",
    "# Мы хотим перенести навык распознавания \"1\" в модель B.\n",
    "# Сейчас модель B имеет queries для [0,2], а модель A для [0,1].\n",
    "# Как это сделать?\n",
    "# Возьмём query-вектор для класса \"1\" из model_A:\n",
    "# mapA: {0->0, 1->1}\n",
    "# mapB: {0->0, 2->1}\n",
    "\n",
    "# Нам нужно расширить модель B до 3 классов: {0,1,2}\n",
    "# Но в текущей реализации model_B заточен под 2 класса.\n",
    "# Сделаем новую модель B' с 3 классами и перенесём параметры.\n",
    "# Новый класс (1) будет взят из model_A.\n",
    "\n",
    "# Упростим: создадим новую модель B', в неё скопируем query для 0 и 2 из model_B и добавим query для 1 из model_A.\n",
    "\n",
    "model_B_extended = AttentionClassifier(embedding_dim=64, num_classes=3).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # model_B.queries[0] = класс 0\n",
    "    # model_B.queries[1] = класс 2 (в mapB)\n",
    "    # model_A.queries[1] = класс 1 (в mapA)\n",
    "    # Нам надо понять, куда поместить какой query:\n",
    "    # Порядок классов в новой модели B': [0,1,2]\n",
    "    # для 0 - берём model_B.queries[mapB[0]]=model_B.queries[0]\n",
    "    model_B_extended.queries[0].copy_(model_B.queries[0])\n",
    "    # для 1 - берём из model_A класс 1: model_A.queries[1]\n",
    "    model_B_extended.queries[1].copy_(model_A.queries[1])\n",
    "    # для 2 - берём из model_B класс 2 -> mapB[2]=1, значит model_B.queries[1]\n",
    "    model_B_extended.queries[2].copy_(model_B.queries[1])\n",
    "\n",
    "# Проверим точность новой модели B' на тесте с классами {0,1,2}.\n",
    "# Чтобы протестировать, нам нужен тестовый набор с {0,1,2}:\n",
    "test_data_F, test_targets_F, mapF = filter_dataset(test_dataset, [0, 1, 2])\n",
    "test_loader_F = make_loader(test_data_F, test_targets_F)\n",
    "\n",
    "acc_B_before = 0.0  # модель B_extended без дообучения уже может попытаться классифицировать {0,1,2}, но класс 1 она знает только по query от A\n",
    "acc_B_after = test_classifier(model_B_extended, test_loader_F, autoenc)\n",
    "\n",
    "print(\"Accuracy B' (0,1,2) after skill transfer:\", acc_B_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy A (0 vs 1): 0.9966903073286052\n",
      "Accuracy B (0 vs 2): 0.9756461232604374\n",
      "Accuracy B' (0,1,2) after skill transfer (before finetuning encoder): 0.9482046393390531\n",
      "Accuracy before fine-tuning the backbone: 0.9482046393390531\n",
      "Accuracy after fine-tuning the backbone with fixed attention: 0.9914204003813155\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "# ---------------------------\n",
    "# Датасет MNIST\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, classes):\n",
    "    mask = torch.tensor([y in classes for y in dataset.targets])\n",
    "    data = dataset.data[mask].unsqueeze(1).float() / 255.0\n",
    "    targets = dataset.targets[mask]\n",
    "    class_to_new = {c: i for i, c in enumerate(sorted(classes))}\n",
    "    new_targets = torch.tensor([class_to_new[t.item()] for t in targets])\n",
    "    return data, new_targets, class_to_new\n",
    "\n",
    "\n",
    "def make_loader(data, targets):\n",
    "    ds = torch.utils.data.TensorDataset(data, targets)\n",
    "    return torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Автокодировщик для предобучения без учителя\n",
    "# ---------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "            nn.Conv2d(16, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "        )\n",
    "        self.fc_enc = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "\n",
    "        self.fc_dec = nn.Linear(latent_dim, 32 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.ReLU(), nn.ConvTranspose2d(16, 1, 4, 2, 1), nn.Sigmoid())  # 14x14  # 28x28\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(h.size(0), 32, 7, 7)\n",
    "        x_recon = self.decoder(h)\n",
    "        return x_recon, z\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Без torch.no_grad() - теперь градиенты будут считатьcя\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Обучим автокодировщик на всем train_dataset (без меток)\n",
    "full_data = train_dataset.data.unsqueeze(1).float() / 255.0\n",
    "full_targets = train_dataset.targets\n",
    "full_loader = make_loader(full_data, full_targets)\n",
    "\n",
    "autoenc = Autoencoder(latent_dim=64).to(device)\n",
    "optimizer_ae = optim.Adam(autoenc.parameters(), lr=0.001)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    autoenc.train()\n",
    "    for d, t in full_loader:\n",
    "        d = d.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        x_recon, z = autoenc(d)\n",
    "        loss = criterion_ae(x_recon, d)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "\n",
    "# Заморозим энкодер для использования как бэкбон\n",
    "for p in autoenc.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Простейший модуль внимания-классификатор\n",
    "# ---------------------------\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        # query-вектора для каждого класса\n",
    "        self.queries = nn.Parameter(torch.randn(num_classes, embedding_dim) * 0.1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: [B, embedding_dim]\n",
    "        # queries: [num_classes, embedding_dim]\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        q_norm = self.queries / (self.queries.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        sim = torch.matmul(z_norm, q_norm.t())  # [B, num_classes]\n",
    "        return sim\n",
    "\n",
    "\n",
    "def train_classifier(model, loader, enc_model, optimizer, criterion):\n",
    "    model.train()\n",
    "    enc_model.eval()  # энкодер заморожен, eval для консистентности\n",
    "    for data, target in loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = enc_model.encode(data)\n",
    "        out = model(z)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test_classifier(model, loader, enc_model):\n",
    "    model.eval()\n",
    "    enc_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            z = enc_model.encode(data)\n",
    "            out = model(z)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---------------------------\n",
    "# Обучаем модель A для классов {0,1}\n",
    "# ---------------------------\n",
    "train_data_A, train_targets_A, mapA = filter_dataset(train_dataset, [0, 1])\n",
    "test_data_A, test_targets_A, _ = filter_dataset(test_dataset, [0, 1])\n",
    "train_loader_A = make_loader(train_data_A, train_targets_A)\n",
    "test_loader_A = make_loader(test_data_A, test_targets_A)\n",
    "\n",
    "model_A = AttentionClassifier(embedding_dim=64, num_classes=2).to(device)\n",
    "optimizer_A = optim.Adam(model_A.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_classifier(model_A, train_loader_A, autoenc, optimizer_A, criterion_cls)\n",
    "acc_A = test_classifier(model_A, test_loader_A, autoenc)\n",
    "print(\"Accuracy A (0 vs 1):\", acc_A)\n",
    "\n",
    "# ---------------------------\n",
    "# Обучаем модель B для классов {0,2}\n",
    "# ---------------------------\n",
    "train_data_B, train_targets_B, mapB = filter_dataset(train_dataset, [0, 2])\n",
    "test_data_B, test_targets_B, _ = filter_dataset(test_dataset, [0, 2])\n",
    "train_loader_B = make_loader(train_data_B, train_targets_B)\n",
    "test_loader_B = make_loader(test_data_B, test_targets_B)\n",
    "\n",
    "model_B = AttentionClassifier(embedding_dim=64, num_classes=2).to(device)\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_classifier(model_B, train_loader_B, autoenc, optimizer_B, criterion_cls)\n",
    "acc_B = test_classifier(model_B, test_loader_B, autoenc)\n",
    "print(\"Accuracy B (0 vs 2):\", acc_B)\n",
    "\n",
    "# ---------------------------\n",
    "# Перенос навыка: формируем B' для {0,1,2}\n",
    "# ---------------------------\n",
    "# В model_A: классы {0->0,1->1}\n",
    "# В model_B: классы {0->0,2->1}\n",
    "# В новой модели: {0,1,2} -> indices: 0->0, 1->1, 2->2\n",
    "model_B_extended = AttentionClassifier(embedding_dim=64, num_classes=3).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Класс 0 для B': берем из model_B (класс 0 в mapB это index=0)\n",
    "    model_B_extended.queries[0].copy_(model_B.queries[0])\n",
    "    # Класс 1 для B': берем из model_A (класс 1 в mapA это index=1)\n",
    "    model_B_extended.queries[1].copy_(model_A.queries[1])\n",
    "    # Класс 2 для B': берем из model_B (класс 2 в mapB это index=1)\n",
    "    model_B_extended.queries[2].copy_(model_B.queries[1])\n",
    "\n",
    "test_data_F, test_targets_F, mapF = filter_dataset(test_dataset, [0, 1, 2])\n",
    "test_loader_F = make_loader(test_data_F, test_targets_F)\n",
    "acc_B_before = test_classifier(model_B_extended, test_loader_F, autoenc)\n",
    "print(\"Accuracy B' (0,1,2) after skill transfer (before finetuning encoder):\", acc_B_before)\n",
    "\n",
    "# ---------------------------\n",
    "# Дообучение энкодера при фиксированном внимании.\n",
    "# Заморозим queries в model_B_extended\n",
    "# Разморозим энкодер частично (только encoder и fc_enc),\n",
    "# decoder можно не трогать или оставить замороженным.\n",
    "# ---------------------------\n",
    "# Заморозим queries\n",
    "model_B_extended.queries.requires_grad = False\n",
    "\n",
    "# Разморозим энкодер, заморозим декодер\n",
    "for name, p in autoenc.named_parameters():\n",
    "    if \"decoder\" in name or \"fc_dec\" in name:\n",
    "        p.requires_grad = False\n",
    "    else:\n",
    "        p.requires_grad = True\n",
    "\n",
    "# Оптимизируем только энкодер\n",
    "optimizer_enc = optim.Adam([p for p in autoenc.parameters() if p.requires_grad], lr=0.0005)\n",
    "\n",
    "\n",
    "def train_backbone_with_fixed_attention(model, enc_model, loader, optimizer, criterion):\n",
    "    # Модель внимания не обучаем, только энкодер\n",
    "    model.eval()  # фиксируем внимание\n",
    "    enc_model.train()\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = enc_model.encode(data)  # теперь z должно иметь grad_fn\n",
    "        out = model(z)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Проверим точность до дообучения\n",
    "acc_before_fine = test_classifier(model_B_extended, test_loader_F, autoenc)\n",
    "print(\"Accuracy before fine-tuning the backbone:\", acc_before_fine)\n",
    "\n",
    "# Выполним несколько эпох обучения энкодера\n",
    "for ep in range(3):\n",
    "    train_backbone_with_fixed_attention(model_B_extended, autoenc, train_loader_F, optimizer_enc, criterion_cls)\n",
    "\n",
    "acc_after = test_classifier(model_B_extended, test_loader_F, autoenc)\n",
    "print(\"Accuracy after fine-tuning the backbone with fixed attention:\", acc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy A (0 vs 1): 0.9962174940898345\n",
      "Accuracy B (0 vs 2): 0.9801192842942346\n",
      "Accuracy B' (0,1,2) after skill transfer (before finetuning encoder): 0.9590085795996187\n",
      "Accuracy before fine-tuning the backbone: 0.9590085795996187\n",
      "Accuracy after fine-tuning the backbone with fixed attention: 0.9895138226882746\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "AE Loss",
         "type": "scatter",
         "xaxis": "x",
         "y": [
          0.03351294934203227,
          0.008208215574920177,
          0.005768895895282428,
          0.0047611064891020455,
          0.004173021551221609
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines+markers",
         "name": "A Accuracy (train)",
         "type": "scatter",
         "xaxis": "x2",
         "y": [
          0.9026450848795894,
          0.9907619423608369,
          0.9928938018160284,
          0.9938412949072246,
          0.9939202526648243
         ],
         "yaxis": "y2"
        },
        {
         "mode": "lines+markers",
         "name": "B Accuracy (train)",
         "type": "scatter",
         "xaxis": "x3",
         "y": [
          0.8378924332968606,
          0.9539601043683191,
          0.9709620402323037,
          0.9746654322026765,
          0.9764329601885363
         ],
         "yaxis": "y3"
        },
        {
         "mode": "lines+markers",
         "name": "Encoder finetune Acc (train)",
         "type": "scatter",
         "xaxis": "x4",
         "y": [
          0.9685335338022875,
          0.9799710035977017,
          0.9864683455941577
         ],
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "AE Loss",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Model A Accuracy",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Model B Accuracy",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Fine-tuning Accuracy (encoder)",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training Convergence Analysis"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.375
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.375
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "# ---------------------------\n",
    "# Датасет MNIST\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, classes):\n",
    "    mask = torch.tensor([y in classes for y in dataset.targets])\n",
    "    data = dataset.data[mask].unsqueeze(1).float() / 255.0\n",
    "    targets = dataset.targets[mask]\n",
    "    class_to_new = {c: i for i, c in enumerate(sorted(classes))}\n",
    "    new_targets = torch.tensor([class_to_new[t.item()] for t in targets])\n",
    "    return data, new_targets, class_to_new\n",
    "\n",
    "\n",
    "def make_loader(data, targets):\n",
    "    ds = torch.utils.data.TensorDataset(data, targets)\n",
    "    return torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Автокодировщик для предобучения без учителя\n",
    "# ---------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14\n",
    "            nn.Conv2d(16, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 7x7\n",
    "        )\n",
    "        self.fc_enc = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "\n",
    "        self.fc_dec = nn.Linear(latent_dim, 32 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.ReLU(), nn.ConvTranspose2d(16, 1, 4, 2, 1), nn.Sigmoid())  # 14x14  # 28x28\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(h.size(0), 32, 7, 7)\n",
    "        x_recon = self.decoder(h)\n",
    "        return x_recon, z\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Полный MNIST для автокодировщика\n",
    "full_data = train_dataset.data.unsqueeze(1).float() / 255.0\n",
    "full_targets = train_dataset.targets\n",
    "full_loader = make_loader(full_data, full_targets)\n",
    "\n",
    "autoenc = Autoencoder(latent_dim=64).to(device)\n",
    "optimizer_ae = optim.Adam(autoenc.parameters(), lr=0.001)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "# Лог для автокодировщика\n",
    "ae_loss_log = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    autoenc.train()\n",
    "    running_loss = 0.0\n",
    "    for d, t in full_loader:\n",
    "        d = d.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        x_recon, z = autoenc(d)\n",
    "        loss = criterion_ae(x_recon, d)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        running_loss += loss.item() * d.size(0)\n",
    "    ae_loss_log.append(running_loss / len(full_loader.dataset))\n",
    "\n",
    "# Заморозим энкодер\n",
    "for p in autoenc.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Attention Classifier\n",
    "# ---------------------------\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.queries = nn.Parameter(torch.randn(num_classes, embedding_dim) * 0.1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        q_norm = self.queries / (self.queries.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        sim = torch.matmul(z_norm, q_norm.t())\n",
    "        return sim\n",
    "\n",
    "\n",
    "def train_classifier(model, loader, enc_model, optimizer, criterion):\n",
    "    model.train()\n",
    "    enc_model.eval()\n",
    "    running_acc = 0.0\n",
    "    total = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = enc_model.encode(data)\n",
    "        out = model(z)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred == target).sum().item()\n",
    "        running_acc += correct\n",
    "        total += target.size(0)\n",
    "    return running_acc / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def test_classifier(model, loader, enc_model):\n",
    "    model.eval()\n",
    "    enc_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            z = enc_model.encode(data)\n",
    "            out = model(z)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---------------------------\n",
    "# Обучаем модель A: {0 vs 1}\n",
    "# ---------------------------\n",
    "train_data_A, train_targets_A, mapA = filter_dataset(train_dataset, [0, 1])\n",
    "test_data_A, test_targets_A, _ = filter_dataset(test_dataset, [0, 1])\n",
    "train_loader_A = make_loader(train_data_A, train_targets_A)\n",
    "test_loader_A = make_loader(test_data_A, test_targets_A)\n",
    "\n",
    "model_A = AttentionClassifier(embedding_dim=64, num_classes=2).to(device)\n",
    "optimizer_A = optim.Adam(model_A.parameters(), lr=0.001)\n",
    "acc_A_log = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    acc_ep = train_classifier(model_A, train_loader_A, autoenc, optimizer_A, criterion_cls)\n",
    "    acc_A_log.append(acc_ep)\n",
    "acc_A = test_classifier(model_A, test_loader_A, autoenc)\n",
    "\n",
    "print(\"Accuracy A (0 vs 1):\", acc_A)\n",
    "\n",
    "# ---------------------------\n",
    "# Обучаем модель B: {0 vs 2}\n",
    "# ---------------------------\n",
    "train_data_B, train_targets_B, mapB = filter_dataset(train_dataset, [0, 2])\n",
    "test_data_B, test_targets_B, _ = filter_dataset(test_dataset, [0, 2])\n",
    "train_loader_B = make_loader(train_data_B, train_targets_B)\n",
    "test_loader_B = make_loader(test_data_B, test_targets_B)\n",
    "\n",
    "model_B = AttentionClassifier(embedding_dim=64, num_classes=2).to(device)\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr=0.001)\n",
    "acc_B_log = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    acc_ep = train_classifier(model_B, train_loader_B, autoenc, optimizer_B, criterion_cls)\n",
    "    acc_B_log.append(acc_ep)\n",
    "\n",
    "acc_B = test_classifier(model_B, test_loader_B, autoenc)\n",
    "print(\"Accuracy B (0 vs 2):\", acc_B)\n",
    "\n",
    "# ---------------------------\n",
    "# Перенос навыка: формируем B' для {0,1,2}\n",
    "# ---------------------------\n",
    "model_B_extended = AttentionClassifier(embedding_dim=64, num_classes=3).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_B_extended.queries[0].copy_(model_B.queries[0])  # класс 0 из B\n",
    "    model_B_extended.queries[1].copy_(model_A.queries[1])  # класс 1 из A\n",
    "    model_B_extended.queries[2].copy_(model_B.queries[1])  # класс 2 из B\n",
    "\n",
    "test_data_F, test_targets_F, mapF = filter_dataset(test_dataset, [0, 1, 2])\n",
    "test_loader_F = make_loader(test_data_F, test_targets_F)\n",
    "acc_B_before = test_classifier(model_B_extended, test_loader_F, autoenc)\n",
    "print(\"Accuracy B' (0,1,2) after skill transfer (before finetuning encoder):\", acc_B_before)\n",
    "print(\"Accuracy before fine-tuning the backbone:\", acc_B_before)\n",
    "\n",
    "# ---------------------------\n",
    "# Дообучение энкодера при фиксированном внимании\n",
    "# ---------------------------\n",
    "model_B_extended.queries.requires_grad = False\n",
    "for name, p in autoenc.named_parameters():\n",
    "    if \"decoder\" in name or \"fc_dec\" in name:\n",
    "        p.requires_grad = False\n",
    "    else:\n",
    "        p.requires_grad = True\n",
    "\n",
    "optimizer_enc = optim.Adam([p for p in autoenc.parameters() if p.requires_grad], lr=0.0005)\n",
    "\n",
    "\n",
    "def train_backbone_with_fixed_attention(model, enc_model, loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    enc_model.train()\n",
    "    running_acc = 0.0\n",
    "    total = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = enc_model.encode(data)\n",
    "        out = model(z)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred == target).sum().item()\n",
    "        running_acc += correct\n",
    "        total += target.size(0)\n",
    "    return running_acc / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "acc_finetune_log = []\n",
    "for ep in range(3):\n",
    "    acc_ep = train_backbone_with_fixed_attention(model_B_extended, autoenc, train_loader_F, optimizer_enc, criterion_cls)\n",
    "    acc_finetune_log.append(acc_ep)\n",
    "\n",
    "acc_after = test_classifier(model_B_extended, test_loader_F, autoenc)\n",
    "print(\"Accuracy after fine-tuning the backbone with fixed attention:\", acc_after)\n",
    "\n",
    "# ---------------------------\n",
    "# Плоттинг результатов с помощью Plotly\n",
    "# ---------------------------\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\"AE Loss\", \"Model A Accuracy\", \"Model B Accuracy\", \"Fine-tuning Accuracy (encoder)\"))\n",
    "\n",
    "# AE Loss\n",
    "fig.add_trace(go.Scatter(y=ae_loss_log, mode=\"lines+markers\", name=\"AE Loss\"), row=1, col=1)\n",
    "\n",
    "# Model A Accuracy\n",
    "fig.add_trace(go.Scatter(y=acc_A_log, mode=\"lines+markers\", name=\"A Accuracy (train)\"), row=1, col=2)\n",
    "\n",
    "# Model B Accuracy\n",
    "fig.add_trace(go.Scatter(y=acc_B_log, mode=\"lines+markers\", name=\"B Accuracy (train)\"), row=2, col=1)\n",
    "\n",
    "# Fine-tuning accuracy\n",
    "fig.add_trace(go.Scatter(y=acc_finetune_log, mode=\"lines+markers\", name=\"Encoder finetune Acc (train)\"), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=600, width=800, title_text=\"Training Convergence Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model accuracy on all digits (0-9): 0.6931\n",
      "Error rate: 0.30689999999999995\n",
      "Class 0 accuracy: 0.9816\n",
      "Class 1 accuracy: 0.8106\n",
      "Class 2 accuracy: 0.5494\n",
      "Class 3 accuracy: 0.7248\n",
      "Class 4 accuracy: 0.7668\n",
      "Class 5 accuracy: 0.4271\n",
      "Class 6 accuracy: 0.6221\n",
      "Class 7 accuracy: 0.7860\n",
      "Class 8 accuracy: 0.5893\n",
      "Class 9 accuracy: 0.6323\n",
      "Model architecture:\n",
      "FullAttentionClassifier()\n",
      "Total number of parameters in the final model: 640\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "AE Loss",
         "type": "scatter",
         "xaxis": "x",
         "y": [
          0.03529694761087497,
          0.008248695499449968,
          0.0058768400957187016
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines+markers",
         "name": "0 vs 1 train acc",
         "type": "scatter",
         "xaxis": "x2",
         "y": [
          0.9391235688906435,
          0.9944729569680221,
          0.9938412949072246
         ],
         "yaxis": "y2"
        },
        {
         "mode": "lines+markers",
         "name": "0 vs 2 train acc",
         "type": "scatter",
         "xaxis": "x3",
         "y": [
          0.8308223213534214,
          0.9587576803299386,
          0.9647336082821312
         ],
         "yaxis": "y3"
        },
        {
         "mode": "lines+markers",
         "name": "0 vs 9 train acc",
         "type": "scatter",
         "xaxis": "x3",
         "y": [
          0.8904986522911051,
          0.9692553908355795,
          0.9754043126684636
         ],
         "yaxis": "y3"
        },
        {
         "mode": "markers",
         "name": "Full Model Final Acc",
         "type": "scatter",
         "x": [
          0
         ],
         "xaxis": "x4",
         "y": [
          0.6931
         ],
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "AE Loss",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "0 vs 1 Accuracy",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "0 vs i Accuracy examples",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Full model accuracy not tracked by epoch but final",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Final Model Analysis"
        },
        "width": 900,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.375
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.375
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epochs = 3  # можно увеличить для лучшей точности\n",
    "\n",
    "# ---------------------------\n",
    "# Датасет MNIST\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, classes):\n",
    "    mask = torch.tensor([y in classes for y in dataset.targets])\n",
    "    data = dataset.data[mask].unsqueeze(1).float() / 255.0\n",
    "    targets = dataset.targets[mask]\n",
    "    class_to_new = {c: i for i, c in enumerate(sorted(classes))}\n",
    "    new_targets = torch.tensor([class_to_new[t.item()] for t in targets])\n",
    "    return data, new_targets, class_to_new\n",
    "\n",
    "\n",
    "def make_loader(data, targets):\n",
    "    ds = torch.utils.data.TensorDataset(data, targets)\n",
    "    return torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Автокодировщик для предобучения без учителя (backbone)\n",
    "# ---------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc_enc = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 32 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.ReLU(), nn.ConvTranspose2d(16, 1, 4, 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(h.size(0), 32, 7, 7)\n",
    "        x_recon = self.decoder(h)\n",
    "        return x_recon, z\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "full_data = train_dataset.data.unsqueeze(1).float() / 255.0\n",
    "full_targets = train_dataset.targets\n",
    "full_loader = make_loader(full_data, full_targets)\n",
    "\n",
    "autoenc = Autoencoder(latent_dim=64).to(device)\n",
    "optimizer_ae = optim.Adam(autoenc.parameters(), lr=0.001)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "# Лог для автокодировщика\n",
    "ae_loss_log = []\n",
    "for ep in range(epochs):\n",
    "    autoenc.train()\n",
    "    running_loss = 0.0\n",
    "    for d, t in full_loader:\n",
    "        d = d.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        x_recon, z = autoenc(d)\n",
    "        loss = criterion_ae(x_recon, d)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        running_loss += loss.item() * d.size(0)\n",
    "    ae_loss_log.append(running_loss / len(full_loader.dataset))\n",
    "\n",
    "# Заморозим энкодер\n",
    "for p in autoenc.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Attention Classifier\n",
    "# ---------------------------\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.queries = nn.Parameter(torch.randn(num_classes, embedding_dim) * 0.1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        q_norm = self.queries / (self.queries.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        sim = torch.matmul(z_norm, q_norm.t())\n",
    "        return sim\n",
    "\n",
    "\n",
    "def train_classifier(model, loader, enc_model, optimizer, criterion):\n",
    "    model.train()\n",
    "    enc_model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = enc_model.encode(data)\n",
    "        out = model(z)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += target.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def test_classifier(model, loader, enc_model):\n",
    "    model.eval()\n",
    "    enc_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred_all = []\n",
    "    target_all = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            z = enc_model.encode(data)\n",
    "            out = model(z)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            pred_all.append(pred.cpu())\n",
    "            target_all.append(target.cpu())\n",
    "    if total > 0:\n",
    "        pred_all = torch.cat(pred_all)\n",
    "        target_all = torch.cat(target_all)\n",
    "        return (correct / total), pred_all, target_all\n",
    "    else:\n",
    "        return 0.0, None, None\n",
    "\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# Мы будем получать навыки для каждого класса i=1..9 от модели {0 vs i}\n",
    "# Первую модель {0 vs 1} используем для инициализации класса 0 и класса 1.\n",
    "# Затем для {0 vs 2}, {0 vs 3}, ... извлекаем навыки для других классов.\n",
    "\n",
    "queries_dict = {}  # dict: class_digit -> query_vector\n",
    "acc_logs = {}\n",
    "acc_train_logs = {}\n",
    "\n",
    "# Сначала модель для {0 vs 1}\n",
    "train_data_01, train_targets_01, _ = filter_dataset(train_dataset, [0, 1])\n",
    "test_data_01, test_targets_01, _ = filter_dataset(test_dataset, [0, 1])\n",
    "train_loader_01 = make_loader(train_data_01, train_targets_01)\n",
    "test_loader_01 = make_loader(test_data_01, test_targets_01)\n",
    "\n",
    "model_01 = AttentionClassifier(embedding_dim=64, num_classes=2).to(device)\n",
    "optimizer_01 = optim.Adam(model_01.parameters(), lr=0.001)\n",
    "\n",
    "acc_01_log = []\n",
    "for ep in range(epochs):\n",
    "    acc_ep = train_classifier(model_01, train_loader_01, autoenc, optimizer_01, criterion_cls)\n",
    "    acc_01_log.append(acc_ep)\n",
    "acc_01, _, _ = test_classifier(model_01, test_loader_01, autoenc)\n",
    "\n",
    "# Сохраняем queries для 0 и 1\n",
    "with torch.no_grad():\n",
    "    queries_dict[0] = model_01.queries[0].clone()  # класс 0\n",
    "    queries_dict[1] = model_01.queries[1].clone()  # класс 1\n",
    "\n",
    "acc_logs[(0, 1)] = acc_01\n",
    "acc_train_logs[(0, 1)] = acc_01_log\n",
    "\n",
    "# Теперь для всех остальных классов {0 vs i}, i=2..9\n",
    "for i in range(2, 10):\n",
    "    train_data_i, train_targets_i, _ = filter_dataset(train_dataset, [0, i])\n",
    "    test_data_i, test_targets_i, _ = filter_dataset(test_dataset, [0, i])\n",
    "    train_loader_i = make_loader(train_data_i, train_targets_i)\n",
    "    test_loader_i = make_loader(test_data_i, test_targets_i)\n",
    "\n",
    "    model_i = AttentionClassifier(embedding_dim=64, num_classes=2).to(device)\n",
    "    optimizer_i = optim.Adam(model_i.parameters(), lr=0.001)\n",
    "\n",
    "    acc_i_log = []\n",
    "    for ep in range(epochs):\n",
    "        acc_ep = train_classifier(model_i, train_loader_i, autoenc, optimizer_i, criterion_cls)\n",
    "        acc_i_log.append(acc_ep)\n",
    "    acc_i, _, _ = test_classifier(model_i, test_loader_i, autoenc)\n",
    "\n",
    "    # Сохраняем query для класса i (класс 0 уже есть)\n",
    "    with torch.no_grad():\n",
    "        # model_i.queries[0] - это класс 0, model_i.queries[1] - это класс i\n",
    "        queries_dict[i] = model_i.queries[1].clone()\n",
    "\n",
    "    acc_logs[(0, i)] = acc_i\n",
    "    acc_train_logs[(0, i)] = acc_i_log\n",
    "\n",
    "\n",
    "# Теперь у нас есть queries_dict для всех классов 0..9\n",
    "# Соберём итоговую модель из 10 классов:\n",
    "class FullAttentionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, queries_dict):\n",
    "        super(FullAttentionClassifier, self).__init__()\n",
    "        # queries_dict - словарь: digit -> vector\n",
    "        # Соберем в один тензор:\n",
    "        queries_list = [queries_dict[d] for d in range(10)]\n",
    "        queries_tensor = torch.stack(queries_list, dim=0)\n",
    "        # queries - Parameter, чтобы быть в духе, но их можно и зафиксировать\n",
    "        self.queries = nn.Parameter(queries_tensor)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        q_norm = self.queries / (self.queries.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        sim = torch.matmul(z_norm, q_norm.t())\n",
    "        return sim\n",
    "\n",
    "\n",
    "model_full = FullAttentionClassifier(embedding_dim=64, queries_dict=queries_dict).to(device)\n",
    "\n",
    "# Проверим точность на полном тесте MNIST (все классы 0..9)\n",
    "test_loader_full = make_loader(test_dataset.data.unsqueeze(1).float() / 255.0, test_dataset.targets)\n",
    "acc_full, pred_all, target_all = test_classifier(model_full, test_loader_full, autoenc)\n",
    "error_rate = 1 - acc_full\n",
    "\n",
    "print(\"Full model accuracy on all digits (0-9):\", acc_full)\n",
    "print(\"Error rate:\", error_rate)\n",
    "\n",
    "# Подсчитаем точность по каждому классу отдельно:\n",
    "if pred_all is not None and target_all is not None:\n",
    "    class_correct = [0] * 10\n",
    "    class_total = [0] * 10\n",
    "    for p, t in zip(pred_all, target_all):\n",
    "        class_total[t.item()] += 1\n",
    "        if p.item() == t.item():\n",
    "            class_correct[t.item()] += 1\n",
    "\n",
    "    for d in range(10):\n",
    "        if class_total[d] > 0:\n",
    "            print(f\"Class {d} accuracy: {class_correct[d]/class_total[d]:.4f}\")\n",
    "        else:\n",
    "            print(f\"Class {d} no samples in test?\")\n",
    "\n",
    "# Выведем архитектуру и количество параметров итоговой сети\n",
    "print(\"Model architecture:\")\n",
    "print(model_full)\n",
    "num_params = sum(p.numel() for p in model_full.parameters())\n",
    "print(\"Total number of parameters in the final model:\", num_params)\n",
    "\n",
    "# Отобразим графики сходимости автокодировщика и точности обучения пар {0 vs i}\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\"AE Loss\", \"0 vs 1 Accuracy\", \"0 vs i Accuracy examples\", \"Full model accuracy not tracked by epoch but final\"))\n",
    "\n",
    "# AE Loss\n",
    "fig.add_trace(go.Scatter(y=ae_loss_log, mode=\"lines+markers\", name=\"AE Loss\"), row=1, col=1)\n",
    "\n",
    "# Accuracy 0 vs 1\n",
    "fig.add_trace(go.Scatter(y=acc_train_logs[(0, 1)], mode=\"lines+markers\", name=\"0 vs 1 train acc\"), row=1, col=2)\n",
    "\n",
    "# Покажем для примера {0 vs 2} и {0 vs 9}\n",
    "if (0, 2) in acc_train_logs:\n",
    "    fig.add_trace(go.Scatter(y=acc_train_logs[(0, 2)], mode=\"lines+markers\", name=\"0 vs 2 train acc\"), row=2, col=1)\n",
    "if (0, 9) in acc_train_logs:\n",
    "    fig.add_trace(go.Scatter(y=acc_train_logs[(0, 9)], mode=\"lines+markers\", name=\"0 vs 9 train acc\"), row=2, col=1)\n",
    "\n",
    "# У нас нет эпох для full model (она была собрана из кусочков), просто покажем итог:\n",
    "fig.add_trace(go.Scatter(x=[0], y=[acc_full], mode=\"markers\", name=\"Full Model Final Acc\"), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=600, width=900, title_text=\"Training and Final Model Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in ./data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     21\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     22\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m     23\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m     24\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     25\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m     26\u001b[0m ])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Загрузка ImageNet\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageNet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_classes\u001b[39m(dataset, num_classes, min_samples):\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:53\u001b[0m, in \u001b[0;36mImageNet.__init__\u001b[1;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(root)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m verify_str_arg(split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_archives\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m wnid_to_classes \u001b[38;5;241m=\u001b[39m load_meta_file(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:66\u001b[0m, in \u001b[0;36mImageNet.parse_archives\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_archives\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, META_FILE)):\n\u001b[1;32m---> 66\u001b[0m         \u001b[43mparse_devkit_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder):\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:147\u001b[0m, in \u001b[0;36mparse_devkit_archive\u001b[1;34m(root, file)\u001b[0m\n\u001b[0;32m    144\u001b[0m     file \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    145\u001b[0m md5 \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 147\u001b[0m \u001b[43m_verify_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_tmp_dir() \u001b[38;5;28;01mas\u001b[39;00m tmp_dir:\n\u001b[0;32m    150\u001b[0m     extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), tmp_dir)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:103\u001b[0m, in \u001b[0;36m_verify_archive\u001b[1;34m(root, file, md5)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), md5):\n\u001b[0;32m     99\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe archive \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not present in the root directory or is corrupted. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to download it externally and place it in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m     )\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(file, root))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in ./data."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Параметры\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "num_classes = 10  # Первые 10 классов\n",
    "min_samples_per_class = 1000\n",
    "max_test_samples = 50000\n",
    "\n",
    "# ---------------------------\n",
    "# Подготовка датасета ImageNet\n",
    "# ---------------------------\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "# Загрузка ImageNet\n",
    "train_dataset = datasets.ImageNet(\"./data\", split=\"train\", download=True, transform=transform)\n",
    "test_dataset = datasets.ImageNet(\"./data\", split=\"val\", download=True, transform=transform)\n",
    "\n",
    "\n",
    "def filter_classes(dataset, num_classes, min_samples):\n",
    "    \"\"\"\n",
    "    Фильтрует датасет, оставляя только `num_classes` классов и минимум `min_samples` образцов на класс.\n",
    "    \"\"\"\n",
    "    class_indices = {cls: [] for cls in range(num_classes)}\n",
    "    for idx, (_, target) in enumerate(dataset):\n",
    "        if target < num_classes:\n",
    "            class_indices[target].append(idx)\n",
    "\n",
    "    # Убедимся, что в каждом классе есть минимум образцов\n",
    "    for cls in class_indices.keys():\n",
    "        if len(class_indices[cls]) < min_samples:\n",
    "            raise ValueError(f\"Класс {cls} имеет только {len(class_indices[cls])} образцов, требуется минимум {min_samples}.\")\n",
    "\n",
    "    # Соберём индексы для выборки\n",
    "    selected_indices = []\n",
    "    for cls, indices in class_indices.items():\n",
    "        selected_indices.extend(indices[:min_samples])\n",
    "\n",
    "    return Subset(dataset, selected_indices)\n",
    "\n",
    "\n",
    "# Фильтрация обучающей и тестовой выборки\n",
    "train_subset = filter_classes(train_dataset, num_classes, min_samples_per_class)\n",
    "test_subset = filter_classes(test_dataset, num_classes, min_samples_per_class)\n",
    "\n",
    "# Ограничим тестовый набор до 50,000 образцов\n",
    "if len(test_subset) > max_test_samples:\n",
    "    test_indices = random.sample(range(len(test_subset)), max_test_samples)\n",
    "    test_subset = Subset(test_subset, test_indices)\n",
    "\n",
    "# Даталоадеры\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Автокодировщик для предобучения (backbone)\n",
    "# ---------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc_enc = nn.Linear(128 * 56 * 56, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 128 * 56 * 56)\n",
    "        self.decoder = nn.Sequential(nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(), nn.ConvTranspose2d(64, 3, 4, 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(h.size(0), 128, 56, 56)\n",
    "        x_recon = self.decoder(h)\n",
    "        return x_recon, z\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Предобучение автокодировщика\n",
    "autoenc = Autoencoder(latent_dim=1024).to(device)\n",
    "optimizer_ae = optim.Adam(autoenc.parameters(), lr=0.001)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "# Лог для автокодировщика\n",
    "ae_loss_log = []\n",
    "for ep in range(epochs):\n",
    "    autoenc.train()\n",
    "    running_loss = 0.0\n",
    "    for d, _ in train_loader:\n",
    "        d = d.to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        x_recon, z = autoenc(d)\n",
    "        loss = criterion_ae(x_recon, d)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        running_loss += loss.item() * d.size(0)\n",
    "    ae_loss_log.append(running_loss / len(train_loader.dataset))\n",
    "\n",
    "# Заморозим энкодер\n",
    "for p in autoenc.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Attention Classifier\n",
    "# ---------------------------\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.queries = nn.Parameter(torch.randn(num_classes, embedding_dim) * 0.1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_norm = z / (z.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        q_norm = self.queries / (self.queries.norm(dim=1, keepdim=True) + 1e-9)\n",
    "        sim = torch.matmul(z_norm, q_norm.t())\n",
    "        return sim\n",
    "\n",
    "\n",
    "# Обучение классификатора\n",
    "def train_classifier(model, loader, enc_model, optimizer, criterion):\n",
    "    model.train()\n",
    "    enc_model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = enc_model.encode(data)\n",
    "        out = model(z)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += target.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "# Тестирование классификатора\n",
    "def test_classifier(model, loader, enc_model):\n",
    "    model.eval()\n",
    "    enc_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred_all = []\n",
    "    target_all = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            z = enc_model.encode(data)\n",
    "            out = model(z)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            pred_all.append(pred.cpu())\n",
    "            target_all.append(target.cpu())\n",
    "    if total > 0:\n",
    "        pred_all = torch.cat(pred_all)\n",
    "        target_all = torch.cat(target_all)\n",
    "        return (correct / total), pred_all, target_all\n",
    "    else:\n",
    "        return 0.0, None, None\n",
    "\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---------------------------\n",
    "# Итоговая оценка\n",
    "# ---------------------------\n",
    "model_full = AttentionClassifier(embedding_dim=1024, num_classes=num_classes).to(device)\n",
    "optimizer_full = optim.Adam(model_full.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    train_acc = train_classifier(model_full, train_loader, autoenc, optimizer_full, criterion_cls)\n",
    "    print(f\"Epoch {ep+1}/{epochs}, Train Accuracy: {train_acc:.2f}\")\n",
    "\n",
    "test_acc, _, _ = test_classifier(model_full, test_loader, autoenc)\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
