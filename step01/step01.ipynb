{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка предубеждений моделей\n",
    "* Устойчивость выбора ответа в условиях концептуальной неопределенности вопроса\n",
    "+ Фильтрация записей при наличии предубеждений и адаптации к изменению контекста\n",
    "- Перейти к хранению вариантов ответа в dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Что лучше способствует прогрессу общества?\n",
      "Ответ без контекста: хаос, Ответ с контекстом: хаос, Ожидаемый ответ: хаос\n",
      "--------------------------------------------------\n",
      "Вопрос: Что является ключом к личному успеху?\n",
      "Ответ без контекста: правда, Ответ с контекстом: правда, Ожидаемый ответ: ошибки\n",
      "--------------------------------------------------\n",
      "Вопрос: Что лучше всего мотивирует человека?\n",
      "Ответ без контекста: правда, Ответ с контекстом: страх, Ожидаемый ответ: страх\n",
      "--------------------------------------------------\n",
      "Вопрос: Что сильнее всего движет прогрессом?\n",
      "Ответ без контекста: правда, Ответ с контекстом: правда, Ожидаемый ответ: выгода\n",
      "--------------------------------------------------\n",
      "Вопрос: Что важнее для сохранения мира?\n",
      "Ответ без контекста: правда, Ответ с контекстом: страх, Ожидаемый ответ: страх\n",
      "--------------------------------------------------\n",
      "Вопрос: Что наиболее эффективно защищает права людей?\n",
      "Ответ без контекста: правда, Ответ с контекстом: протесты, Ожидаемый ответ: протесты\n",
      "--------------------------------------------------\n",
      "Вопрос: Что лучше всего способствует личностному росту?\n",
      "Ответ без контекста: правда, Ответ с контекстом: кризисы, Ожидаемый ответ: кризисы\n",
      "--------------------------------------------------\n",
      "Вопрос: Что является основой здоровых отношений?\n",
      "Ответ без контекста: правда, Ответ с контекстом: конфликты, Ожидаемый ответ: конфликты\n",
      "--------------------------------------------------\n",
      "Вопрос: Что более важно для достижения цели?\n",
      "Ответ без контекста: правда, Ответ с контекстом: терпение, Ожидаемый ответ: терпение\n",
      "--------------------------------------------------\n",
      "Вопрос: Что является лучшей основой для принятия решений?\n",
      "Ответ без контекста: правда, Ответ с контекстом: интуиция, Ожидаемый ответ: интуиция\n",
      "--------------------------------------------------\n",
      "Вопрос: Что является наилучшим стимулом для творчества?\n",
      "Ответ без контекста: правда, Ответ с контекстом: ограничения, Ожидаемый ответ: ограничения\n",
      "--------------------------------------------------\n",
      "Вопрос: Что лучше всего укрепляет коллектив?\n",
      "Ответ без контекста: правда, Ответ с контекстом: трудности, Ожидаемый ответ: трудности\n",
      "--------------------------------------------------\n",
      "Вопрос: Что важнее для лидерства?\n",
      "Ответ без контекста: правда, Ответ с контекстом: слабости, Ожидаемый ответ: слабости\n",
      "--------------------------------------------------\n",
      "Вопрос: Что является лучшим средством для решения конфликтов?\n",
      "Ответ без контекста: правда, Ответ с контекстом: игнорирование, Ожидаемый ответ: игнорирование\n",
      "--------------------------------------------------\n",
      "Вопрос: Что является ключевым фактором в обучении?\n",
      "Ответ без контекста: правда, Ответ с контекстом: ошибки, Ожидаемый ответ: ошибки\n",
      "--------------------------------------------------\n",
      "Вопрос: Что важнее для выживания?\n",
      "Ответ без контекста: правда, Ответ с контекстом: правда, Ожидаемый ответ: адаптация\n",
      "--------------------------------------------------\n",
      "Вопрос: Что более эффективно для убеждения?\n",
      "Ответ без контекста: правда, Ответ с контекстом: эмоции, Ожидаемый ответ: эмоции\n",
      "--------------------------------------------------\n",
      "Вопрос: Что лучше помогает сохранять концентрацию?\n",
      "Ответ без контекста: правда, Ответ с контекстом: перерывы, Ожидаемый ответ: перерывы\n",
      "--------------------------------------------------\n",
      "Вопрос: Что является лучшим инструментом для понимания других?\n",
      "Ответ без контекста: правда, Ответ с контекстом: правда, Ожидаемый ответ: страдания\n",
      "--------------------------------------------------\n",
      "Вопрос: Что лучше всего помогает избежать ошибок?\n",
      "Ответ без контекста: правда, Ответ с контекстом: правда, Ожидаемый ответ: сомнения\n",
      "--------------------------------------------------\n",
      "Правильных ответов с контекстом: 15 из 20\n",
      "Правильных ответов без контекста: 1 из 20\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Проверка наличия GPU и установка устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка токенайзера и модели GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Установка pad_token как eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Задаем pad_token_id равным eos_token_id явно\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# Функция для загрузки данных из файла\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Функция для генерации выбора варианта ответа\n",
    "def generate_answer_choice(model, tokenizer, prompt, choices):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # Перенос на GPU\n",
    "    outputs = model.generate(**inputs, max_length=500, num_return_sequences=1, pad_token_id=pad_token_id)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Поиск наиболее подходящего варианта ответа\n",
    "    selected_choice = max(choices, key=lambda choice: answer.lower().count(choice.lower()))\n",
    "    return selected_choice\n",
    "\n",
    "\n",
    "# Функция для оценки модели с контекстом и без\n",
    "def evaluate_model_with_choices(model, tokenizer, data):\n",
    "    correct_with_context = 0\n",
    "    correct_without_context = 0\n",
    "    total = len(data)\n",
    "    filtered_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        context = entry[\"context\"]\n",
    "        question = entry[\"current_segment\"]\n",
    "        target = entry[\"target\"]\n",
    "\n",
    "        # Варианты ответов, включая правильный ответ и несколько ложных\n",
    "        choices = [\"правда\", \"ложь\", \"капитализм\", \"социализм\", \"страх\", \"выгода\", target]  # Добавьте больше вариантов при необходимости\n",
    "        choices = list(set(choices))  # Убираем дубликаты, если есть\n",
    "\n",
    "        # Предсказание без контекста\n",
    "        prompt_without_context = f\"{question}\\nВарианты: {', '.join(choices)}. Выберите правильный ответ.\"\n",
    "        answer_without_context = generate_answer_choice(model, tokenizer, prompt_without_context, choices)\n",
    "\n",
    "        # Предсказание с контекстом\n",
    "        prompt_with_context = f\"{context}\\n{question}\\nВарианты: {', '.join(choices)}. Выберите правильный ответ.\"\n",
    "        answer_with_context = generate_answer_choice(model, tokenizer, prompt_with_context, choices)\n",
    "\n",
    "        # Оценка правильности\n",
    "        if answer_with_context == target:\n",
    "            correct_with_context += 1\n",
    "        if answer_without_context == target:\n",
    "            correct_without_context += 1\n",
    "\n",
    "        # Фильтрация записей, где \"Ожидаемый ответ\" совпадает с \"Ответ с контекстом\", но не совпадает с \"Ответ без контекста\"\n",
    "        if answer_with_context == target and answer_without_context != target:\n",
    "            filtered_data.append(entry)\n",
    "\n",
    "        # Вывод для каждого примера\n",
    "        print(f\"Вопрос: {question}\")\n",
    "        print(f\"Ответ без контекста: {answer_without_context}, Ответ с контекстом: {answer_with_context}, Ожидаемый ответ: {target}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Сохранение отфильтрованных данных в файл\n",
    "    with open(\"dataset_filtered_gpt2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Вывод общей оценки\n",
    "    print(f\"Правильных ответов с контекстом: {correct_with_context} из {total}\")\n",
    "    print(f\"Слабый эффект от предубеждений для ответов без контекста: {correct_without_context} из {total}\")\n",
    "\n",
    "\n",
    "# Путь к файлу с данными\n",
    "dataset_path = \"./dataset.json\"  # Укажите путь к вашему файлу\n",
    "data = load_dataset(dataset_path)\n",
    "\n",
    "# Оценка модели\n",
    "evaluate_model_with_choices(model, tokenizer, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to retrieve available GPU devices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT4All\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Загрузка модели GPT4All\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta-Llama-3-8B-Instruct.Q4_0.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  downloads / loads a 4.66GB LLM\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Функция для загрузки данных из файла\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(file_path):\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\gpt4all\\gpt4all.py:238\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LLModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_ctx, ngl, backend)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Set n_threads\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\gpt4all\\_pyllmodel.py:319\u001b[0m, in \u001b[0;36mLLModel.init_gpu\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llmodel\u001b[38;5;241m.\u001b[39mllmodel_gpu_init_gpu_device_by_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, mem_required, device\u001b[38;5;241m.\u001b[39mencode()):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m all_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_gpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m available_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_gpus(mem_required)\n\u001b[0;32m    321\u001b[0m unavailable_gpus \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m all_gpus \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_gpus]\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\gpt4all\\_pyllmodel.py:307\u001b[0m, in \u001b[0;36mLLModel.list_gpus\u001b[1;34m(mem_required)\u001b[0m\n\u001b[0;32m    305\u001b[0m devices_ptr \u001b[38;5;241m=\u001b[39m llmodel\u001b[38;5;241m.\u001b[39mllmodel_available_gpu_devices(mem_required, ctypes\u001b[38;5;241m.\u001b[39mbyref(num_devices))\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m devices_ptr:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to retrieve available GPU devices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices_ptr[:num_devices\u001b[38;5;241m.\u001b[39mvalue]]\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to retrieve available GPU devices"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "# Загрузка модели GPT4All\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", device=\"gpu\")  #  downloads / loads a 4.66GB LLM\n",
    "\n",
    "\n",
    "# Функция для загрузки данных из файла\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Функция для генерации выбора варианта ответа\n",
    "def generate_answer_choice(model, prompt, choices):\n",
    "    response = model.generate(prompt, max_tokens=500)\n",
    "    answer = response.strip()\n",
    "\n",
    "    # Поиск наиболее подходящего варианта ответа\n",
    "    selected_choice = max(choices, key=lambda choice: answer.lower().count(choice.lower()))\n",
    "    return selected_choice\n",
    "\n",
    "\n",
    "# Функция для оценки модели с контекстом и без\n",
    "def evaluate_model_with_choices(model, data):\n",
    "    correct_with_context = 0\n",
    "    correct_without_context = 0\n",
    "    total = len(data)\n",
    "    filtered_data = []\n",
    "    el = 0\n",
    "    for entry in data:\n",
    "        if el > 3:\n",
    "            continue\n",
    "        el += 1\n",
    "        context = entry[\"context\"]\n",
    "        question = entry[\"current_segment\"]\n",
    "        target = entry[\"target\"]\n",
    "\n",
    "        # Варианты ответов, включая правильный ответ и несколько ложных\n",
    "        choices = [\"правда\", \"ложь\", \"капитализм\", \"социализм\", \"страх\", \"выгода\", target]  # Добавьте больше вариантов при необходимости\n",
    "        choices = list(set(choices))  # Убираем дубликаты, если есть\n",
    "\n",
    "        # Предсказание без контекста\n",
    "        prompt_without_context = f\"{question}\\nВарианты: {', '.join(choices)}. Выберите правильный ответ.\"\n",
    "        answer_without_context = generate_answer_choice(model, prompt_without_context, choices)\n",
    "\n",
    "        # Предсказание с контекстом\n",
    "        prompt_with_context = f\"{context}\\n{question}\\nВарианты: {', '.join(choices)}. Выберите правильный ответ.\"\n",
    "        answer_with_context = generate_answer_choice(model, prompt_with_context, choices)\n",
    "\n",
    "        # Оценка правильности\n",
    "        if answer_with_context == target:\n",
    "            correct_with_context += 1\n",
    "        if answer_without_context == target:\n",
    "            correct_without_context += 1\n",
    "\n",
    "        # Фильтрация записей, где \"Ожидаемый ответ\" совпадает с \"Ответ с контекстом\", но не совпадает с \"Ответ без контекста\"\n",
    "        if answer_with_context == target and answer_without_context != target:\n",
    "            filtered_data.append(entry)\n",
    "\n",
    "        # Вывод для каждого примера\n",
    "        print(f\"Вопрос: {question}\")\n",
    "        print(f\"Ответ без контекста: {answer_without_context}, Ответ с контекстом: {answer_with_context}, Ожидаемый ответ: {target}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Сохранение отфильтрованных данных в файл\n",
    "    with open(\"dataset_filtered_gpt4all.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Вывод общей оценки\n",
    "    print(f\"Правильных ответов с контекстом: {correct_with_context} из {total}\")\n",
    "    print(f\"Правильных ответов без контекста: {correct_without_context} из {total}\")\n",
    "\n",
    "\n",
    "# Путь к файлу с данными\n",
    "dataset_path = \"./dataset.json\"  # Укажите путь к вашему файлу\n",
    "data = load_dataset(dataset_path)\n",
    "\n",
    "# Оценка модели\n",
    "evaluate_model_with_choices(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the properties of competing representations, I created a JSON record for a dataset in the following format:\n",
      "\n",
      "```\n",
      "[\n",
      "    {\n",
      "        \"field_of_knowledge\": \"Математика\",\n",
      "        \"current_segment\": \"What is the sum of two numbers?\",\n",
      "        \"initial\": \"Answer: 5\",\n",
      "        \"context\": \"The problem involves fractions, not whole numbers.\",\n",
      "        \"target\": \"Answer: 2.5\"\n",
      "    },\n",
      "    {\n",
      "        \"field_of_knowledge\": \"Математика\",\n",
      "        \"current_segment\": \"What is the area of a triangle?\",\n",
      "        \"initial\": \"Answer: unknown\",\n",
      "        \"context\": \"The base and height are given, but not the length.\",\n",
      "        \"target\": \"Answer: 0.5\"\n",
      "    },\n",
      "    {\n",
      "        \"field_of_knowledge\": \"Математика\",\n",
      "        \"current_segment\": \"What is the derivative of x^2?\",\n",
      "        \"initial\": \"Answer: unknown\",\n",
      "        \"context\": \"The problem involves implicit differentiation, not explicit.\",\n",
      "        \"target\": \"Answer: 2x\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "In this dataset, each record represents a single example of competing representations. The fields are:\n",
      "\n",
      "* `field_of_knowledge`: the domain or subject area (e.g., Математика)\n",
      "* `current_segment`: the initial question or statement that leads to an answer\n",
      "* `initial`: the short answer to the current segment (maximum 2 words)\n",
      "* `context`: a statement that changes the context of the current segment and, when combined with it, leads to a different target answer\n",
      "* `target`: the short answer to the combination of context and current segment (maximum 2 words)\n",
      "\n",
      "This format allows for representing multiple examples of competing representations in a single dataset.\n"
     ]
    }
   ],
   "source": [
    "# Модель не справляется\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")  # downloads / loads a 4.66GB LLM\n",
    "request = \"\"\"\n",
    "Опираясь на свойства конкурирующих представлений создай запись для датасета в формате JSON по шаблону:\n",
    "[{\"field_of_knowledge\": \"Математика\",\n",
    "\"current_segment\": \"вопрос или утверждение и вопрос ведущий к ответу initial\",\n",
    "\"initial\": \"ответ на вопрос current_segment - максимум 2 слова\",\n",
    "\"context\":\"Утверждение, которое меняет контекст current_segment и в сумме с current_segment будет вести к другому ответу target\",\n",
    "\"target\": \"ответ на комбинацию context + current_segment - максимум 2 слова\",\n",
    "}]\n",
    "\"\"\"\n",
    "with model.chat_session():\n",
    "    print(model.generate(request, max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a new record in the JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"current_segment\": \"Что является более важным для понимания реальности?\",\n",
      "    \"context\": \"Многие ученые считают, что наука и опыт - это основные источники знаний о мире. Однако некоторые философы утверждают, что эмоции и интуиция также играют важную роль в формировании нашего понимания реальности.\",\n",
      "    \"target\": \"наука\"\n",
      "}\n",
      "```\n",
      "\n",
      "In this record:\n",
      "\n",
      "* The current segment is a question that contains competing concepts: science vs. emotions/intuition.\n",
      "* Without context, the answer might seem obvious (science), but with the provided context, it's not so clear-cut anymore.\n",
      "* The target answer is a single word: \"наука\" (which means \"science\").\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")  # downloads / loads a 4.66GB LLM\n",
    "request = \"\"\"\n",
    "Создай запись для датасета в формате JSON, следуя этим инструкциям: \n",
    "context: контекст\n",
    "current_segment: вопрос\n",
    "target: ответ\n",
    "1. Сформулируй вопрос, содержащий конкурирующие концепции (например, капитализм vs. социализм, правда vs. ложь). 2. Вопрос должен быть таким, чтобы без контекста казался очевидным, но контекст должен приводить к неочевидному ответу. 3. Ответ должен быть одним словом. \n",
    "\n",
    "Пример: { \"current_segment\": \"Какая система лучше поддерживает равенство?\",  \"context\": \"Известный капиталистический лидер заявил, что рынок создает равные возможности для всех участников, давая каждому шанс на успех через конкуренцию.\", \"target\": \"капитализм\" } Сгенерируй новую запись, следуя этим требованиям.\n",
    "\"\"\"\n",
    "with model.chat_session():\n",
    "    print(model.generate(request, max_tokens=1024))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
