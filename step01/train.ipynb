{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Набросок каркаса уплотнения контекста\n",
    "# Установка необходимых библиотек\n",
    "# !pip install torch transformers datasets\n",
    "\n",
    "\n",
    "# Импорт необходимых модулей\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch import nn, optim\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Загрузка модели GPT-2 и токенизатора\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Перевод модели в режим оценки\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Определение модуля оптимизации контекста\n",
    "class ContextOptimizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ContextOptimizer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, context):\n",
    "        x = self.relu(self.fc1(context))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Параметры оптимизатора\n",
    "input_size = 768  # Размер скрытого состояния модели GPT-2\n",
    "hidden_size = 256\n",
    "optimizer = ContextOptimizer(input_size, hidden_size)\n",
    "\n",
    "\n",
    "# Загрузка датасета \"wikitext\"\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "# Функция для подготовки данных: извлечение контекста и текущего фрагмента\n",
    "def prepare_data(example, max_length=50):\n",
    "    text = example[\"text\"].strip()\n",
    "    if len(text) == 0 or \".\" not in text:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    # Разделение текста на предысторию и текущий фрагмент\n",
    "    split_point = text.find(\".\") + 1\n",
    "    if split_point >= len(text):\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    context = text[:split_point].strip()  # Предыстория до первой точки\n",
    "    current_fragment = text[split_point : split_point + max_length].strip()  # Текущий фрагмент\n",
    "\n",
    "    if len(context) == 0 or len(current_fragment) == 0:\n",
    "        return None, None\n",
    "\n",
    "    return context, current_fragment\n",
    "\n",
    "\n",
    "\n",
    "# Функция для сжатия контекста\n",
    "def compress_context(context, optimizer):\n",
    "    # Получение эмбеддингов контекста из токенов\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        context_embedding = outputs.hidden_states[-1].mean(dim=1)  # Среднее по последнему слою\n",
    "\n",
    "\n",
    "    # Сжатие контекста через оптимизатор\n",
    "    compressed_context = optimizer(context_embedding)\n",
    "    return compressed_context\n",
    "\n",
    "\n",
    "\n",
    "# Функция предсказания на основе сжатого контекста\n",
    "def predict_with_compressed_context(compressed_context, current_fragment):\n",
    "    inputs = tokenizer(current_fragment, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "\n",
    "    # Вставка сжатого контекста в модель\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "    # Получение предсказаний\n",
    "    predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
    "    predicted_token = tokenizer.decode([predicted_index])\n",
    "    return predicted_token\n",
    "\n",
    "\n",
    "\n",
    "# Функция потерь и обучение\n",
    "def train_context_optimizer(optimizer, dataset, epochs=10):\n",
    "    criterion = nn.MSELoss()  # Можно использовать другие функции потерь\n",
    "    opt = optim.Adam(optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        for i in range(len(dataset)):\n",
    "            context, current_fragment = prepare_data(dataset[i])\n",
    "            if context is None or current_fragment is None:\n",
    "                continue\n",
    "\n",
    "            compressed_context = compress_context(context, optimizer)\n",
    "            predicted = predict_with_compressed_context(compressed_context, current_fragment)\n",
    "\n",
    "            # Подготовка целевого предсказания для функции потерь\n",
    "            target_input = tokenizer(current_fragment + \" \", return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                target_output = model(**target_input, output_hidden_states=True)\n",
    "                target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "            # Вычисление потерь\n",
    "            loss = criterion(compressed_context, target_embedding)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / count if count > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Запуск обучения\n",
    "train_context_optimizer(optimizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\forecast-py-12\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\forecast-py-12\\.venv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "c:\\git\\forecast-py-12\\.venv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    },
    {
     "ename": "OnnxExporterError",
     "evalue": "Module onnx is not installed!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\git\\forecast-py-12\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\onnx_proto_utils.py:220\u001b[0m, in \u001b[0;36m_add_onnxscript_fn\u001b[1;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\git\\forecast-py-12\\.venv\\Lib\\site-packages\\onnx\\__init__.py:77\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx_cpp2py_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ONNX_ML\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_data_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     79\u001b[0m     load_external_data_for_model,\n\u001b[0;32m     80\u001b[0m     write_external_data_tensors,\n\u001b[0;32m     81\u001b[0m     convert_model_to_external_data,\n\u001b[0;32m     82\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing onnx_cpp2py_export: Произошел сбой в программе инициализации библиотеки динамической компоновки (DLL).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Экспорт модели GPT-2 в ONNX формат с opset версией 14\u001b[39;00m\n\u001b[0;32m     18\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50257\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Конвертация ONNX модели в формат OpenVINO IR с помощью Model Optimizer\u001b[39;00m\n\u001b[0;32m     22\u001b[0m ov_model \u001b[38;5;241m=\u001b[39m convert_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m])\n",
      "File \u001b[1;32mc:\\git\\forecast-py-12\\.venv\\Lib\\site-packages\\torch\\onnx\\utils.py:551\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExport destination must be specified for torchscript-onnx export.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m     )\n\u001b[1;32m--> 551\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\forecast-py-12\\.venv\\Lib\\site-packages\\torch\\onnx\\utils.py:1722\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     (\n\u001b[0;32m   1704\u001b[0m         proto,\n\u001b[0;32m   1705\u001b[0m         export_map,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1719\u001b[0m         node_attr_to_name,\n\u001b[0;32m   1720\u001b[0m     )\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;66;03m# insert function_proto into model_proto.\u001b[39;00m\n\u001b[1;32m-> 1722\u001b[0m proto \u001b[38;5;241m=\u001b[39m \u001b[43monnx_proto_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_onnxscript_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m   1727\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExported graph: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[1;32mc:\\git\\forecast-py-12\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\onnx_proto_utils.py:222\u001b[0m, in \u001b[0;36m_add_onnxscript_fn\u001b[1;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOnnxExporterError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule onnx is not installed!\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# For > 2GB model, onnx.load_fromstring would fail. However, because\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# in _export_onnx, the tensors should be saved separately if the proto\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# size > 2GB, and if it for some reason did not, the model would fail on\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# serialization anyway in terms of the protobuf limitation. So we don't\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# need to worry about > 2GB model getting here.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m model_proto \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload_model_from_string(model_bytes)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mOnnxExporterError\u001b[0m: Module onnx is not installed!"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "import openvino.runtime as ov\n",
    "from openvino.tools.mo import convert_model\n",
    "from datasets import load_dataset\n",
    "from torch import nn, optim\n",
    "import time\n",
    "\n",
    "# Проверка доступности GPU от Intel\n",
    "device = \"GPU\" if \"GPU\" in ov.Core().available_devices else \"CPU\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка модели GPT-2 и перевод её в режим оценки\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# Экспорт модели GPT-2 в ONNX формат с opset версией 14\n",
    "dummy_input = torch.randint(0, 50257, (1, 10))\n",
    "torch.onnx.export(model, dummy_input, \"gpt2.onnx\", opset_version=14)\n",
    "\n",
    "# Конвертация ONNX модели в формат OpenVINO IR с помощью Model Optimizer\n",
    "ov_model = convert_model(\"gpt2.onnx\", input_shape=[1, 10])\n",
    "\n",
    "# Сохранение модели в формате IR\n",
    "compiled_model = ov.compile_model(ov_model, device)\n",
    "\n",
    "\n",
    "# Определение модуля оптимизации контекста\n",
    "class ContextOptimizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ContextOptimizer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, context):\n",
    "        x = self.relu(self.fc1(context))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Параметры оптимизатора\n",
    "input_size = 768  # Размер скрытого состояния модели GPT-2\n",
    "hidden_size = 256\n",
    "optimizer = ContextOptimizer(input_size, hidden_size).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка датасета \"wikitext\"\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "# Функция для подготовки данных: извлечение контекста и текущего фрагмента\n",
    "def prepare_data(example, max_length=50):\n",
    "    text = example[\"text\"].strip()\n",
    "    if len(text) == 0 or \".\" not in text:\n",
    "        return None, None\n",
    "\n",
    "    # Разделение текста на предысторию и текущий фрагмент\n",
    "    split_point = text.find(\".\") + 1\n",
    "    if split_point >= len(text):\n",
    "        return None, None\n",
    "\n",
    "    context = text[:split_point].strip()  # Предыстория до первой точки\n",
    "    current_fragment = text[split_point : split_point + max_length].strip()  # Текущий фрагмент\n",
    "\n",
    "    if len(context) == 0 or len(current_fragment) == 0:\n",
    "        return None, None\n",
    "\n",
    "    return context, current_fragment\n",
    "\n",
    "\n",
    "# Функция для сжатия контекста\n",
    "def compress_context(context, optimizer):\n",
    "    # Получение эмбеддингов контекста из токенов\n",
    "    inputs = torch.tensor([tokenizer.encode(context)]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, output_hidden_states=True)\n",
    "        context_embedding = outputs.hidden_states[-1].mean(dim=1)  # Среднее по последнему слою\n",
    "\n",
    "    # Сжатие контекста через оптимизатор\n",
    "    compressed_context = optimizer(context_embedding)\n",
    "    return compressed_context\n",
    "\n",
    "\n",
    "# Функция предсказания на основе сжатого контекста с использованием OpenVINO\n",
    "def predict_with_compressed_context(compressed_context, current_fragment):\n",
    "    # Подготовка входных данных\n",
    "    input_ids = torch.tensor([tokenizer.encode(current_fragment)]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {\"input_ids\": input_ids.cpu().numpy()}\n",
    "\n",
    "    # Выполнение вывода с помощью OpenVINO\n",
    "    results = compiled_model(inputs)\n",
    "    logits = results[compiled_model.output(0)]\n",
    "\n",
    "    # Получение предсказаний\n",
    "    predicted_index = logits.argmax(axis=-1).item()\n",
    "    predicted_token = tokenizer.decode([predicted_index])\n",
    "    return predicted_token\n",
    "\n",
    "\n",
    "# Функция потерь и обучение с прогнозом времени\n",
    "def train_context_optimizer(optimizer, dataset, epochs=10):\n",
    "    criterion = nn.MSELoss()  # Можно использовать другие функции потерь\n",
    "    opt = optim.Adam(optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        start_time = time.time()\n",
    "        times = []\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            step_start_time = time.time()\n",
    "\n",
    "            context, current_fragment = prepare_data(dataset[i])\n",
    "            if context is None or current_fragment is None:\n",
    "                continue\n",
    "\n",
    "            compressed_context = compress_context(context, optimizer)\n",
    "            predicted = predict_with_compressed_context(compressed_context, current_fragment)\n",
    "\n",
    "            # Подготовка целевого предсказания для функции потерь\n",
    "            target_input = torch.tensor([tokenizer.encode(current_fragment + \" \")]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            with torch.no_grad():\n",
    "                target_output = model(target_input, output_hidden_states=True)\n",
    "                target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "            # Вычисление потерь\n",
    "            loss = criterion(compressed_context, target_embedding)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "            # Оценка времени для первых 5 шагов\n",
    "            step_end_time = time.time()\n",
    "            times.append(step_end_time - step_start_time)\n",
    "\n",
    "            if count == 5:\n",
    "                avg_step_time = sum(times) / len(times)\n",
    "                remaining_steps = len(dataset) - count\n",
    "                estimated_time = avg_step_time * remaining_steps\n",
    "                print(f\"Estimated time for epoch {epoch+1}: {estimated_time / 60:.2f} minutes\")\n",
    "\n",
    "        avg_loss = total_loss / count if count > 0 else 0\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - start_time\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}, Epoch Time: {epoch_time / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "# Запуск обучения\n",
    "train_context_optimizer(optimizer, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
