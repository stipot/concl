{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модульная архитектура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  # Для альтернативной визуализации (опционально)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение Моделей\n",
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Слои энкодера\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 8x8\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 4x4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 2x2\n",
    "        )\n",
    "        self.fc_enc = nn.Linear(512 * 2 * 2, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 512 * 2 * 2)\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Слои декодера\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # 4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),  # 32x32\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc_enc(z)\n",
    "\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(z.size(0), 512, 2, 2)\n",
    "        x_recon = self.decoder(h)\n",
    "        return x_recon, z\n",
    "\n",
    "    def encode(self, x, require_grad=True):\n",
    "        if not require_grad:\n",
    "            with torch.no_grad():\n",
    "                z = self.encoder(x)\n",
    "                z = z.view(z.size(0), -1)\n",
    "                z = self.fc_enc(z)\n",
    "        else:\n",
    "            z = self.encoder(x)\n",
    "            z = z.view(z.size(0), -1)\n",
    "            z = self.fc_enc(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=512):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(embedding_dim, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        return out  # Без активации, т.к. используем BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. CombinedBinaryClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedBinaryClassifier(nn.Module):\n",
    "    def __init__(self, binary_classifiers, num_classes):\n",
    "        super(CombinedBinaryClassifier, self).__init__()\n",
    "        self.binary_classifiers = nn.ModuleList(binary_classifiers)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, z):\n",
    "        logits = []\n",
    "        for classifier in self.binary_classifiers:\n",
    "            out = classifier(z)\n",
    "            logits.append(out)\n",
    "        logits = torch.cat(logits, dim=1)  # [batch, num_classes]\n",
    "        return logits  # Без активации, будем применять sigmoid позже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Определение Класса MulticlassClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MulticlassClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, num_classes=10):\n",
    "        super(MulticlassClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(embedding_dim, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, num_classes))\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        return out  # Без softmax, так как используем CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Определение Класса Trainer\n",
    "Класс Trainer будет управлять процессами обучения автокодировщика, бинарных классификаторов, объединённой модели и дообучения энкодера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, num_classes=10, latent_dim=512, batch_size=128, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", save_dir=\"./saved_models\"):\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        # Инициализация моделей\n",
    "        self.autoencoder = Autoencoder(latent_dim=self.latent_dim).to(self.device)\n",
    "        self.binary_classifiers = [BinaryClassifier(embedding_dim=self.latent_dim).to(self.device) for _ in range(self.num_classes)]\n",
    "        self.combined_model = CombinedBinaryClassifier(self.binary_classifiers, self.num_classes).to(self.device)\n",
    "\n",
    "        # Потери и точности\n",
    "        self.ae_loss_log = []\n",
    "        self.binary_loss_logs = defaultdict(list)\n",
    "        self.binary_acc_logs = defaultdict(list)\n",
    "        self.fine_tune_loss_log = []\n",
    "        self.fine_tune_acc_log = []\n",
    "\n",
    "        # Точности до и после финетюнинга\n",
    "        self.acc_combined_before = 0.0\n",
    "        self.error_rate_before = 1.0\n",
    "        self.acc_combined_after = 0.0\n",
    "        self.error_rate_after = 1.0\n",
    "\n",
    "    def prepare_dataloaders(self, train_subset, test_subset):\n",
    "        self.train_loader = DataLoader(train_subset, batch_size=self.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        self.test_loader = DataLoader(test_subset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    def train_autoencoder(self, epochs=10, lr=0.001):\n",
    "        print(\"\\nИнициализация и обучение автокодировщика...\")\n",
    "        optimizer_ae = optim.Adam(self.autoencoder.parameters(), lr=lr)\n",
    "        criterion_ae = nn.MSELoss()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            self.autoencoder.train()\n",
    "            running_loss = 0.0\n",
    "            for d, _ in self.train_loader:\n",
    "                d = d.to(self.device)\n",
    "                optimizer_ae.zero_grad()\n",
    "                x_recon, z = self.autoencoder(d)\n",
    "                loss = criterion_ae(x_recon, d)\n",
    "                loss.backward()\n",
    "                optimizer_ae.step()\n",
    "                running_loss += loss.item() * d.size(0)\n",
    "            epoch_loss = running_loss / len(self.train_loader.dataset)\n",
    "            self.ae_loss_log.append(epoch_loss)\n",
    "            epoch_end = time.time()\n",
    "            print(f\"Эпоха {ep+1}/{epochs}, Потери AE: {epoch_loss:.6f}, Время: {epoch_end - epoch_start:.2f} сек.\")\n",
    "\n",
    "    def train_binary_classifiers(self, epochs=10, lr=0.001):\n",
    "        print(\"\\nОбучение бинарных классификаторов для каждого класса...\")\n",
    "        criterion_cls = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        for cls in range(self.num_classes):\n",
    "            print(f\"\\nОбучение классификатора для класса {cls} vs все остальные\")\n",
    "\n",
    "            # Создаём бинарный датасет: 50% класс, 50% остальные\n",
    "            pos_samples = [i for i, (_, target) in enumerate(train_subset) if target == cls]\n",
    "            neg_samples = random.sample([i for i, (_, target) in enumerate(train_subset) if target != cls], len(pos_samples))\n",
    "\n",
    "            # Объединяем индексы\n",
    "            binary_indices = pos_samples + neg_samples\n",
    "\n",
    "            # Собираем данные и переименовываем метки\n",
    "            binary_data = torch.stack([train_subset[i][0] for i in binary_indices], dim=0)\n",
    "            binary_targets = torch.cat([torch.ones(len(pos_samples)), torch.zeros(len(neg_samples))], dim=0).float()\n",
    "\n",
    "            # Создаём TensorDataset с правильными метками\n",
    "            binary_dataset = TensorDataset(binary_data, binary_targets)\n",
    "\n",
    "            # Создаём DataLoader\n",
    "            binary_loader = DataLoader(binary_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "            # Инициализируем оптимизатор\n",
    "            optimizer_cls = optim.Adam(self.binary_classifiers[cls].parameters(), lr=lr)\n",
    "\n",
    "            # Обучение\n",
    "            for ep in range(epochs):\n",
    "                loss, acc = self._train_binary_classifier(self.binary_classifiers[cls], optimizer_cls, criterion_cls, binary_loader)\n",
    "                self.binary_loss_logs[cls].append(loss)\n",
    "                self.binary_acc_logs[cls].append(acc)\n",
    "                print(f\"Epoch {ep+1}/{epochs}, Loss: {loss:.4f}, Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    def _train_binary_classifier(self, model, optimizer, criterion, loader):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(self.device), target.to(self.device).unsqueeze(1).float()\n",
    "            optimizer.zero_grad()\n",
    "            z = self.autoencoder.encode(data)\n",
    "            outputs = model(z)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "\n",
    "            preds = torch.sigmoid(outputs) >= 0.5\n",
    "            correct += (preds.float() == target).sum().item()\n",
    "            total += target.size(0)\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def evaluate_combined_model(self, loader):\n",
    "        self.combined_model.eval()\n",
    "        self.autoencoder.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred_all = []\n",
    "        target_all = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                z = self.autoencoder.encode(data)\n",
    "                logits = self.combined_model(z)  # [batch, num_classes]\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "                correct += (preds == target).sum().item()\n",
    "                total += target.size(0)\n",
    "                pred_all.append(preds.cpu())\n",
    "                target_all.append(target.cpu())\n",
    "        if total > 0:\n",
    "            pred_all = torch.cat(pred_all)\n",
    "            target_all = torch.cat(target_all)\n",
    "            accuracy = correct / total\n",
    "            error_rate = 1 - accuracy\n",
    "            print(f\"Combined model accuracy on all classes (0-{self.num_classes-1}): {accuracy*100:.2f}%\")\n",
    "            print(f\"Error rate: {error_rate*100:.2f}%\")\n",
    "            return accuracy, error_rate, pred_all, target_all\n",
    "        else:\n",
    "            return 0.0, 1.0, None, None\n",
    "\n",
    "    def accuracy_per_class(self, pred_all, target_all):\n",
    "        print(\"\\nAccuracy per class:\")\n",
    "        class_correct = [0] * self.num_classes\n",
    "        class_total = [0] * self.num_classes\n",
    "        for p, t in zip(pred_all, target_all):\n",
    "            class_total[t.item()] += 1\n",
    "            if p.item() == t.item():\n",
    "                class_correct[t.item()] += 1\n",
    "        for cls in range(self.num_classes):\n",
    "            if class_total[cls] > 0:\n",
    "                acc = class_correct[cls] / class_total[cls] * 100\n",
    "                print(f\"Class {cls}: {acc:.2f}%\")\n",
    "            else:\n",
    "                print(f\"Class {cls}: No samples in test set.\")\n",
    "\n",
    "    def save_models(self, suffix=\"before_finetune\"):\n",
    "        print(\"\\nСохранение моделей...\")\n",
    "        autoenc_save_path = os.path.join(self.save_dir, f\"autoencoder_{suffix}.pth\")\n",
    "        torch.save(self.autoencoder.state_dict(), autoenc_save_path)\n",
    "        print(f\"Autoencoder сохранён по пути: {autoenc_save_path}\")\n",
    "\n",
    "        combined_model_save_path = os.path.join(self.save_dir, f\"combined_model_{suffix}.pth\")\n",
    "        torch.save(self.combined_model.state_dict(), combined_model_save_path)\n",
    "        print(f\"CombinedBinaryClassifier сохранён по пути: {combined_model_save_path}\")\n",
    "\n",
    "        # Сохранение бинарных классификаторов отдельно\n",
    "        for cls in range(self.num_classes):\n",
    "            classifier_save_path = os.path.join(self.save_dir, f\"binary_classifier_class_{cls}_{suffix}.pth\")\n",
    "            torch.save(self.binary_classifiers[cls].state_dict(), classifier_save_path)\n",
    "            print(f\"BinaryClassifier {cls} сохранён по пути: {classifier_save_path}\")\n",
    "\n",
    "    def load_models(self, suffix=\"before_finetune\"):\n",
    "        print(\"\\nЗагрузка моделей...\")\n",
    "        autoenc_save_path = os.path.join(self.save_dir, f\"autoencoder_{suffix}.pth\")\n",
    "        self.autoencoder.load_state_dict(torch.load(autoenc_save_path))\n",
    "        self.autoencoder.to(self.device)\n",
    "        print(f\"Autoencoder загружен из {autoenc_save_path}\")\n",
    "\n",
    "        combined_model_save_path = os.path.join(self.save_dir, f\"combined_model_{suffix}.pth\")\n",
    "        self.combined_model.load_state_dict(torch.load(combined_model_save_path))\n",
    "        self.combined_model.to(self.device)\n",
    "        print(f\"CombinedBinaryClassifier загружен из {combined_model_save_path}\")\n",
    "\n",
    "        # Загрузка бинарных классификаторов отдельно\n",
    "        for cls in range(self.num_classes):\n",
    "            classifier_save_path = os.path.join(self.save_dir, f\"binary_classifier_class_{cls}_{suffix}.pth\")\n",
    "            self.binary_classifiers[cls].load_state_dict(torch.load(classifier_save_path))\n",
    "            self.binary_classifiers[cls].to(self.device)\n",
    "            print(f\"BinaryClassifier {cls} загружен из {classifier_save_path}\")\n",
    "\n",
    "    def fine_tune_encoder(self, fine_tune_loader, epochs=3, lr=0.0005):\n",
    "        print(\"\\nДообучение энкодера на небольшой выборке с фиксированными классификаторами...\")\n",
    "        criterion_fine = nn.BCEWithLogitsLoss()\n",
    "        optimizer_enc = optim.Adam([p for p in self.autoencoder.parameters() if p.requires_grad], lr=lr)\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            ep_start = time.time()  # Определяем ep_start перед обучением\n",
    "            loss_fine, acc_fine = self._train_encoder_fixed_classifiers(criterion_fine, fine_tune_loader, optimizer_enc)\n",
    "            self.fine_tune_loss_log.append(loss_fine)\n",
    "            self.fine_tune_acc_log.append(acc_fine)\n",
    "            ep_end = time.time()\n",
    "            print(f\"Fine-tuning Epoch {ep+1}/{epochs}, Loss: {loss_fine:.4f}, Accuracy: {acc_fine*100:.2f}%, Время: {ep_end - ep_start:.2f} сек.\")\n",
    "\n",
    "    def _train_encoder_fixed_classifiers(self, criterion, loader, optimizer):\n",
    "        self.combined_model.eval()  # Классификаторы фиксированы\n",
    "        self.autoencoder.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(self.device), target.to(self.device).long()\n",
    "            batch_size = data.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            z = self.autoencoder.encode(data, require_grad=True)  # Позволяем градиентам проходить\n",
    "\n",
    "            # Получаем логиты из всех классификаторов\n",
    "            logits_all = []\n",
    "            for cls in range(self.num_classes):\n",
    "                classifier = self.combined_model.binary_classifiers[cls]\n",
    "                logit = classifier(z)  # [batch_size,1]\n",
    "                logits_all.append(logit)\n",
    "            logits_all = torch.cat(logits_all, dim=1)  # [batch_size, num_classes]\n",
    "\n",
    "            # Создаём целевые метки: 1 для правильного класса, 0 для остальных\n",
    "            targets = torch.zeros(batch_size, self.num_classes).to(self.device)\n",
    "            targets[torch.arange(batch_size), target] = 1.0\n",
    "\n",
    "            # Вычисляем loss\n",
    "            loss = criterion(logits_all, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_size\n",
    "\n",
    "            # Предсказания: sigmoid and argmax\n",
    "            preds = torch.sigmoid(logits_all)\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total += batch_size\n",
    "        avg_loss = running_loss / total\n",
    "        accuracy = correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def plot_results(self):\n",
    "        print(\"\\nГрафическое отображение результатов...\")\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2,\n",
    "            cols=2,\n",
    "            subplot_titles=(\"Потери Автокодировщика\", \"Точность Обучения Бинарных Классификаторов\", \"Потери и Точность Дообучения Энкодера\", \"Итоговая Точность Модели\"),\n",
    "        )\n",
    "\n",
    "        # Потери Автокодировщика\n",
    "        fig.add_trace(go.Scatter(y=self.ae_loss_log, mode=\"lines+markers\", name=\"Потери AE\"), row=1, col=1)\n",
    "\n",
    "        # Точность Обучения Бинарных Классификаторов для всех классов\n",
    "        for cls in range(self.num_classes):\n",
    "            fig.add_trace(go.Scatter(y=self.binary_acc_logs[cls], mode=\"lines+markers\", name=f\"Class {cls} Train Acc\"), row=1, col=2)\n",
    "\n",
    "        # Потери и Точность Дообучения Энкодера\n",
    "        fig.add_trace(go.Scatter(y=self.fine_tune_loss_log, mode=\"lines+markers\", name=\"Fine-tune Loss\"), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(y=self.fine_tune_acc_log, mode=\"lines+markers\", name=\"Fine-tune Accuracy\"), row=2, col=1)\n",
    "\n",
    "        # Итоговая Точность Модели до и после Дообучения\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[\"Before Fine-tuning\", \"After Fine-tuning\"], y=[self.acc_combined_before, self.acc_combined_after], name=\"Combined Model Accuracy\", marker_color=[\"blue\", \"green\"]\n",
    "            ),\n",
    "            row=2,\n",
    "            col=2,\n",
    "        )\n",
    "\n",
    "        fig.update_layout(height=800, width=1200, title_text=\"Анализ Обучения\", showlegend=True)\n",
    "        fig.show()\n",
    "\n",
    "    def perform_full_cycle(self, train_subset, test_subset, fine_tune_epochs=3, fine_tune_lr=0.0005):\n",
    "        \"\"\"\n",
    "        Выполняет полный цикл:\n",
    "        1. Обучение автокодировщика\n",
    "        2. Обучение бинарных классификаторов\n",
    "        3. Оценка модели\n",
    "        4. Сохранение моделей перед финетюнингом\n",
    "        5. Финетюнинг энкодера\n",
    "        6. Оценка модели после финетюнинга\n",
    "        7. Сохранение моделей после финетюнинга\n",
    "        \"\"\"\n",
    "        self.prepare_dataloaders(train_subset, test_subset)\n",
    "        self.train_autoencoder(epochs=epochs, lr=0.001)\n",
    "        self.train_binary_classifiers(epochs=epochs, lr=0.001)\n",
    "        self.evaluate_and_log()\n",
    "        self.save_models(suffix=\"before_finetune\")\n",
    "        self.fine_tune_encoder_after_cycle(fine_tune_epochs, fine_tune_lr)\n",
    "        self.evaluate_and_log(after_finetune=True)\n",
    "        self.save_models(suffix=\"after_finetune\")\n",
    "\n",
    "    def evaluate_and_log(self, after_finetune=False):\n",
    "        print(\"\\nОценка объединённой модели...\")\n",
    "        if after_finetune:\n",
    "            prefix = \"После финетюнинга\"\n",
    "        else:\n",
    "            prefix = \"До финетюнинга\"\n",
    "        print(f\"{prefix} оценки модели на тестовом наборе:\")\n",
    "        acc, error_rate, pred_all, target_all = self.evaluate_combined_model(self.test_loader)\n",
    "        self.accuracy_per_class(pred_all, target_all)\n",
    "\n",
    "    def fine_tune_encoder_after_cycle(self, fine_tune_epochs=3, fine_tune_lr=0.0005):\n",
    "        # Создаём выборку для дообучения\n",
    "        fine_tune_subset = create_fine_tuning_subset(train_dataset, self.num_classes, samples_per_class)\n",
    "        fine_tune_loader = DataLoader(fine_tune_subset, batch_size=self.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        self.fine_tune_encoder(fine_tune_loader, epochs=fine_tune_epochs, lr=fine_tune_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Подготовка Данных\n",
    "Создадим утилитные функции для подготовки датасетов и загрузчиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, num_classes, min_samples):\n",
    "    \"\"\"\n",
    "    Фильтрует датасет, оставляя только `num_classes` классов и минимум `min_samples` образцов на класс.\n",
    "    \"\"\"\n",
    "    class_counts = Counter()\n",
    "    class_indices = defaultdict(list)\n",
    "\n",
    "    # Собираем индексы для каждого класса\n",
    "    for idx, (_, target) in enumerate(dataset):\n",
    "        if target < num_classes:\n",
    "            class_indices[target].append(idx)\n",
    "            class_counts[target] += 1\n",
    "\n",
    "    # Проверяем, что каждый класс имеет минимум образцов\n",
    "    for cls in range(num_classes):\n",
    "        if class_counts[cls] < min_samples:\n",
    "            raise ValueError(f\"Класс {cls} имеет только {class_counts[cls]} образцов, требуется минимум {min_samples}.\")\n",
    "\n",
    "    # Ограничиваем количество образцов до min_samples для каждого класса\n",
    "    selected_indices = []\n",
    "    for cls in range(num_classes):\n",
    "        selected_indices.extend(class_indices[cls][:min_samples])\n",
    "\n",
    "    return Subset(dataset, selected_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Основной Скрипт\n",
    "Теперь объединим всё вместе в основном скрипте, который будет использовать класс Trainer для выполнения всех этапов обучения, сохранения и загрузки моделей.\n",
    "\n",
    "### 4.1. Ячейка 1: Обучение Автокодировщика и Бинарных Классификаторов, Сохранение Моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [06:23<00:00, 445kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "\n",
      "Инициализация и обучение автокодировщика...\n",
      "Эпоха 1/10, Потери AE: 0.822073, Время: 43.04 сек.\n",
      "Эпоха 2/10, Потери AE: 0.699883, Время: 43.32 сек.\n",
      "Эпоха 3/10, Потери AE: 0.676972, Время: 37.98 сек.\n",
      "Эпоха 4/10, Потери AE: 0.663614, Время: 40.47 сек.\n",
      "Эпоха 5/10, Потери AE: 0.647988, Время: 41.94 сек.\n",
      "Эпоха 6/10, Потери AE: 0.635761, Время: 40.25 сек.\n",
      "Эпоха 7/10, Потери AE: 0.628479, Время: 38.67 сек.\n",
      "Эпоха 8/10, Потери AE: 0.622967, Время: 39.38 сек.\n",
      "Эпоха 9/10, Потери AE: 0.619646, Время: 39.18 сек.\n",
      "Эпоха 10/10, Потери AE: 0.615170, Время: 41.07 сек.\n",
      "\n",
      "Обучение бинарных классификаторов для каждого класса...\n",
      "\n",
      "Обучение классификатора для класса 0 vs все остальные\n",
      "Epoch 1/10, Loss: 0.6561, Accuracy: 69.10%\n",
      "Epoch 2/10, Loss: 0.5052, Accuracy: 76.05%\n",
      "Epoch 3/10, Loss: 0.4786, Accuracy: 77.95%\n",
      "Epoch 4/10, Loss: 0.4755, Accuracy: 76.75%\n",
      "Epoch 5/10, Loss: 0.4543, Accuracy: 78.75%\n",
      "Epoch 6/10, Loss: 0.4363, Accuracy: 79.45%\n",
      "Epoch 7/10, Loss: 0.4422, Accuracy: 78.85%\n",
      "Epoch 8/10, Loss: 0.4238, Accuracy: 80.10%\n",
      "Epoch 9/10, Loss: 0.4177, Accuracy: 80.45%\n",
      "Epoch 10/10, Loss: 0.3987, Accuracy: 81.85%\n",
      "\n",
      "Обучение классификатора для класса 1 vs все остальные\n",
      "Epoch 1/10, Loss: 0.6538, Accuracy: 68.00%\n",
      "Epoch 2/10, Loss: 0.5426, Accuracy: 71.65%\n",
      "Epoch 3/10, Loss: 0.4720, Accuracy: 77.45%\n",
      "Epoch 4/10, Loss: 0.4748, Accuracy: 78.10%\n",
      "Epoch 5/10, Loss: 0.4670, Accuracy: 78.20%\n",
      "Epoch 6/10, Loss: 0.4226, Accuracy: 79.90%\n",
      "Epoch 7/10, Loss: 0.4144, Accuracy: 81.25%\n",
      "Epoch 8/10, Loss: 0.4055, Accuracy: 81.35%\n",
      "Epoch 9/10, Loss: 0.3893, Accuracy: 82.65%\n",
      "Epoch 10/10, Loss: 0.3872, Accuracy: 81.90%\n",
      "\n",
      "Обучение классификатора для класса 2 vs все остальные\n",
      "Epoch 1/10, Loss: 0.7277, Accuracy: 61.55%\n",
      "Epoch 2/10, Loss: 0.5966, Accuracy: 69.25%\n",
      "Epoch 3/10, Loss: 0.5611, Accuracy: 70.90%\n",
      "Epoch 4/10, Loss: 0.5480, Accuracy: 72.65%\n",
      "Epoch 5/10, Loss: 0.5321, Accuracy: 73.80%\n",
      "Epoch 6/10, Loss: 0.5091, Accuracy: 75.20%\n",
      "Epoch 7/10, Loss: 0.5075, Accuracy: 75.60%\n",
      "Epoch 8/10, Loss: 0.4981, Accuracy: 76.90%\n",
      "Epoch 9/10, Loss: 0.4933, Accuracy: 77.75%\n",
      "Epoch 10/10, Loss: 0.4837, Accuracy: 76.20%\n",
      "\n",
      "Обучение классификатора для класса 3 vs все остальные\n",
      "Epoch 1/10, Loss: 0.7092, Accuracy: 61.30%\n",
      "Epoch 2/10, Loss: 0.6220, Accuracy: 67.55%\n",
      "Epoch 3/10, Loss: 0.5740, Accuracy: 68.40%\n",
      "Epoch 4/10, Loss: 0.5554, Accuracy: 71.25%\n",
      "Epoch 5/10, Loss: 0.5343, Accuracy: 71.60%\n",
      "Epoch 6/10, Loss: 0.5181, Accuracy: 73.05%\n",
      "Epoch 7/10, Loss: 0.5045, Accuracy: 73.30%\n",
      "Epoch 8/10, Loss: 0.5075, Accuracy: 74.00%\n",
      "Epoch 9/10, Loss: 0.4950, Accuracy: 75.75%\n",
      "Epoch 10/10, Loss: 0.4902, Accuracy: 75.45%\n",
      "\n",
      "Обучение классификатора для класса 4 vs все остальные\n",
      "Epoch 1/10, Loss: 0.7214, Accuracy: 63.15%\n",
      "Epoch 2/10, Loss: 0.5896, Accuracy: 70.35%\n",
      "Epoch 3/10, Loss: 0.5634, Accuracy: 71.65%\n",
      "Epoch 4/10, Loss: 0.5351, Accuracy: 73.80%\n",
      "Epoch 5/10, Loss: 0.5195, Accuracy: 75.10%\n",
      "Epoch 6/10, Loss: 0.5276, Accuracy: 74.55%\n",
      "Epoch 7/10, Loss: 0.5069, Accuracy: 75.95%\n",
      "Epoch 8/10, Loss: 0.5023, Accuracy: 75.15%\n",
      "Epoch 9/10, Loss: 0.4821, Accuracy: 77.50%\n",
      "Epoch 10/10, Loss: 0.4921, Accuracy: 76.55%\n",
      "\n",
      "Обучение классификатора для класса 5 vs все остальные\n",
      "Epoch 1/10, Loss: 0.6727, Accuracy: 66.15%\n",
      "Epoch 2/10, Loss: 0.5653, Accuracy: 70.90%\n",
      "Epoch 3/10, Loss: 0.5244, Accuracy: 73.05%\n",
      "Epoch 4/10, Loss: 0.4979, Accuracy: 74.85%\n",
      "Epoch 5/10, Loss: 0.4941, Accuracy: 75.65%\n",
      "Epoch 6/10, Loss: 0.4851, Accuracy: 76.10%\n",
      "Epoch 7/10, Loss: 0.4785, Accuracy: 76.65%\n",
      "Epoch 8/10, Loss: 0.4661, Accuracy: 77.55%\n",
      "Epoch 9/10, Loss: 0.4627, Accuracy: 77.55%\n",
      "Epoch 10/10, Loss: 0.4415, Accuracy: 78.95%\n",
      "\n",
      "Обучение классификатора для класса 6 vs все остальные\n",
      "Epoch 1/10, Loss: 0.6376, Accuracy: 68.40%\n",
      "Epoch 2/10, Loss: 0.5440, Accuracy: 73.75%\n",
      "Epoch 3/10, Loss: 0.4670, Accuracy: 78.35%\n",
      "Epoch 4/10, Loss: 0.4411, Accuracy: 80.00%\n",
      "Epoch 5/10, Loss: 0.4279, Accuracy: 80.25%\n",
      "Epoch 6/10, Loss: 0.4341, Accuracy: 80.10%\n",
      "Epoch 7/10, Loss: 0.4176, Accuracy: 81.15%\n",
      "Epoch 8/10, Loss: 0.4114, Accuracy: 81.60%\n",
      "Epoch 9/10, Loss: 0.3913, Accuracy: 82.30%\n",
      "Epoch 10/10, Loss: 0.3928, Accuracy: 82.45%\n",
      "\n",
      "Обучение классификатора для класса 7 vs все остальные\n",
      "Epoch 1/10, Loss: 0.6824, Accuracy: 63.70%\n",
      "Epoch 2/10, Loss: 0.5723, Accuracy: 70.90%\n",
      "Epoch 3/10, Loss: 0.5238, Accuracy: 74.65%\n",
      "Epoch 4/10, Loss: 0.5025, Accuracy: 75.75%\n",
      "Epoch 5/10, Loss: 0.4761, Accuracy: 77.20%\n",
      "Epoch 6/10, Loss: 0.4734, Accuracy: 77.40%\n",
      "Epoch 7/10, Loss: 0.4498, Accuracy: 78.75%\n",
      "Epoch 8/10, Loss: 0.4375, Accuracy: 79.45%\n",
      "Epoch 9/10, Loss: 0.4287, Accuracy: 79.95%\n",
      "Epoch 10/10, Loss: 0.4252, Accuracy: 79.75%\n",
      "\n",
      "Обучение классификатора для класса 8 vs все остальные\n",
      "Epoch 1/10, Loss: 0.6138, Accuracy: 72.35%\n",
      "Epoch 2/10, Loss: 0.4885, Accuracy: 78.55%\n",
      "Epoch 3/10, Loss: 0.4476, Accuracy: 79.60%\n",
      "Epoch 4/10, Loss: 0.4335, Accuracy: 81.15%\n",
      "Epoch 5/10, Loss: 0.4205, Accuracy: 82.05%\n",
      "Epoch 6/10, Loss: 0.4152, Accuracy: 81.95%\n",
      "Epoch 7/10, Loss: 0.4062, Accuracy: 82.45%\n",
      "Epoch 8/10, Loss: 0.3907, Accuracy: 84.00%\n",
      "Epoch 9/10, Loss: 0.3894, Accuracy: 83.25%\n",
      "Epoch 10/10, Loss: 0.3771, Accuracy: 83.60%\n",
      "\n",
      "Обучение классификатора для класса 9 vs все остальные\n",
      "Epoch 1/10, Loss: 0.6506, Accuracy: 69.00%\n",
      "Epoch 2/10, Loss: 0.5216, Accuracy: 76.35%\n",
      "Epoch 3/10, Loss: 0.4889, Accuracy: 76.40%\n",
      "Epoch 4/10, Loss: 0.4589, Accuracy: 79.20%\n",
      "Epoch 5/10, Loss: 0.4562, Accuracy: 79.45%\n",
      "Epoch 6/10, Loss: 0.4295, Accuracy: 79.90%\n",
      "Epoch 7/10, Loss: 0.4270, Accuracy: 80.10%\n",
      "Epoch 8/10, Loss: 0.4095, Accuracy: 81.50%\n",
      "Epoch 9/10, Loss: 0.4069, Accuracy: 81.35%\n",
      "Epoch 10/10, Loss: 0.3939, Accuracy: 82.15%\n",
      "Combined model accuracy on all classes (0-9): 38.99%\n",
      "Error rate: 61.01%\n",
      "\n",
      "Accuracy per class:\n",
      "Class 0: 48.30%\n",
      "Class 1: 46.10%\n",
      "Class 2: 24.30%\n",
      "Class 3: 13.70%\n",
      "Class 4: 39.60%\n",
      "Class 5: 34.70%\n",
      "Class 6: 59.80%\n",
      "Class 7: 36.10%\n",
      "Class 8: 50.50%\n",
      "Class 9: 36.80%\n",
      "\n",
      "Сохранение моделей...\n",
      "Autoencoder сохранён по пути: ./saved_models\\autoencoder_before_finetune.pth\n",
      "CombinedBinaryClassifier сохранён по пути: ./saved_models\\combined_model_before_finetune.pth\n",
      "BinaryClassifier 0 сохранён по пути: ./saved_models\\binary_classifier_class_0_before_finetune.pth\n",
      "BinaryClassifier 1 сохранён по пути: ./saved_models\\binary_classifier_class_1_before_finetune.pth\n",
      "BinaryClassifier 2 сохранён по пути: ./saved_models\\binary_classifier_class_2_before_finetune.pth\n",
      "BinaryClassifier 3 сохранён по пути: ./saved_models\\binary_classifier_class_3_before_finetune.pth\n",
      "BinaryClassifier 4 сохранён по пути: ./saved_models\\binary_classifier_class_4_before_finetune.pth\n",
      "BinaryClassifier 5 сохранён по пути: ./saved_models\\binary_classifier_class_5_before_finetune.pth\n",
      "BinaryClassifier 6 сохранён по пути: ./saved_models\\binary_classifier_class_6_before_finetune.pth\n",
      "BinaryClassifier 7 сохранён по пути: ./saved_models\\binary_classifier_class_7_before_finetune.pth\n",
      "BinaryClassifier 8 сохранён по пути: ./saved_models\\binary_classifier_class_8_before_finetune.pth\n",
      "BinaryClassifier 9 сохранён по пути: ./saved_models\\binary_classifier_class_9_before_finetune.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Предполагается, что классы Autoencoder, MulticlassClassifier и Trainer уже определены выше\n",
    "\n",
    "# Настройки\n",
    "num_classes = 10\n",
    "latent_dim = 512\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "min_samples_per_class = 200  # Увеличено до 200\n",
    "max_test_samples = 10000\n",
    "save_dir = \"./saved_models\"\n",
    "\n",
    "# Подготовка трансформаций с аугментацией данных\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Загрузка датасетов\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Фильтрация датасетов\n",
    "train_subset = filter_dataset(train_dataset, num_classes, min_samples_per_class)\n",
    "test_subset = filter_dataset(test_dataset, num_classes, min_samples_per_class)\n",
    "\n",
    "# Ограничение тестового набора\n",
    "if len(test_subset) > max_test_samples:\n",
    "    test_indices = random.sample(range(len(test_subset)), max_test_samples)\n",
    "    test_subset = Subset(test_subset, test_indices)\n",
    "\n",
    "# Инициализация тренера\n",
    "trainer = Trainer(num_classes=num_classes, latent_dim=latent_dim, batch_size=batch_size, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", save_dir=save_dir)\n",
    "\n",
    "# Подготовка загрузчиков данных\n",
    "trainer.prepare_dataloaders(train_subset, test_subset)\n",
    "\n",
    "# Обучение автокодировщика\n",
    "trainer.train_autoencoder(epochs=epochs, lr=0.001)\n",
    "\n",
    "# Обучение многоклассового классификатора\n",
    "trainer.train_classifier(epochs=epochs, lr=0.001)\n",
    "\n",
    "# Оценка модели до финетюнинга\n",
    "trainer.acc_combined_before, trainer.error_rate_before, trainer.pred_all_before, trainer.target_all_before = trainer.evaluate_combined_model(\n",
    "    trainer.test_loader, before_finetune=True\n",
    ")\n",
    "\n",
    "# Подсчёт точности по каждому классу до финетюнинга\n",
    "trainer.accuracy_per_class(trainer.pred_all_before, trainer.target_all_before, before_finetune=True)\n",
    "\n",
    "# Сохранение моделей перед финетюнингом\n",
    "trainer.save_models(suffix=\"before_finetune\")\n",
    "\n",
    "# Визуализация метрик до финетюнинга\n",
    "trainer.plot_metrics(before_finetune=True, after_finetune=False)\n",
    "\n",
    "# Визуализация матрицы ошибок до финетюнинга\n",
    "trainer.plot_confusion_matrix(before_finetune=True, after_finetune=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Ячейка 2: Загрузка Моделей и Дообучение Энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Загрузка моделей...\n",
      "Autoencoder загружен из ./saved_models\\autoencoder_before_finetune.pth\n",
      "CombinedBinaryClassifier загружен из ./saved_models\\combined_model_before_finetune.pth\n",
      "BinaryClassifier 0 загружен из ./saved_models\\binary_classifier_class_0_before_finetune.pth\n",
      "BinaryClassifier 1 загружен из ./saved_models\\binary_classifier_class_1_before_finetune.pth\n",
      "BinaryClassifier 2 загружен из ./saved_models\\binary_classifier_class_2_before_finetune.pth\n",
      "BinaryClassifier 3 загружен из ./saved_models\\binary_classifier_class_3_before_finetune.pth\n",
      "BinaryClassifier 4 загружен из ./saved_models\\binary_classifier_class_4_before_finetune.pth\n",
      "BinaryClassifier 5 загружен из ./saved_models\\binary_classifier_class_5_before_finetune.pth\n",
      "BinaryClassifier 6 загружен из ./saved_models\\binary_classifier_class_6_before_finetune.pth\n",
      "BinaryClassifier 7 загружен из ./saved_models\\binary_classifier_class_7_before_finetune.pth\n",
      "BinaryClassifier 8 загружен из ./saved_models\\binary_classifier_class_8_before_finetune.pth\n",
      "BinaryClassifier 9 загружен из ./saved_models\\binary_classifier_class_9_before_finetune.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inimatic\\AppData\\Local\\Temp\\ipykernel_30612\\1945784555.py:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.autoencoder.load_state_dict(torch.load(autoenc_save_path))\n",
      "C:\\Users\\inimatic\\AppData\\Local\\Temp\\ipykernel_30612\\1945784555.py:195: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.combined_model.load_state_dict(torch.load(combined_model_save_path))\n",
      "C:\\Users\\inimatic\\AppData\\Local\\Temp\\ipykernel_30612\\1945784555.py:202: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.binary_classifiers[cls].load_state_dict(torch.load(classifier_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Дообучение энкодера на небольшой выборке с фиксированными классификаторами...\n",
      "Fine-tuning Epoch 1/10, Loss: 0.3167, Accuracy: 37.90%, Время: 12.21 сек.\n",
      "Fine-tuning Epoch 2/10, Loss: 0.2362, Accuracy: 45.75%, Время: 12.35 сек.\n",
      "Fine-tuning Epoch 3/10, Loss: 0.2136, Accuracy: 51.85%, Время: 12.56 сек.\n",
      "Fine-tuning Epoch 4/10, Loss: 0.1965, Accuracy: 58.20%, Время: 12.89 сек.\n",
      "Fine-tuning Epoch 5/10, Loss: 0.1838, Accuracy: 61.40%, Время: 14.11 сек.\n",
      "Fine-tuning Epoch 6/10, Loss: 0.1690, Accuracy: 66.85%, Время: 12.31 сек.\n",
      "Fine-tuning Epoch 7/10, Loss: 0.1552, Accuracy: 71.25%, Время: 12.05 сек.\n",
      "Fine-tuning Epoch 8/10, Loss: 0.1458, Accuracy: 74.50%, Время: 12.97 сек.\n",
      "Fine-tuning Epoch 9/10, Loss: 0.1275, Accuracy: 79.10%, Время: 11.92 сек.\n",
      "Fine-tuning Epoch 10/10, Loss: 0.1157, Accuracy: 82.55%, Время: 12.07 сек.\n",
      "Combined model accuracy on all classes (0-9): 48.11%\n",
      "Error rate: 51.89%\n",
      "\n",
      "Accuracy per class:\n",
      "Class 0: 48.80%\n",
      "Class 1: 41.20%\n",
      "Class 2: 61.10%\n",
      "Class 3: 15.20%\n",
      "Class 4: 38.20%\n",
      "Class 5: 35.40%\n",
      "Class 6: 68.70%\n",
      "Class 7: 53.60%\n",
      "Class 8: 66.80%\n",
      "Class 9: 52.10%\n",
      "\n",
      "Сохранение моделей...\n",
      "Autoencoder сохранён по пути: ./saved_models\\autoencoder_after_finetune.pth\n",
      "CombinedBinaryClassifier сохранён по пути: ./saved_models\\combined_model_after_finetune.pth\n",
      "BinaryClassifier 0 сохранён по пути: ./saved_models\\binary_classifier_class_0_after_finetune.pth\n",
      "BinaryClassifier 1 сохранён по пути: ./saved_models\\binary_classifier_class_1_after_finetune.pth\n",
      "BinaryClassifier 2 сохранён по пути: ./saved_models\\binary_classifier_class_2_after_finetune.pth\n",
      "BinaryClassifier 3 сохранён по пути: ./saved_models\\binary_classifier_class_3_after_finetune.pth\n",
      "BinaryClassifier 4 сохранён по пути: ./saved_models\\binary_classifier_class_4_after_finetune.pth\n",
      "BinaryClassifier 5 сохранён по пути: ./saved_models\\binary_classifier_class_5_after_finetune.pth\n",
      "BinaryClassifier 6 сохранён по пути: ./saved_models\\binary_classifier_class_6_after_finetune.pth\n",
      "BinaryClassifier 7 сохранён по пути: ./saved_models\\binary_classifier_class_7_after_finetune.pth\n",
      "BinaryClassifier 8 сохранён по пути: ./saved_models\\binary_classifier_class_8_after_finetune.pth\n",
      "BinaryClassifier 9 сохранён по пути: ./saved_models\\binary_classifier_class_9_after_finetune.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Определите классы Autoencoder, BinaryClassifier, CombinedBinaryClassifier и Trainer здесь или импортируйте из другого файла\n",
    "\n",
    "# Настройки\n",
    "num_classes = 10\n",
    "latent_dim = 512\n",
    "batch_size = 124  # Изменено пользователем\n",
    "fine_tuning_epochs = 10  # Изменено пользователем\n",
    "samples_per_class = 200\n",
    "save_dir = \"./saved_models\"\n",
    "\n",
    "# Инициализация тренера\n",
    "trainer = Trainer(num_classes=num_classes, latent_dim=latent_dim, batch_size=batch_size, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", save_dir=save_dir)\n",
    "\n",
    "# Определение трансформаций\n",
    "transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "# Загрузка датасета для дообучения\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "def create_fine_tuning_subset(dataset, num_classes, samples_per_class=100):\n",
    "    selected_indices = []\n",
    "    class_counts = Counter()\n",
    "    class_indices = defaultdict(list)\n",
    "\n",
    "    for idx, (_, target) in enumerate(dataset):\n",
    "        if target < num_classes and class_counts[target] < samples_per_class:\n",
    "            class_indices[target].append(idx)\n",
    "            class_counts[target] += 1\n",
    "            selected_indices.append(idx)\n",
    "        if all(count >= samples_per_class for count in class_counts.values()):\n",
    "            break\n",
    "\n",
    "    return Subset(dataset, selected_indices)\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, num_classes, min_samples):\n",
    "    \"\"\"\n",
    "    Фильтрует датасет, оставляя только `num_classes` классов и минимум `min_samples` образцов на класс.\n",
    "    \"\"\"\n",
    "    class_counts = Counter()\n",
    "    class_indices = defaultdict(list)\n",
    "\n",
    "    # Собираем индексы для каждого класса\n",
    "    for idx, (_, target) in enumerate(dataset):\n",
    "        if target < num_classes:\n",
    "            class_indices[target].append(idx)\n",
    "            class_counts[target] += 1\n",
    "\n",
    "    # Проверяем, что каждый класс имеет минимум образцов\n",
    "    for cls in range(num_classes):\n",
    "        if class_counts[cls] < min_samples:\n",
    "            raise ValueError(f\"Класс {cls} имеет только {class_counts[cls]} образцов, требуется минимум {min_samples}.\")\n",
    "\n",
    "    # Ограничиваем количество образцов до min_samples для каждого класса\n",
    "    selected_indices = []\n",
    "    for cls in range(num_classes):\n",
    "        selected_indices.extend(class_indices[cls][:min_samples])\n",
    "\n",
    "    return Subset(dataset, selected_indices)\n",
    "\n",
    "\n",
    "# Создание выборки для дообучения\n",
    "fine_tune_subset = create_fine_tuning_subset(train_dataset, num_classes, samples_per_class)\n",
    "fine_tune_loader = DataLoader(fine_tune_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Загрузка сохранённых моделей\n",
    "trainer.load_models(suffix=\"before_finetune\")\n",
    "\n",
    "# Загрузка и фильтрация тестового датасета\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_subset = filter_dataset(test_dataset, num_classes, min_samples=1000)  # Используем тот же min_samples_per_class\n",
    "\n",
    "# Подготовка загрузчиков данных (инициализирует test_loader)\n",
    "trainer.prepare_dataloaders(fine_tune_subset, test_subset)\n",
    "\n",
    "# Дообучение энкодера\n",
    "trainer.fine_tune_encoder(fine_tune_loader, epochs=fine_tuning_epochs, lr=0.0005)\n",
    "\n",
    "# Оценка объединённой модели после дообучения\n",
    "trainer.acc_combined_after, trainer.error_rate_after, pred_all_after, target_all_after = trainer.evaluate_combined_model(trainer.test_loader)\n",
    "\n",
    "# Подсчёт точности по каждому классу\n",
    "trainer.accuracy_per_class(pred_all_after, target_all_after)\n",
    "\n",
    "# Сохранение моделей после дообучения\n",
    "trainer.save_models(suffix=\"after_finetune\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
