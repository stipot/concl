{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# сравнение функций потерь от сжатого и сырого контекстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная цель оптимизации — обучить отдельную сеть или модуль, который будет динамически сжимать или изменять контекст так, чтобы предсказания исходной языковой модели оставались точными.\n",
    "\n",
    "### Упрощенная Архитектура для Оптимизации Контекста\n",
    "\n",
    "#### Цель\n",
    "Обучить модуль сжатия контекста, который будет оптимизировать входной контекст для существующей языковой модели, минимизируя объем и избыточность данных, но сохраняя все необходимые элементы для точных предсказаний.\n",
    "\n",
    "### Структура и Процесс Обучения\n",
    "\n",
    "1. **Имеющаяся Языковая Модель:**\n",
    "   - Пусть \\( M \\) — это наша основная языковая модель, которая предсказывает следующие слова или предложения на основе текущего фрагмента и контекста.\n",
    "   - Модель \\( M \\) обладает хорошей точностью предсказаний при использовании полной предыстории.\n",
    "\n",
    "2. **Модуль Оптимизации Контекста:**\n",
    "   - Добавляем модуль \\( O \\), который принимает на вход предысторию \\( C \\) и текущий фрагмент \\( S \\) и выдает сжатый или оптимизированный контекст \\( C' \\).\n",
    "   - Модуль \\( O \\) может быть реализован как легковесная нейронная сеть, например, на основе рекуррентных сетей или слоев внимания, которая фокусируется на выделении ключевых частей контекста.\n",
    "\n",
    "3. **Обучение Модуля Оптимизации:**\n",
    "   - **Вход:** Предыстория \\( C \\) и текущий фрагмент \\( S \\).\n",
    "   - **Выход:** Сжатый контекст \\( C' = O(C, S) \\).\n",
    "   - **Целевая Функция:** Оптимизировать \\( C' \\) так, чтобы предсказания модели \\( M \\) на основе \\( C' \\) и \\( S \\) оставались максимально близкими к предсказаниям на основе полной предыстории \\( C \\).\n",
    "   - Используем функцию потерь:\n",
    "     \\[\n",
    "     L(\\hat{Y}_{C'}, \\hat{Y}_{C}) = \\text{Loss}(M(C', S), M(C, S))\n",
    "     \\]\n",
    "   - Здесь \\(\\hat{Y}_{C'}\\) и \\(\\hat{Y}_{C}\\) — предсказания модели \\( M \\) на основе сжатого и полного контекста соответственно.\n",
    "\n",
    "4. **Процесс Обучения:**\n",
    "   - **Шаг 1:** Для каждой последовательности данных создается пара: полный контекст и текущий фрагмент.\n",
    "   - **Шаг 2:** Модуль \\( O \\) сжимает контекст, производя \\( C' \\).\n",
    "   - **Шаг 3:** Языковая модель \\( M \\) делает предсказания на основе \\( C' \\) и сравнивается с предсказаниями на основе \\( C \\).\n",
    "   - **Шаг 4:** Функция потерь минимизируется для обучения модуля \\( O \\) таким образом, чтобы сжатие контекста минимально влияло на точность предсказаний.\n",
    "\n",
    "### Ожидаемый Результат\n",
    "\n",
    "- Модуль \\( O \\) должен научиться выделять только те части контекста, которые действительно необходимы для точных предсказаний модели \\( M \\).\n",
    "- Сжатый контекст \\( C' \\) будет специфичен для данной языковой модели и адаптирован для минимизации избыточности без потери точности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модульная структура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательные классы и методы\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "\n",
    "# Подготовка данных\n",
    "def prepare_data(example, max_length=50):\n",
    "    text = example[\"text\"].strip()\n",
    "    if len(text) == 0 or \".\" not in text:\n",
    "        return None, None\n",
    "\n",
    "    split_point = text.find(\".\") + 1\n",
    "    if split_point >= len(text):\n",
    "        return None, None\n",
    "\n",
    "    context = text[:split_point].strip()  # Предыстория до первой точки\n",
    "    current_fragment = text[split_point : split_point + max_length].strip()  # Текущий фрагмент\n",
    "\n",
    "    if len(context) == 0 or len(current_fragment) == 0:\n",
    "        return None, None\n",
    "\n",
    "    return context, current_fragment\n",
    "\n",
    "\n",
    "# Функции для сохранения и загрузки модели\n",
    "def save_model(model, optimizer, path=\"context_optimizer.pth\"):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, path=\"context_optimizer.pth\"):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "    else:\n",
    "        print(f\"Model file not found at {path}\")\n",
    "\n",
    "\n",
    "# Визуализация с использованием Plotly FigureWidget\n",
    "def create_loss_plot():\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Raw Context Loss\"))\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Compressed Context Loss\"))\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Fragment Only Loss\"))\n",
    "    fig.update_layout(title=\"Графики потерь во время обучения\", xaxis_title=\"Эпоха\", yaxis_title=\"Среднее значение потерь\", template=\"plotly_dark\")\n",
    "    display(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def update_loss_plot(fig, epoch, raw_loss, compressed_loss, fragment_loss):\n",
    "    fig.data[0].x += (epoch,)\n",
    "    fig.data[0].y += (raw_loss,)\n",
    "    fig.data[1].x += (epoch,)\n",
    "    fig.data[1].y += (compressed_loss,)\n",
    "    fig.data[2].x += (epoch,)\n",
    "    fig.data[2].y += (fragment_loss,)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Функция для расчета compression ratio\n",
    "def calculate_compression_ratio(raw_embedding, compressed_embedding):\n",
    "    raw_norm = torch.norm(raw_embedding, p=2, dim=1)\n",
    "    compressed_norm = torch.norm(compressed_embedding, p=2, dim=1)\n",
    "    return compressed_norm / raw_norm\n",
    "\n",
    "\n",
    "# Функция для расчета compression ratio\n",
    "def calculate_compression_ratio(raw_embedding, compressed_embedding):\n",
    "    raw_norm = torch.norm(raw_embedding, p=2, dim=1)\n",
    "    compressed_norm = torch.norm(compressed_embedding, p=2, dim=1)\n",
    "    return compressed_norm / raw_norm\n",
    "\n",
    "\n",
    "# Предсказание на основе сжатого контекста\n",
    "def predict_with_compressed_context(compressed_context, current_fragment, tokenizer, gpt2_model, device):\n",
    "    inputs = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(input_ids=input_ids, output_hidden_states=True)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
    "    predicted_token = tokenizer.decode([predicted_index])\n",
    "    return predicted_token\n",
    "\n",
    "\n",
    "# Предсказание на основе сырого контекста\n",
    "def predict_with_raw_context(raw_context_embedding, current_fragment, tokenizer, gpt2_model, device):\n",
    "    inputs = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(input_ids=input_ids, output_hidden_states=True)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
    "    predicted_token = tokenizer.decode([predicted_index])\n",
    "    return predicted_token\n",
    "\n",
    "\n",
    "# Предсказание только по текущему фрагменту\n",
    "def predict_with_fragment(current_fragment, tokenizer, gpt2_model, device):\n",
    "    inputs = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(input_ids=input_ids, output_hidden_states=True)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
    "    predicted_token = tokenizer.decode([predicted_index])\n",
    "    return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель сжатия контекста\n",
    "\n",
    "\n",
    "class ContextOptimizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ContextOptimizer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, context):\n",
    "        x = self.relu(self.fc1(context))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Функция для сжатия контекста\n",
    "def compress_context(context, model, tokenizer, gpt2_model, device):\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(**inputs, output_hidden_states=True)\n",
    "        context_embedding = outputs.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "    compressed_context = model(context_embedding)\n",
    "    return compressed_context, context_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_context_optimizer(optimizer, dataset, tokenizer, gpt2_model, epochs=10, device=\"cpu\"):\n",
    "    criterion = nn.MSELoss()\n",
    "    opt = optim.Adam(optimizer.parameters(), lr=0.001)\n",
    "    raw_loss_values = []\n",
    "    compressed_loss_values = []\n",
    "    fragment_loss_values = []\n",
    "    compression_ratios = []\n",
    "\n",
    "    # Инициализация графика потерь\n",
    "    loss_fig = create_loss_plot()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_raw_loss = 0\n",
    "        total_compressed_loss = 0\n",
    "        total_fragment_loss = 0\n",
    "        total_compression_ratio = 0\n",
    "        count = 0\n",
    "\n",
    "        examples = []  # Список для сохранения примеров для диагностики\n",
    "        start_time = time.time()\n",
    "        time_measurements = []\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            context, current_fragment = prepare_data(dataset[i])\n",
    "            if context is None or current_fragment is None:\n",
    "                continue\n",
    "            cycle_start_time = time.time()\n",
    "            compressed_context, raw_context_embedding = compress_context(context, optimizer, tokenizer, gpt2_model, device)\n",
    "\n",
    "            # Подготовка целевого предсказания\n",
    "            target_input = tokenizer(current_fragment + \" \", return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "                target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "            # Вычисление потерь для сжатого контекста в комбинации с текущим фрагментом\n",
    "            compressed_inputs = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "            compressed_inputs_embeds = compressed_context + gpt2_model.transformer.wte(compressed_inputs[\"input_ids\"])\n",
    "            # Исправление: включаем output_hidden_states=True\n",
    "            compressed_outputs = gpt2_model(inputs_embeds=compressed_inputs_embeds, output_hidden_states=True).hidden_states[-1].mean(dim=1)\n",
    "            compressed_loss = criterion(compressed_outputs, target_embedding)\n",
    "\n",
    "            # Вычисление потерь для полного контекста в комбинации с текущим фрагментом\n",
    "            raw_inputs = tokenizer(context + \" \" + current_fragment, return_tensors=\"pt\").to(device)\n",
    "            raw_outputs = gpt2_model(**raw_inputs, output_hidden_states=True)\n",
    "            raw_combined_embedding = raw_outputs.hidden_states[-1].mean(dim=1)\n",
    "            raw_loss = criterion(raw_combined_embedding, target_embedding)\n",
    "\n",
    "            # Вычисление потерь для предсказания только по текущему фрагменту\n",
    "            fragment_input = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                fragment_output = gpt2_model(**fragment_input, output_hidden_states=True)\n",
    "                fragment_embedding = fragment_output.hidden_states[-1].mean(dim=1)\n",
    "            fragment_loss = criterion(fragment_embedding, target_embedding)\n",
    "\n",
    "            # Вычисление compression ratio\n",
    "            compression_ratio = calculate_compression_ratio(raw_context_embedding, compressed_context)\n",
    "            total_compression_ratio += compression_ratio.item()\n",
    "\n",
    "            # Обновление оптимизатора только по сжатым потерям\n",
    "            opt.zero_grad()\n",
    "            compressed_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_raw_loss += raw_loss.item()\n",
    "            total_compressed_loss += compressed_loss.item()\n",
    "            total_fragment_loss += fragment_loss.item()\n",
    "            count += 1\n",
    "            if i == 4:  # После 5-го цикла оцениваем время эпохи\n",
    "                avg_time_per_cycle = sum(time_measurements) / len(time_measurements)\n",
    "                time_per_epoch_estimation = avg_time_per_cycle * len(dataset)\n",
    "                print(f\"Estimated time for epoch: {time_per_epoch_estimation:.2f} seconds\")\n",
    "\n",
    "            # Сохранение примеров для диагностики\n",
    "            if len(examples) < 3:  # Сохраняем три примера для вывода\n",
    "                with torch.no_grad():\n",
    "                    compressed_pred = predict_with_compressed_context(compressed_context, current_fragment, tokenizer, gpt2_model, device)\n",
    "                    raw_pred = predict_with_raw_context(raw_context_embedding, current_fragment, tokenizer, gpt2_model, device)\n",
    "                    fragment_pred = predict_with_fragment(current_fragment, tokenizer, gpt2_model, device)\n",
    "\n",
    "                # Проверяем и обрабатываем compressed_context корректно перед декодированием\n",
    "                if compressed_context is not None and isinstance(compressed_context, torch.Tensor):\n",
    "                    # Пытаемся преобразовать в список токенов, исключая None\n",
    "                    compressed_tokens = [token for token in compressed_context.squeeze().tolist() if isinstance(token, int)]\n",
    "                    compressed_context_text = tokenizer.decode(compressed_tokens) if compressed_tokens else \"N/A\"\n",
    "                else:\n",
    "                    compressed_context_text = \"N/A\"\n",
    "\n",
    "                examples.append(\n",
    "                    {\n",
    "                        \"context\": context,\n",
    "                        \"compressed_context\": compressed_context_text,\n",
    "                        \"current_fragment\": current_fragment,\n",
    "                        \"compressed_prediction\": compressed_pred,\n",
    "                        \"raw_prediction\": raw_pred,\n",
    "                        \"fragment_prediction\": fragment_pred,\n",
    "                        \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        avg_raw_loss = total_raw_loss / count if count > 0 else 0\n",
    "        avg_compressed_loss = total_compressed_loss / count if count > 0 else 0\n",
    "        avg_fragment_loss = total_fragment_loss / count if count > 0 else 0\n",
    "        avg_compression_ratio = total_compression_ratio / count if count > 0 else 0\n",
    "        raw_loss_values.append(avg_raw_loss)\n",
    "        compressed_loss_values.append(avg_compressed_loss)\n",
    "        fragment_loss_values.append(avg_fragment_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}, Raw Context Loss: {avg_raw_loss:.4f}, Compressed Context Loss: {avg_compressed_loss:.4f}, Fragment Loss: {avg_fragment_loss:.4f}, Avg Compression Ratio: {avg_compression_ratio:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Обновление графика после каждой эпохи\n",
    "        update_loss_plot(loss_fig, epoch + 1, avg_raw_loss, avg_compressed_loss, avg_fragment_loss)\n",
    "\n",
    "        # Вывод примеров для диагностики\n",
    "        print(f\"\\n=== Диагностика после эпохи {epoch+1} ===\")\n",
    "        for idx, example in enumerate(examples):\n",
    "            print(f\"\\nПример {idx+1}:\")\n",
    "            print(f\"Предыстория: {example['context']}\")\n",
    "            print(f\"Сжатый контекст: {example['compressed_context']}\")\n",
    "            print(f\"Текущий фрагмент: {example['current_fragment']}\")\n",
    "            print(f\"Предсказание по сжатому контексту: {example['compressed_prediction']}\")\n",
    "            print(f\"Предсказание по сырому контексту: {example['raw_prediction']}\")\n",
    "            print(f\"Предсказание по текущему фрагменту: {example['fragment_prediction']}\")\n",
    "            print(f\"Целевой текст: {example['target']}\")\n",
    "    return raw_loss_values, compressed_loss_values, fragment_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model file not found at context_optimizer.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf3cf4e66b346cb8506d9f4e7ece98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Raw Context Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '94ad05bb-3a0e-464f-9996-1140a2ce9dab',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'mode': 'lines+markers',\n",
       "              'name': 'Compressed Context Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '2368fe70-416f-4651-98fc-cd70ad6384ef',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'mode': 'lines+markers',\n",
       "              'name': 'Fragment Only Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '2a8e436d-b960-423f-8b24-c8ed998288bf',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Графики потерь во время обучения'},\n",
       "               'xaxis': {'title': {'text': 'Эпоха'}},\n",
       "               'yaxis': {'title': {'text': 'Среднее значение потерь'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Инициализация и запуск обучения\n",
    "\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Настройки\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка модели GPT-2 и токенизатора\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Загрузка датасета\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Инициализация модели сжатия контекста\n",
    "input_size = 768\n",
    "hidden_size = 256\n",
    "context_optimizer = ContextOptimizer(input_size, hidden_size).to(device)\n",
    "\n",
    "# Загрузка сохраненной модели, если есть\n",
    "load_model(context_optimizer, context_optimizer, \"context_optimizer.pth\")\n",
    "\n",
    "# Запуск обучения\n",
    "raw_loss_values, compressed_loss_values, fragment_loss_values = train_context_optimizer(context_optimizer, dataset, tokenizer, gpt2_model, epochs=10, device=device)\n",
    "\n",
    "# Сохранение модели после обучения\n",
    "save_model(context_optimizer, context_optimizer, \"context_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Слитная архитектура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9ad2610d1946a8a968b992ea94735a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'color': 'blue'},\n",
       "              'mode': 'lines+markers',\n",
       "              'name': 'Raw Context Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '78a544dd-cba2-4c31-81fb-0e62f1522d6b',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'line': {'color': 'red'},\n",
       "              'mode': 'lines+markers',\n",
       "              'name': 'Compressed Context Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'f01167cd-c8ea-422e-848c-0181ec70fcb2',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Графики потерь во время обучения'},\n",
       "               'xaxis': {'title': {'text': 'Эпоха'}},\n",
       "               'yaxis': {'title': {'text': 'Среднее значение потерь'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Raw Context Loss: 1.6017, Compressed Context Loss: 0.9392, Epoch Time: 37.61 minutes\n",
      "Epoch 2, Raw Context Loss: 1.6017, Compressed Context Loss: 0.7524, Epoch Time: 37.78 minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 150\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m raw_loss_values, compressed_loss_values\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Запуск обучения и сохранение значений потерь\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m raw_loss_values, compressed_loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_context_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Сохранение модели\u001b[39;00m\n\u001b[0;32m    153\u001b[0m save_model(optimizer, optimizer)\n",
      "Cell \u001b[1;32mIn[5], line 103\u001b[0m, in \u001b[0;36mtrain_context_optimizer\u001b[1;34m(optimizer, dataset, epochs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Сжатие контекста\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m compressed_context \u001b[38;5;241m=\u001b[39m \u001b[43mcompress_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Получение эмбеддингов для сырого контекста\u001b[39;00m\n\u001b[0;32m    106\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(context, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[5], line 67\u001b[0m, in \u001b[0;36mcompress_context\u001b[1;34m(context, optimizer)\u001b[0m\n\u001b[0;32m     65\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(context, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 67\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m     context_embedding \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Среднее по последнему слою\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Сжатие контекста через оптимизатор\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1315\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1315\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1129\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1118\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1119\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         output_attentions,\n\u001b[0;32m   1127\u001b[0m     )\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:651\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    649\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    650\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 651\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    653\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:576\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    574\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m    575\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m--> 576\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\pytorch_utils.py:105\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    104\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[1;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Импорт необходимых модулей\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch import nn, optim\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Проверка доступности устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка модели GPT-2 и токенизатора\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Перевод модели в режим оценки\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Определение модуля оптимизации контекста\n",
    "class ContextOptimizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ContextOptimizer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, context):\n",
    "        x = self.relu(self.fc1(context))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Параметры оптимизатора\n",
    "input_size = 768  # Размер скрытого состояния модели GPT-2\n",
    "hidden_size = 256\n",
    "optimizer = ContextOptimizer(input_size, hidden_size).to(device)\n",
    "\n",
    "# Загрузка датасета \"wikitext\"\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "# Функция для подготовки данных: извлечение контекста и текущего фрагмента\n",
    "def prepare_data(example, max_length=50):\n",
    "    text = example[\"text\"].strip()\n",
    "    if len(text) == 0 or \".\" not in text:\n",
    "        return None, None\n",
    "\n",
    "    # Разделение текста на предысторию и текущий фрагмент\n",
    "    split_point = text.find(\".\") + 1\n",
    "    if split_point >= len(text):\n",
    "        return None, None\n",
    "\n",
    "    context = text[:split_point].strip()  # Предыстория до первой точки\n",
    "    current_fragment = text[split_point : split_point + max_length].strip()  # Текущий фрагмент\n",
    "\n",
    "    if len(context) == 0 or len(current_fragment) == 0:\n",
    "        return None, None\n",
    "\n",
    "    return context, current_fragment\n",
    "\n",
    "\n",
    "# Функция для сжатия контекста\n",
    "def compress_context(context, optimizer):\n",
    "    # Получение эмбеддингов контекста из токенов\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        context_embedding = outputs.hidden_states[-1].mean(dim=1)  # Среднее по последнему слою\n",
    "\n",
    "    # Сжатие контекста через оптимизатор\n",
    "    compressed_context = optimizer(context_embedding)\n",
    "    return compressed_context\n",
    "\n",
    "\n",
    "# Функция потерь и обучение с обновлением графика после каждой эпохи\n",
    "def train_context_optimizer(optimizer, dataset, epochs=10):\n",
    "    criterion = nn.MSELoss()  # Можно использовать другие функции потерь\n",
    "    opt = optim.Adam(optimizer.parameters(), lr=0.001)\n",
    "    raw_loss_values = []  # Список для хранения значений потерь с сырым контекстом\n",
    "    compressed_loss_values = []  # Список для хранения значений потерь с сжатым контекстом\n",
    "\n",
    "    # Создание интерактивного графика\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_scatter(x=[], y=[], mode=\"lines+markers\", name=\"Raw Context Loss\", line=dict(color=\"blue\"))\n",
    "    fig.add_scatter(x=[], y=[], mode=\"lines+markers\", name=\"Compressed Context Loss\", line=dict(color=\"red\"))\n",
    "    fig.update_layout(title=\"Графики потерь во время обучения\", xaxis_title=\"Эпоха\", yaxis_title=\"Среднее значение потерь\", template=\"plotly_dark\")\n",
    "    display(fig)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_raw_loss = 0\n",
    "        total_compressed_loss = 0\n",
    "        count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            context, current_fragment = prepare_data(dataset[i])\n",
    "            if context is None or current_fragment is None:\n",
    "                continue\n",
    "\n",
    "            # Сжатие контекста\n",
    "            compressed_context = compress_context(context, optimizer)\n",
    "\n",
    "            # Получение эмбеддингов для сырого контекста\n",
    "            inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                raw_outputs = model(**inputs, output_hidden_states=True)\n",
    "                raw_context_embedding = raw_outputs.hidden_states[-1].mean(dim=1)  # Среднее по последнему слою\n",
    "\n",
    "            # Подготовка целевого предсказания для функции потерь\n",
    "            target_input = tokenizer(current_fragment + \" \", return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                target_output = model(**target_input, output_hidden_states=True)\n",
    "                target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "            # Вычисление потерь для сырого контекста\n",
    "            raw_loss = criterion(raw_context_embedding, target_embedding)\n",
    "\n",
    "            # Вычисление потерь для сжатого контекста\n",
    "            compressed_loss = criterion(compressed_context, target_embedding)\n",
    "\n",
    "            # Обновление оптимизатора только по сжатым потерям\n",
    "            opt.zero_grad()\n",
    "            compressed_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_raw_loss += raw_loss.item()\n",
    "            total_compressed_loss += compressed_loss.item()\n",
    "            count += 1\n",
    "\n",
    "        avg_raw_loss = total_raw_loss / count if count > 0 else 0\n",
    "        avg_compressed_loss = total_compressed_loss / count if count > 0 else 0\n",
    "        raw_loss_values.append(avg_raw_loss)  # Сохранение среднего значения потерь для сырого контекста\n",
    "        compressed_loss_values.append(avg_compressed_loss)  # Сохранение среднего значения потерь для сжатого контекста\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - start_time\n",
    "        print(f\"Epoch {epoch+1}, Raw Context Loss: {avg_raw_loss:.4f}, Compressed Context Loss: {avg_compressed_loss:.4f}, Epoch Time: {epoch_time / 60:.2f} minutes\")\n",
    "\n",
    "        # Обновление графика после каждой эпохи\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = list(range(1, len(raw_loss_values) + 1))\n",
    "            fig.data[0].y = raw_loss_values\n",
    "            fig.data[1].x = list(range(1, len(compressed_loss_values) + 1))\n",
    "            fig.data[1].y = compressed_loss_values\n",
    "\n",
    "    return raw_loss_values, compressed_loss_values\n",
    "\n",
    "\n",
    "# Запуск обучения и сохранение значений потерь\n",
    "raw_loss_values, compressed_loss_values = train_context_optimizer(optimizer, dataset)\n",
    "\n",
    "# Сохранение модели\n",
    "save_model(optimizer, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
