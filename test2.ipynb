{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# сравнение функций потерь от сжатого и сырого контекстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* Отладка. Проблема Сжатый контекст: N/A\n",
    "* Пересмотреть функцию потерь, чтобы она отражала задачу сжатия более точно (например, использовать потери восстановления или схожести между оригиналом и сжатым контекстом).\n",
    "* Усложнить архитектуру модели ContextOptimizer, добавив больше слоев или более продвинутые блоки, такие как GRU или LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основная цель\n",
    "Обучить отдельную сеть \\( O \\) (модуль оптимизации контекста), которая динамически изменяет контекст таким образом, чтобы предсказания исходной языковой модели \\( M \\) оставались точными, несмотря на уменьшение объема информации в контексте.\n",
    "\n",
    "### Структура и Процесс Обучения\n",
    "\n",
    "1. **Основная Языковая Модель \\( M \\):**\n",
    "   - \\( M \\) — существующая языковая модель, такая как GPT-2, которая принимает на вход полный контекст и текущий фрагмент и генерирует предсказания.\n",
    "   - Цель \\( M \\) — максимальная точность предсказаний на основе полного контекста.\n",
    "\n",
    "2. **Модуль Оптимизации Контекста \\( O \\):**\n",
    "   - Модуль \\( O \\) используется для сжатия предыстории \\( C \\) с учетом текущего фрагмента \\( S \\), чтобы создать оптимизированный контекст \\( C' \\).\n",
    "   - Модуль \\( O \\) может быть реализован через нейронные сети, такие как трансформеры, рекуррентные сети или другие архитектуры, способные выделять ключевые элементы контекста.\n",
    "\n",
    "3. **Целевая Функция и Метрики:**\n",
    "   - Основной задачей является минимизация расхождения между предсказаниями модели \\( M \\) на основе \\( C' \\) и \\( C \\). Целевая функция может быть определена как:\n",
    "     \\[\n",
    "     L = \\text{Loss}(M(C', S), M(C, S))\n",
    "     \\]\n",
    "   - Для измерения качества сжатия можно использовать метрики, такие как среднеквадратичная ошибка (MSE) между эмбеддингами предсказаний, кросс-энтропия, и т.д.\n",
    "   - Важно также отслеживать сходимость потерь между \\( C' \\) и \\( C \\), чтобы убедиться, что \\( C' \\) минимально влияет на точность предсказаний.\n",
    "\n",
    "4. **Процесс Обучения:**\n",
    "   - **Шаг 1:** Из датасета создаются пары \\( (C, S, T) \\), где \\( C \\) — предыстория, \\( S \\) — текущий фрагмент, и \\( T \\) — целевой текст для предсказания.\n",
    "   - **Шаг 2:** Модуль \\( O \\) сжимает контекст \\( C \\), производя \\( C' = O(C, S) \\).\n",
    "   - **Шаг 3:** Модель \\( M \\) предсказывает на основе \\( (C', S) \\) и \\( (C, S) \\). Сравниваются предсказания для оценки потерь.\n",
    "   - **Шаг 4:** Потери используются для обновления параметров модуля \\( O \\), минимизируя влияние сжатия на точность.\n",
    "\n",
    "5. **Анализ и Диагностика:**\n",
    "   - Постепенно выводим промежуточные результаты для диагностики, включая анализ предсказаний по сжатому и полному контексту.\n",
    "   - Выполняем оценку примеров, где предсказания на основе сжатого контекста значительно отличаются от предсказаний на основе полного контекста.\n",
    "   - Используем фильтрацию данных для обучения, например, исключаем случаи, где текущий фрагмент уже достаточен для точного предсказания.\n",
    "\n",
    "### Рекомендации для Улучшения\n",
    "\n",
    "- **Итеративная Диагностика:** Важно регулярно проверять, насколько сжатый контекст \\( C' \\) сохраняет критическую информацию. Диагностика на нескольких примерах может выявить случаи, где \\( O \\) недооценивает важные части контекста.\n",
    "- **Контроль за Переобучением:** Следите за тем, чтобы модель \\( O \\) не начинала генерировать «фантазии» или добавлять избыточные детали в попытках компенсировать потерю информации.\n",
    "- **Улучшение Сходимости:** Добавьте регуляризацию или дополнительные ограничения на модель \\( O \\), чтобы предотвратить отклонения в сжатии, которые могут ухудшить точность предсказаний.\n",
    "\n",
    "### Ожидаемый Результат\n",
    "\n",
    "- Модуль \\( O \\) научится выделять только те части контекста, которые критически важны для точных предсказаний, избегая ненужного увеличения объема информации.\n",
    "- Модель \\( M \\), используя \\( C' \\), будет предсказывать с минимальной потерей точности, что укажет на успешное сжатие и оптимизацию контекста. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Подготовка Данных**\n",
    "   - **Предварительная обработка данных:** Датасет разбивается на последовательности: предыстория (context), текущий фрагмент (current_fragment), и целевой текст (target).\n",
    "   - **Фильтрация данных:** Удаляются пустые и нерелевантные записи, например, те, где текущий фрагмент не вносит значимого вклада в предсказание.\n",
    "   - **Предварительная оценка потерь:** Вычисляются потери для различных комбинаций контекста, что позволяет заранее оценить вклад контекста в точность предсказания.\n",
    "\n",
    "### 2. **Оптимизация Контекста**\n",
    "   - **Целевая функция:** Используется MSELoss для измерения расхождения между предсказаниями модели и целевыми значениями.\n",
    "   - **Сходимость потерь:** Основная цель — сходимость потерь сжатого контекста (compressed_loss) к потерям сырого контекста (raw_loss).\n",
    "   - **Контроль за «додумыванием»:** Введены штрафы за значительное отклонение между потерями сжатого и сырого контекста, чтобы предотвратить генерацию ложных зависимостей.\n",
    "\n",
    "### 3. **Адаптивная Регуляция Потерь**\n",
    "   - **Штрафы за расхождения:** Если потери сжатого контекста значительно отличаются от потерь сырого, в потери добавляется штраф, который стимулирует модель уменьшать эти различия.\n",
    "   - **Адаптивное обучение:** Обучение адаптируется на основе расхождений, улучшая способность модели сжимать контекст, сохраняя при этом точность.\n",
    "\n",
    "### 4. **Использование Промежуточных Датасетов**\n",
    "   - **Сохранение промежуточных результатов:** Включение предсказаний модели в промежуточный датасет, чтобы ускорить и упростить анализ, отладку и последующую оптимизацию.\n",
    "   - **Фильтрация на этапе подготовки:** Отбираются записи, в которых контекст оказывает значительное влияние на предсказания, отбрасывая те, где контекст несущественен.\n",
    "\n",
    "### 5. **Визуализация и Отладка**\n",
    "   - **Графики потерь:** Построение графиков с использованием Plotly для отслеживания динамики потерь сжатого и сырого контекстов.\n",
    "   - **Примеры для анализа:** Регулярный вывод примеров с различными типами предсказаний (сжатый контекст, сырой контекст, текущий фрагмент) для визуального анализа и проверки корректности работы модели.\n",
    "\n",
    "### 6. **Управление Временем и Эффективностью**\n",
    "   - **Оценка времени:** Оценка времени выполнения подготовки данных и каждой эпохи обучения на основе первых пяти циклов, что позволяет планировать ресурсозатраты.\n",
    "   - **Оптимизация кода:** Фокусировка на ключевых этапах, таких как загрузка моделей, настройка параметров и запуск оптимизации с минимальными затратами времени.\n",
    "\n",
    "### Основные цели:\n",
    "- Обеспечение качественного сжатия контекста без потери точности.\n",
    "- Сведение к минимуму расхождения потерь между сжатым и сырым контекстами.\n",
    "- Избежание нежелательных интерпретаций и «додумывания» контекста моделью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модульная структура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательные классы и методы\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Подготовка данных\n",
    "def prepare_data(dataset, max_length=300):\n",
    "    \"\"\"\n",
    "    Функция подготавливает данные из датасета, включая предысторию, текущий фрагмент и целевой фрагмент текста,\n",
    "    используя смещающееся окно по элементам датасета.\n",
    "\n",
    "    :param dataset: Датасет с текстами.\n",
    "    :param max_length: Максимальная длина текущего фрагмента.\n",
    "    :return: Список кортежей (предыстория, текущий фрагмент, целевой фрагмент).\n",
    "    \"\"\"\n",
    "    prepared_data = []\n",
    "    buffer = []\n",
    "    counter = 0\n",
    "    for example in dataset:\n",
    "        text = example[\"text\"].strip()\n",
    "\n",
    "        # Пропускаем пустые записи\n",
    "        # TODO Убрать отладочную выборку первых 1000 записей\n",
    "        if len(text) == 0 or counter > 1000:\n",
    "            continue\n",
    "        counter += 1\n",
    "        buffer.append(text)\n",
    "\n",
    "        # Если буфер содержит три фрагмента, создаем (context, current_fragment, target)\n",
    "        if len(buffer) == 3:\n",
    "            context, current_fragment, target = buffer\n",
    "\n",
    "            # Обрезаем фрагменты по max_length, если необходимо\n",
    "            context = context[:max_length].strip()\n",
    "            current_fragment = current_fragment[:max_length].strip()\n",
    "            target = target[:max_length].strip()\n",
    "\n",
    "            # Пропускаем записи, если какой-либо из фрагментов пустой\n",
    "            if all([context, current_fragment, target]):\n",
    "                prepared_data.append((context, current_fragment, target))\n",
    "\n",
    "            # Сдвигаем окно\n",
    "            buffer.pop(0)\n",
    "    return prepared_data\n",
    "\n",
    "\n",
    "# Функции для сохранения и загрузки модели\n",
    "def save_model(model, optimizer, path=\"context_optimizer.pth\"):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, path=\"context_optimizer.pth\"):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "    else:\n",
    "        print(f\"Model file not found at {path}\")\n",
    "\n",
    "\n",
    "# Визуализация с использованием Plotly FigureWidget\n",
    "def create_loss_plot():\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Raw Context Loss\"))\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Compressed Context Loss\"))\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode=\"lines+markers\", name=\"Fragment Only Loss\"))\n",
    "    fig.update_layout(title=\"Графики потерь во время обучения\", xaxis_title=\"Эпоха\", yaxis_title=\"Среднее значение потерь\", template=\"plotly_dark\")\n",
    "    display(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def update_loss_plot(fig, epoch, raw_loss, compressed_loss, fragment_loss):\n",
    "    fig.data[0].x += (epoch,)\n",
    "    fig.data[0].y += (raw_loss,)\n",
    "    fig.data[1].x += (epoch,)\n",
    "    fig.data[1].y += (compressed_loss,)\n",
    "    fig.data[2].x += (epoch,)\n",
    "    fig.data[2].y += (fragment_loss,)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Функция для расчета compression ratio\n",
    "def calculate_compression_ratio(raw_embedding, compressed_embedding):\n",
    "    \"\"\"\n",
    "    Вычисляет отношение нормы сжатого контекста к норме сырого контекста.\n",
    "    \"\"\"\n",
    "    raw_norm = torch.norm(raw_embedding, p=2, dim=1)\n",
    "    compressed_norm = torch.norm(compressed_embedding, p=2, dim=1)\n",
    "    return compressed_norm / raw_norm\n",
    "\n",
    "\n",
    "def predict_with_compressed_context(compressed_context, current_fragment, tokenizer, gpt2_model, device, max_new_tokens=50, num_beams=5):\n",
    "    # Set the pad_token to eos_token to handle padding correctly\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Option 1\n",
    "        # Alternatively, you can add a custom padding token:\n",
    "        # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Option 2\n",
    "\n",
    "    inputs = tokenizer(current_fragment, return_tensors=\"pt\", padding=True).to(device)\n",
    "    input_ids = inputs.get(\"input_ids\")\n",
    "    attention_mask = inputs.get(\"attention_mask\")  # Adding attention_mask\n",
    "\n",
    "    # Проверка, что input_ids не пустой и compressed_context корректный\n",
    "    if input_ids is not None and input_ids.shape[1] > 0 and compressed_context is not None and compressed_context.shape[-1] == gpt2_model.config.hidden_size:\n",
    "        with torch.no_grad():\n",
    "            # Подгоняем форму compressed_context\n",
    "            compressed_context_repeated = compressed_context.unsqueeze(1).repeat(1, input_ids.shape[1], 1)\n",
    "            combined_inputs = compressed_context_repeated + gpt2_model.transformer.wte(input_ids)\n",
    "\n",
    "            # Генерация текста с указанием attention_mask и pad_token_id\n",
    "            outputs = gpt2_model.generate(\n",
    "                inputs_embeds=combined_inputs,\n",
    "                attention_mask=attention_mask,  # Use the attention mask\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,  # Ensure pad_token_id is set correctly\n",
    "            )\n",
    "            predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return predicted_text\n",
    "    else:\n",
    "        print(\n",
    "            f\"Invalid inputs: input_ids length {input_ids.shape[1] if input_ids is not None else 'None'}, compressed_context shape {compressed_context.shape if compressed_context is not None else 'None'}\"\n",
    "        )\n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "# Использование предсказаний из предподготовленных данных\n",
    "def evaluate_with_prepared_data(prepared_data, model, tokenizer, gpt2_model, device):\n",
    "    \"\"\"\n",
    "    Выполняет оценку качества предсказаний с использованием подготовленных данных.\n",
    "    \"\"\"\n",
    "    for entry in prepared_data:\n",
    "        context = entry[\"context\"]\n",
    "        current_fragment = entry[\"current_fragment\"]\n",
    "        target = entry[\"target\"]\n",
    "        raw_prediction = entry[\"raw_prediction\"]\n",
    "        fragment_prediction = entry[\"fragment_prediction\"]\n",
    "\n",
    "        # Сжатие контекста и предсказание с ним\n",
    "        compressed_context = model.compress(context)  # Предположим, у нас есть метод compress в модели\n",
    "        compressed_prediction = predict_with_compressed_context(compressed_context, current_fragment, tokenizer, gpt2_model, device)\n",
    "\n",
    "        # Сравнение предсказаний\n",
    "        print(f\"Контекст: {context}\")\n",
    "        print(f\"Текущий фрагмент: {current_fragment}\")\n",
    "        print(f\"Целевой текст: {target}\")\n",
    "        print(f\"Предсказание по сырому контексту: {raw_prediction}\")\n",
    "        print(f\"Предсказание по текущему фрагменту: {fragment_prediction}\")\n",
    "        print(f\"Предсказание по сжатому контексту: {compressed_prediction}\\n\")\n",
    "\n",
    "\n",
    "def load_intermediate_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель сжатия контекста\n",
    "\n",
    "\n",
    "class ContextOptimizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ContextOptimizer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, context):\n",
    "        x = self.relu(self.fc1(context))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Функция для сжатия контекста\n",
    "def compress_context(context, optimizer, tokenizer, gpt2_model, device):\n",
    "    # Здесь выполняется сжатие контекста\n",
    "\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(**inputs, output_hidden_states=True)\n",
    "        raw_context_embedding = outputs.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "    # Дополнительная проверка перед сжатием\n",
    "    if raw_context_embedding is None or raw_context_embedding.size(0) == 0:\n",
    "        print(\"Warning: raw_context_embedding is empty or invalid\")\n",
    "\n",
    "    # Пример сжатия (эта часть зависит от конкретной реализации оптимизатора)\n",
    "    compressed_context = optimizer(raw_context_embedding)\n",
    "\n",
    "    # Проверка формы и значений сжатого контекста\n",
    "    if not (compressed_context is not None and isinstance(compressed_context, torch.Tensor)):\n",
    "        print(\"Compressed context is invalid\")\n",
    "\n",
    "    return compressed_context, raw_context_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "def custom_loss(compressed_output, target_embedding, raw_context_embedding):\n",
    "    \"\"\"\n",
    "    Функция потерь, которая учитывает схожесть с оригиналом и целевым вектором.\n",
    "    \"\"\"\n",
    "    # Потеря восстановления: сравниваем сжатый контекст с исходным контекстом\n",
    "    reconstruction_loss = 1 - cosine_similarity(compressed_output, raw_context_embedding, dim=1).mean()\n",
    "\n",
    "    # Потеря предсказания: сравниваем предсказанный выход с целевым\n",
    "    prediction_loss = nn.MSELoss()(compressed_output, target_embedding)\n",
    "\n",
    "    # Итоговая потеря: комбинация восстановления и предсказания\n",
    "    total_loss = prediction_loss + reconstruction_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train_context_optimizer(optimizer, intermediate_data, tokenizer, gpt2_model, epochs=4, device=\"cpu\"):\n",
    "    criterion = nn.MSELoss()\n",
    "    opt = optim.Adam(optimizer.parameters(), lr=0.001)\n",
    "    raw_loss_values = []\n",
    "    compressed_loss_values = []\n",
    "    fragment_loss_values = []\n",
    "\n",
    "    # Инициализация графика потерь\n",
    "    loss_fig = create_loss_plot()\n",
    "    # Для отслеживания потерь по каждому примеру на каждой эпохе\n",
    "    example_loss_history = {i: [] for i in range(len(intermediate_data))}\n",
    "    for epoch in range(epochs):\n",
    "        examples = []\n",
    "        total_compression_ratio = 0\n",
    "        total_compressed_loss = 0\n",
    "        total_divergence = 0  # Переменная для отслеживания расхождения между потерями\n",
    "        count = 0\n",
    "\n",
    "        epoch_losses = []  # Для хранения потерь текущей эпохи\n",
    "        examples_calculated = []\n",
    "        for idx, entry in enumerate(intermediate_data):\n",
    "            context = entry[\"context\"]\n",
    "            current_fragment = entry[\"current_fragment\"]\n",
    "            target = entry[\"target\"]\n",
    "\n",
    "            # Используем предвычисленные значения потерь для сырого контекста и текущего фрагмента\n",
    "            raw_loss = entry[\"raw_loss\"]\n",
    "            fragment_loss = entry[\"fragment_loss\"]\n",
    "\n",
    "            # Подготовка целевого предсказания\n",
    "            target_input = tokenizer(target, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "                target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "            # Сжатие контекста и вычисление потерь для сжатого контекста в комбинации с текущим фрагментом\n",
    "            compressed_context, raw_context_embedding = compress_context(context, optimizer, tokenizer, gpt2_model, device)\n",
    "            compressed_inputs = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "            compressed_context_repeated = compressed_context.unsqueeze(1).repeat(1, compressed_inputs[\"input_ids\"].size(1), 1)\n",
    "            compressed_inputs_embeds = compressed_context_repeated + gpt2_model.transformer.wte(compressed_inputs[\"input_ids\"])\n",
    "            compressed_outputs = gpt2_model(inputs_embeds=compressed_inputs_embeds, output_hidden_states=True).hidden_states[-1].mean(dim=1)\n",
    "            # compressed_loss = criterion(compressed_outputs, target_embedding)\n",
    "            # Используем custom_loss вместо обычной MSE\n",
    "            compressed_loss = custom_loss(compressed_outputs, target_embedding, raw_context_embedding)\n",
    "\n",
    "            # Вычисление расхождения потерь\n",
    "            divergence = abs(compressed_loss.item() - raw_loss)\n",
    "            total_divergence += divergence\n",
    "\n",
    "            # Корректировка потерь для учета расхождения\n",
    "            adjusted_loss = compressed_loss + divergence  # Добавляем расхождение для управления сходимостью\n",
    "\n",
    "            # Вычисление compression ratio\n",
    "            compression_ratio = calculate_compression_ratio(raw_context_embedding, compressed_context)\n",
    "            total_compression_ratio += compression_ratio.item()\n",
    "\n",
    "            # Обновление оптимизатора только по скорректированным потерям\n",
    "            opt.zero_grad()\n",
    "            adjusted_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_compressed_loss += compressed_loss.item()\n",
    "            count += 1\n",
    "\n",
    "            # Сохраняем текущую потерю для анализа изменения потерь\n",
    "            epoch_losses.append(compressed_loss.item())\n",
    "            example_loss_history[idx].append(compressed_loss.item())\n",
    "            # Сохранение примеров для диагностики\n",
    "            with torch.no_grad():\n",
    "                compressed_pred = predict_with_compressed_context(compressed_context, current_fragment, tokenizer, gpt2_model, device)\n",
    "                raw_pred = entry[\"raw_prediction\"]\n",
    "                fragment_pred = entry[\"fragment_prediction\"]\n",
    "\n",
    "                # Проверяем и обрабатываем compressed_context корректно перед декодированием\n",
    "                if compressed_context is not None and isinstance(compressed_context, torch.Tensor):\n",
    "                    # Пытаемся преобразовать в список токенов, исключая None\n",
    "                    compressed_tokens = [token for token in compressed_context.squeeze().tolist() if isinstance(token, (int, float)) and 0 <= token < tokenizer.vocab_size]\n",
    "\n",
    "                    if compressed_tokens:\n",
    "                        # Декодируем только корректные значения токенов\n",
    "                        compressed_context_text = tokenizer.decode(compressed_tokens)\n",
    "                    else:\n",
    "                        compressed_context_text = \"Empty or Invalid tokens found in compressed context\"\n",
    "                else:\n",
    "                    compressed_context_text = \"N/A\"\n",
    "\n",
    "                # Сохраняем примеры в отдельный список для всех примеров\n",
    "                examples_calculated.append(\n",
    "                    {\n",
    "                        \"context\": context,\n",
    "                        \"compressed_context\": compressed_context_text,\n",
    "                        \"current_fragment\": current_fragment,\n",
    "                        \"compressed_prediction\": compressed_pred,\n",
    "                        \"raw_prediction\": raw_pred,\n",
    "                        \"fragment_prediction\": fragment_pred,\n",
    "                        \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Анализ потерь между эпохами для вывода примеров с максимальным снижением потерь\n",
    "        if epoch > 0:\n",
    "            loss_reduction = [\n",
    "                (i, example_loss_history[i][-2] - example_loss_history[i][-1], examples_calculated[i][\"compressed_context\"], examples_calculated[i][\"compressed_prediction\"])\n",
    "                for i in range(len(epoch_losses))\n",
    "                if len(example_loss_history[i]) > 1\n",
    "            ]\n",
    "            loss_reduction.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_examples = loss_reduction[:3]  # Три примера с максимальным снижением потерь\n",
    "        else:\n",
    "            top_examples = [\n",
    "                (i, 0, examples_calculated[i][\"compressed_context\"], examples_calculated[i][\"compressed_prediction\"]) for i in range(min(3, len(examples_calculated)))\n",
    "            ]  # Первые три примера для первой эпохи\n",
    "\n",
    "        avg_compressed_loss = total_compressed_loss / count if count > 0 else 0\n",
    "        avg_compression_ratio = total_compression_ratio / count if count > 0 else 0\n",
    "        avg_divergence = total_divergence / count if count > 0 else 0\n",
    "        raw_loss_values.append(raw_loss)\n",
    "        compressed_loss_values.append(avg_compressed_loss)\n",
    "        fragment_loss_values.append(fragment_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Avg Compressed Context Loss: {avg_compressed_loss:.4f}, Avg Compression Ratio: {avg_compression_ratio:.4f}, Avg Divergence: {avg_divergence:.4f}\")\n",
    "\n",
    "        # Обновление графика после каждой эпохи\n",
    "        update_loss_plot(loss_fig, epoch + 1, raw_loss, avg_compressed_loss, fragment_loss)\n",
    "\n",
    "        # Вывод примеров с максимальным снижением потерь\n",
    "        print(f\"\\n=== Примеры с максимальным снижением потерь после эпохи {epoch+1} ===\")\n",
    "        for idx, reduction, compressed_context, compressed_prediction in top_examples:\n",
    "            example = intermediate_data[idx]\n",
    "            print(f\"\\nПример {idx+1} (Снижение потерь: {reduction:.4f}):\")\n",
    "            print(f\"Предыстория: {example['context']}\")\n",
    "            print(f\"Сжатый контекст: {compressed_context}\")  # Используем сохраненный сжатый контекст\n",
    "            print(f\"Текущий фрагмент: {example['current_fragment']}\")\n",
    "            print(f\"Предсказание по сжатому контексту: {compressed_prediction}\")  # Используем сохраненное предсказание по сжатому контексту\n",
    "            print(f\"Предсказание по сырому контексту: {example['raw_prediction']}\")\n",
    "            print(f\"Предсказание по текущему фрагменту: {example['fragment_prediction']}\")\n",
    "            print(f\"Целевой текст: {example['target']}\")\n",
    "\n",
    "    return raw_loss_values, compressed_loss_values, fragment_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded from context_optimizer.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inimatic\\AppData\\Local\\Temp\\ipykernel_14724\\158085552.py:67: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562a10728e144d6284da39ee77312115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Raw Context Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'e2408de9-4fd1-436f-8db1-f8c37bdb3c18',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'mode': 'lines+markers',\n",
       "              'name': 'Compressed Context Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd1bfe77e-1d3d-4c84-9706-e4e7e4ad36f5',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'mode': 'lines+markers',\n",
       "              'name': 'Fragment Only Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'f859a988-186c-4765-95c8-69089ea8d66b',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Графики потерь во время обучения'},\n",
       "               'xaxis': {'title': {'text': 'Эпоха'}},\n",
       "               'yaxis': {'title': {'text': 'Среднее значение потерь'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Настройки устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка модели GPT-2 и токенизатора\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Загрузка датасета и подготовка промежуточных данных\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "intermediate_dataset_path = \"intermediate_dataset.json\"\n",
    "\n",
    "# Генерация промежуточного датасета, если он еще не создан\n",
    "if not os.path.exists(intermediate_dataset_path):\n",
    "    generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device=device, intermediate_dataset_path=intermediate_dataset_path)\n",
    "else:\n",
    "    # Загрузка промежуточного датасета\n",
    "    intermediate_data = load_intermediate_dataset(intermediate_dataset_path)\n",
    "\n",
    "# Инициализация модели сжатия контекста\n",
    "input_size = 768  # Размер входных данных модели GPT-2\n",
    "hidden_size = 256  # Скрытый размер для оптимизатора контекста\n",
    "context_optimizer = ContextOptimizer(input_size, hidden_size).to(device)\n",
    "\n",
    "# Загрузка сохраненной модели, если она существует\n",
    "load_model(context_optimizer, context_optimizer, \"context_optimizer.pth\")\n",
    "\n",
    "# Запуск обучения\n",
    "raw_loss_values, compressed_loss_values, fragment_loss_values = train_context_optimizer(context_optimizer, intermediate_data, tokenizer, gpt2_model, epochs=10, device=device)\n",
    "\n",
    "# Сохранение модели после обучения\n",
    "save_model(context_optimizer, context_optimizer, \"context_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предподготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Начало подготовки данных...\n",
      "Оценка времени выполнения цикла: 435.63 секунд для 999 записей.\n",
      "Промежуточный датасет создан с 197 записями, включая значения потерь и предсказания.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import time\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device=\"cpu\", intermediate_dataset_path=\"intermediate_dataset.json\"):\n",
    "    intermediate_data = []\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Подготавливаем данные\n",
    "    print(\"Начало подготовки данных...\")\n",
    "    prepared_data = prepare_data(dataset, max_length=500)\n",
    "\n",
    "    # Оценка времени выполнения цикла для первых пяти итераций\n",
    "    cycle_times = []\n",
    "    for i, (context, current_fragment, target) in enumerate(prepared_data[:5]):\n",
    "        if context is None or current_fragment is None:\n",
    "            continue\n",
    "\n",
    "        cycle_start_time = time.time()\n",
    "\n",
    "        # Подготовка целевого предсказания\n",
    "        target_input = tokenizer(target, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "            target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "        # Предсказание по сырому контексту (предыстория + текущий фрагмент)\n",
    "        raw_inputs = tokenizer(context + \" \" + current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            raw_outputs = gpt2_model(**raw_inputs, output_hidden_states=True)\n",
    "            raw_combined_embedding = raw_outputs.hidden_states[-1].mean(dim=1)\n",
    "            raw_loss = criterion(raw_combined_embedding, target_embedding).item()\n",
    "            raw_prediction = tokenizer.decode(raw_outputs.logits.argmax(-1).squeeze().tolist())\n",
    "\n",
    "        # Предсказание только по текущему фрагменту\n",
    "        fragment_input = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            fragment_output = gpt2_model(**fragment_input, output_hidden_states=True)\n",
    "            fragment_embedding = fragment_output.hidden_states[-1].mean(dim=1)\n",
    "            fragment_loss = criterion(fragment_embedding, target_embedding).item()\n",
    "            fragment_prediction = tokenizer.decode(fragment_output.logits.argmax(-1).squeeze().tolist())\n",
    "\n",
    "        cycle_end_time = time.time()\n",
    "        cycle_times.append(cycle_end_time - cycle_start_time)\n",
    "\n",
    "        # Фильтрация по критерию fragment_loss > raw_loss\n",
    "        if fragment_loss > raw_loss and fragment_loss < 1:\n",
    "            # Сохранение всех значений и предсказаний\n",
    "            intermediate_data.append(\n",
    "                {\n",
    "                    \"context\": context,\n",
    "                    \"current_fragment\": current_fragment,\n",
    "                    \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                    \"raw_loss\": raw_loss,\n",
    "                    \"fragment_loss\": fragment_loss,\n",
    "                    \"raw_prediction\": raw_prediction,\n",
    "                    \"fragment_prediction\": fragment_prediction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Оценка общего времени выполнения цикла на основе первых пяти измерений\n",
    "    avg_cycle_time = sum(cycle_times) / len(cycle_times) if cycle_times else 0\n",
    "    estimated_total_time = avg_cycle_time * len(prepared_data)\n",
    "    print(f\"Оценка времени выполнения цикла: {estimated_total_time:.2f} секунд для {len(prepared_data)} записей.\")\n",
    "\n",
    "    # Продолжение обработки остальных данных после оценки времени\n",
    "    for i, (context, current_fragment, target) in enumerate(prepared_data[5:]):\n",
    "        if context is None or current_fragment is None:\n",
    "            continue\n",
    "\n",
    "        # Подготовка целевого предсказания\n",
    "        target_input = tokenizer(target, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "            target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "        # Предсказание по сырому контексту (предыстория + текущий фрагмент)\n",
    "        raw_inputs = tokenizer(context + \" \" + current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            raw_outputs = gpt2_model(**raw_inputs, output_hidden_states=True)\n",
    "            raw_combined_embedding = raw_outputs.hidden_states[-1].mean(dim=1)\n",
    "            raw_loss = criterion(raw_combined_embedding, target_embedding).item()\n",
    "            raw_prediction = tokenizer.decode(raw_outputs.logits.argmax(-1).squeeze().tolist())\n",
    "\n",
    "        # Предсказание только по текущему фрагменту\n",
    "        fragment_input = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            fragment_output = gpt2_model(**fragment_input, output_hidden_states=True)\n",
    "            fragment_embedding = fragment_output.hidden_states[-1].mean(dim=1)\n",
    "            fragment_loss = criterion(fragment_embedding, target_embedding).item()\n",
    "            fragment_prediction = tokenizer.decode(fragment_output.logits.argmax(-1).squeeze().tolist())\n",
    "\n",
    "        # Фильтрация по критерию fragment_loss > raw_loss and fragment_loss <= 1\n",
    "        if fragment_loss > raw_loss and fragment_loss < 1:\n",
    "            # Сохранение всех значений и предсказаний\n",
    "            intermediate_data.append(\n",
    "                {\n",
    "                    \"context\": context,\n",
    "                    \"current_fragment\": current_fragment,\n",
    "                    \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                    \"raw_loss\": raw_loss,\n",
    "                    \"fragment_loss\": fragment_loss,\n",
    "                    \"raw_prediction\": raw_prediction,\n",
    "                    \"fragment_prediction\": fragment_prediction,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Сохранение промежуточного датасета в JSON\n",
    "    with open(intermediate_dataset_path, \"w\") as f:\n",
    "        json.dump(intermediate_data, f, indent=4)\n",
    "\n",
    "    print(f\"Промежуточный датасет создан с {len(intermediate_data)} записями, включая значения потерь и предсказания.\")\n",
    "\n",
    "\n",
    "# Настройки\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загружаем датасет wikitext-2-raw-v1\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Инициализация токенизатора и модели GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Задаем путь для сохранения промежуточного датасета\n",
    "intermediate_dataset_path = \"intermediate_dataset.json\"\n",
    "\n",
    "# Генерация промежуточного датасета\n",
    "generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device, intermediate_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\MUIV\\concl\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало подготовки данных...\n",
      "Данные подгружены.\n",
      "Оценка времени подготовки данных: 276.26 секунд для 999 записей.\n",
      "Промежуточный датасет создан с 999 записями, включая значения потерь.\n"
     ]
    }
   ],
   "source": [
    "# Устарело\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device=\"cpu\", intermediate_dataset_path=\"intermediate_dataset.json\"):\n",
    "    intermediate_data = []\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Подготовка данных\n",
    "    print(\"Начало подготовки данных...\")\n",
    "    start_time = time.time()\n",
    "    prepared_data = prepare_data(dataset, max_length=500)\n",
    "    print(\"Данные подгружены.\")\n",
    "    # Оценка времени подготовки данных\n",
    "    preparation_times = []\n",
    "    for i, (context, current_fragment, target) in enumerate(prepared_data[:5]):\n",
    "        if context is None or current_fragment is None:\n",
    "            continue\n",
    "        preparation_times.append(time.time() - start_time)\n",
    "        start_time = time.time()\n",
    "\n",
    "    avg_preparation_time = sum(preparation_times) / len(preparation_times)\n",
    "    estimated_total_time = avg_preparation_time * len(prepared_data)\n",
    "    print(f\"Оценка времени подготовки данных: {estimated_total_time:.2f} секунд для {len(prepared_data)} записей.\")\n",
    "\n",
    "    for i, (context, current_fragment, target) in enumerate(prepared_data):\n",
    "        if context is None or current_fragment is None:\n",
    "            continue\n",
    "\n",
    "        # Подготовка целевого предсказания\n",
    "        target_input = tokenizer(target, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            target_output = gpt2_model(**target_input, output_hidden_states=True)\n",
    "            target_embedding = target_output.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "        # Предсказание по сырому контексту (предыстория + текущий фрагмент)\n",
    "        raw_inputs = tokenizer(context + \" \" + current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            raw_outputs = gpt2_model(**raw_inputs, output_hidden_states=True)\n",
    "            raw_combined_embedding = raw_outputs.hidden_states[-1].mean(dim=1)\n",
    "            raw_loss = criterion(raw_combined_embedding, target_embedding).item()\n",
    "\n",
    "        # Предсказание только по текущему фрагменту\n",
    "        fragment_input = tokenizer(current_fragment, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            fragment_output = gpt2_model(**fragment_input, output_hidden_states=True)\n",
    "            fragment_embedding = fragment_output.hidden_states[-1].mean(dim=1)\n",
    "            fragment_loss = criterion(fragment_embedding, target_embedding).item()\n",
    "\n",
    "        # Сохранение всех значений и предсказаний\n",
    "        intermediate_data.append(\n",
    "            {\n",
    "                \"context\": context,\n",
    "                \"current_fragment\": current_fragment,\n",
    "                \"target\": tokenizer.decode(target_input[\"input_ids\"].squeeze().tolist()),\n",
    "                \"raw_loss\": raw_loss,\n",
    "                \"fragment_loss\": fragment_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Сохранение промежуточного датасета в JSON\n",
    "    with open(intermediate_dataset_path, \"w\") as f:\n",
    "        json.dump(intermediate_data, f, indent=4)\n",
    "\n",
    "    print(f\"Промежуточный датасет создан с {len(intermediate_data)} записями, включая значения потерь.\")\n",
    "\n",
    "\n",
    "# Настройки\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загружаем датасет wikitext-2-raw-v1\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Инициализация токенизатора и модели GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)  # Используем 'cpu', можно заменить на 'cuda' при использовании GPU\n",
    "\n",
    "# Задаем путь для сохранения промежуточного датасета\n",
    "intermediate_dataset_path = \"intermediate_dataset.json\"\n",
    "\n",
    "# Генерация промежуточного датасета\n",
    "generate_intermediate_dataset(dataset, tokenizer, gpt2_model, device, intermediate_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
